{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sJM4MLKL8bA"
      },
      "source": [
        "**QUESTION 1: Feedforward Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sFY_3-5LKQq",
        "outputId": "dc68b2ba-8abf-42b6-a5b0-3f2329deca97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data load\n",
        "\n",
        "\n",
        "# choose between steps 5 (device is cpu) - 6 (device is gpu) - 7 (training with validation)\n",
        "step = 6\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np \n",
        "from torchvision import datasets \n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mfcctraindata = np.load('/content/drive/My Drive/HW3DATA/music_genre_data_di/train/mfccs/X.npy')\n",
        "mfcctrainlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/train/mfccs/labels.npy\")\n",
        "\n",
        "mfcctestdata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/mfccs/X.npy\")\n",
        "mfcctestlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/mfccs/labels.npy\")\n",
        "\n",
        "mfccvaldata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/mfccs/X.npy\")\n",
        "mfccvallabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/mfccs/labels.npy\")\n",
        "\n",
        "class mfccdataset(Dataset): #use opposite labels\n",
        "  def __init__(self,data, labels,labels_map,transform=None, target_transform=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.mfcclabels = []\n",
        "    for l in labels:\n",
        "      self.mfcclabels.append(labels_map[l])\n",
        "    self.mfcclabels = np.array(self.mfcclabels)\n",
        "    self.mfccdata = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mfcclabels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sound = self.mfccdata[index]\n",
        "    label = self.mfcclabels[index]\n",
        "    if self.transform:\n",
        "      sound = self.transform(sound)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "\n",
        "    return sound, label\n",
        "\n",
        "labels_map = {\n",
        "    0: \"classical\",\n",
        "    1: \"blues\",\n",
        "    2: \"rock_metal_hardrock\",\n",
        "    3: \"hiphop\",\n",
        "}\n",
        "\n",
        "opposite_labels_map = {\n",
        "    \"classical\":0,\n",
        "    \"blues\":1,\n",
        "    \"rock_metal_hardrock\":2,\n",
        "    \"hiphop\":3,\n",
        "}\n",
        "\n",
        "training_data = mfccdataset(mfcctraindata, mfcctrainlabels, opposite_labels_map, torch.tensor)\n",
        "val_data = mfccdataset(mfccvaldata, mfccvallabels, opposite_labels_map, torch.tensor)\n",
        "test_data = mfccdataset(mfcctestdata, mfcctestlabels, opposite_labels_map, torch.tensor)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=800,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=1376,shuffle=False )  #biggest batch size due to making sure all data is counted towards f1 score instead of the last batch\n",
        "#                                                                           also there is no need for batches in training\n",
        "\n",
        "\n",
        "# It is usually just for memory use limitation vs speed of assessment.\n",
        "#  Larger batches evaluate faster on parallelised systems such as GPUs, but use more memory to process. \n",
        "# Test results should be identical, with same size of dataset and same model, regardless of batch size.\n",
        "# https://ai.stackexchange.com/questions/10201/what-is-the-reason-behind-using-a-test-batch-size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OznNS-86oiyH",
        "outputId": "1116fdf4-bd3c-4a9a-b431-6876397462c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#step 2\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(NeuralNetwork, self).__init__()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(26, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "if step == 5:\n",
        "  device = 'cpu'\n",
        "else:\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD5zayRQNgRf"
      },
      "outputs": [],
      "source": [
        "#step 3\n",
        "\n",
        "def training(epochs, optimizer, dataloader, costfunct, model):\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    print(f\"current epoch: {ep}\\n\")\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X.float())\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRiN_VF7VH1k",
        "outputId": "65a331e3-0ed4-49b0-c621-d08750be7520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n"
          ]
        }
      ],
      "source": [
        "#step 4\n",
        "\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score\n",
        "from torchmetrics import ConfusionMatrix\n",
        "def testing(dataloader, costfunct, model):\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X.float())\n",
        "      test_loss += costfunct(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  acc = 100*correct\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  f1 = f1_score(pred, y, num_classes=4, average=\"macro\")\n",
        "  confmatrix = ConfusionMatrix(num_classes=4).to(device)\n",
        "  print(f\"f1 score is: {f1.item()}\\n the confusion matrix is:\")\n",
        "  print(confmatrix(pred.argmax(1), y))\n",
        "\n",
        "  return test_loss, f1, acc, confmatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-iT7E-Unpis"
      },
      "outputs": [],
      "source": [
        "#step 7\n",
        "\n",
        "\n",
        "def training_with_validation(epochs, optimizer, dataloader, validationdata,costfunct, model):\n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    print(f\"current epoch: {ep}\\n\")\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X.float())\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print()\n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "\n",
        "  print(f\"best epoch is: {bestepoch}\")\n",
        "  return bestmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf8j0K4oaOcO",
        "outputId": "47bf106a-b3c7-468e-83a5-e27946f0ea9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss: 1.343596  [  944/ 3200]\n",
            "loss: 1.355235  [  960/ 3200]\n",
            "loss: 1.312934  [  976/ 3200]\n",
            "loss: 1.333995  [  992/ 3200]\n",
            "loss: 1.420858  [ 1008/ 3200]\n",
            "loss: 1.374951  [ 1024/ 3200]\n",
            "loss: 1.372311  [ 1040/ 3200]\n",
            "loss: 1.309866  [ 1056/ 3200]\n",
            "loss: 1.314035  [ 1072/ 3200]\n",
            "loss: 1.333807  [ 1088/ 3200]\n",
            "loss: 1.369524  [ 1104/ 3200]\n",
            "loss: 1.358934  [ 1120/ 3200]\n",
            "loss: 1.385604  [ 1136/ 3200]\n",
            "loss: 1.384070  [ 1152/ 3200]\n",
            "loss: 1.373606  [ 1168/ 3200]\n",
            "loss: 1.357012  [ 1184/ 3200]\n",
            "loss: 1.353260  [ 1200/ 3200]\n",
            "loss: 1.357418  [ 1216/ 3200]\n",
            "loss: 1.359389  [ 1232/ 3200]\n",
            "loss: 1.351822  [ 1248/ 3200]\n",
            "loss: 1.337129  [ 1264/ 3200]\n",
            "loss: 1.375250  [ 1280/ 3200]\n",
            "loss: 1.349505  [ 1296/ 3200]\n",
            "loss: 1.350576  [ 1312/ 3200]\n",
            "loss: 1.394325  [ 1328/ 3200]\n",
            "loss: 1.360017  [ 1344/ 3200]\n",
            "loss: 1.345882  [ 1360/ 3200]\n",
            "loss: 1.349106  [ 1376/ 3200]\n",
            "loss: 1.379405  [ 1392/ 3200]\n",
            "loss: 1.372197  [ 1408/ 3200]\n",
            "loss: 1.368750  [ 1424/ 3200]\n",
            "loss: 1.351080  [ 1440/ 3200]\n",
            "loss: 1.360481  [ 1456/ 3200]\n",
            "loss: 1.341548  [ 1472/ 3200]\n",
            "loss: 1.331611  [ 1488/ 3200]\n",
            "loss: 1.350238  [ 1504/ 3200]\n",
            "loss: 1.372676  [ 1520/ 3200]\n",
            "loss: 1.386117  [ 1536/ 3200]\n",
            "loss: 1.353107  [ 1552/ 3200]\n",
            "loss: 1.375266  [ 1568/ 3200]\n",
            "loss: 1.363335  [ 1584/ 3200]\n",
            "loss: 1.345894  [ 1600/ 3200]\n",
            "loss: 1.354349  [ 1616/ 3200]\n",
            "loss: 1.351024  [ 1632/ 3200]\n",
            "loss: 1.369261  [ 1648/ 3200]\n",
            "loss: 1.350072  [ 1664/ 3200]\n",
            "loss: 1.359879  [ 1680/ 3200]\n",
            "loss: 1.371791  [ 1696/ 3200]\n",
            "loss: 1.350679  [ 1712/ 3200]\n",
            "loss: 1.335909  [ 1728/ 3200]\n",
            "loss: 1.370799  [ 1744/ 3200]\n",
            "loss: 1.355200  [ 1760/ 3200]\n",
            "loss: 1.379343  [ 1776/ 3200]\n",
            "loss: 1.347957  [ 1792/ 3200]\n",
            "loss: 1.350670  [ 1808/ 3200]\n",
            "loss: 1.344946  [ 1824/ 3200]\n",
            "loss: 1.356911  [ 1840/ 3200]\n",
            "loss: 1.359737  [ 1856/ 3200]\n",
            "loss: 1.367099  [ 1872/ 3200]\n",
            "loss: 1.374566  [ 1888/ 3200]\n",
            "loss: 1.350427  [ 1904/ 3200]\n",
            "loss: 1.395564  [ 1920/ 3200]\n",
            "loss: 1.359887  [ 1936/ 3200]\n",
            "loss: 1.349463  [ 1952/ 3200]\n",
            "loss: 1.359877  [ 1968/ 3200]\n",
            "loss: 1.368314  [ 1984/ 3200]\n",
            "loss: 1.366373  [ 2000/ 3200]\n",
            "loss: 1.363043  [ 2016/ 3200]\n",
            "loss: 1.379247  [ 2032/ 3200]\n",
            "loss: 1.366467  [ 2048/ 3200]\n",
            "loss: 1.355097  [ 2064/ 3200]\n",
            "loss: 1.386012  [ 2080/ 3200]\n",
            "loss: 1.375930  [ 2096/ 3200]\n",
            "loss: 1.342282  [ 2112/ 3200]\n",
            "loss: 1.366601  [ 2128/ 3200]\n",
            "loss: 1.349636  [ 2144/ 3200]\n",
            "loss: 1.338338  [ 2160/ 3200]\n",
            "loss: 1.381294  [ 2176/ 3200]\n",
            "loss: 1.366104  [ 2192/ 3200]\n",
            "loss: 1.347449  [ 2208/ 3200]\n",
            "loss: 1.349839  [ 2224/ 3200]\n",
            "loss: 1.376586  [ 2240/ 3200]\n",
            "loss: 1.358877  [ 2256/ 3200]\n",
            "loss: 1.354879  [ 2272/ 3200]\n",
            "loss: 1.359075  [ 2288/ 3200]\n",
            "loss: 1.363770  [ 2304/ 3200]\n",
            "loss: 1.347868  [ 2320/ 3200]\n",
            "loss: 1.350321  [ 2336/ 3200]\n",
            "loss: 1.355061  [ 2352/ 3200]\n",
            "loss: 1.363373  [ 2368/ 3200]\n",
            "loss: 1.369398  [ 2384/ 3200]\n",
            "loss: 1.357871  [ 2400/ 3200]\n",
            "loss: 1.341384  [ 2416/ 3200]\n",
            "loss: 1.352772  [ 2432/ 3200]\n",
            "loss: 1.361619  [ 2448/ 3200]\n",
            "loss: 1.352864  [ 2464/ 3200]\n",
            "loss: 1.367241  [ 2480/ 3200]\n",
            "loss: 1.372696  [ 2496/ 3200]\n",
            "loss: 1.359002  [ 2512/ 3200]\n",
            "loss: 1.358246  [ 2528/ 3200]\n",
            "loss: 1.361089  [ 2544/ 3200]\n",
            "loss: 1.360922  [ 2560/ 3200]\n",
            "loss: 1.359022  [ 2576/ 3200]\n",
            "loss: 1.367037  [ 2592/ 3200]\n",
            "loss: 1.346673  [ 2608/ 3200]\n",
            "loss: 1.355603  [ 2624/ 3200]\n",
            "loss: 1.353804  [ 2640/ 3200]\n",
            "loss: 1.379816  [ 2656/ 3200]\n",
            "loss: 1.352835  [ 2672/ 3200]\n",
            "loss: 1.362323  [ 2688/ 3200]\n",
            "loss: 1.362372  [ 2704/ 3200]\n",
            "loss: 1.351101  [ 2720/ 3200]\n",
            "loss: 1.357089  [ 2736/ 3200]\n",
            "loss: 1.326017  [ 2752/ 3200]\n",
            "loss: 1.391486  [ 2768/ 3200]\n",
            "loss: 1.350457  [ 2784/ 3200]\n",
            "loss: 1.359093  [ 2800/ 3200]\n",
            "loss: 1.372367  [ 2816/ 3200]\n",
            "loss: 1.337081  [ 2832/ 3200]\n",
            "loss: 1.363043  [ 2848/ 3200]\n",
            "loss: 1.358299  [ 2864/ 3200]\n",
            "loss: 1.334110  [ 2880/ 3200]\n",
            "loss: 1.381993  [ 2896/ 3200]\n",
            "loss: 1.333700  [ 2912/ 3200]\n",
            "loss: 1.366989  [ 2928/ 3200]\n",
            "loss: 1.359861  [ 2944/ 3200]\n",
            "loss: 1.334592  [ 2960/ 3200]\n",
            "loss: 1.331441  [ 2976/ 3200]\n",
            "loss: 1.341819  [ 2992/ 3200]\n",
            "loss: 1.416316  [ 3008/ 3200]\n",
            "loss: 1.365875  [ 3024/ 3200]\n",
            "loss: 1.334811  [ 3040/ 3200]\n",
            "loss: 1.358037  [ 3056/ 3200]\n",
            "loss: 1.347754  [ 3072/ 3200]\n",
            "loss: 1.350334  [ 3088/ 3200]\n",
            "loss: 1.371269  [ 3104/ 3200]\n",
            "loss: 1.366193  [ 3120/ 3200]\n",
            "loss: 1.343362  [ 3136/ 3200]\n",
            "loss: 1.364733  [ 3152/ 3200]\n",
            "loss: 1.363079  [ 3168/ 3200]\n",
            "loss: 1.369911  [ 3184/ 3200]\n",
            "current epoch: 6\n",
            "\n",
            "loss: 1.344833  [    0/ 3200]\n",
            "loss: 1.364753  [   16/ 3200]\n",
            "loss: 1.352230  [   32/ 3200]\n",
            "loss: 1.337809  [   48/ 3200]\n",
            "loss: 1.357876  [   64/ 3200]\n",
            "loss: 1.359388  [   80/ 3200]\n",
            "loss: 1.350520  [   96/ 3200]\n",
            "loss: 1.359568  [  112/ 3200]\n",
            "loss: 1.348441  [  128/ 3200]\n",
            "loss: 1.384606  [  144/ 3200]\n",
            "loss: 1.343372  [  160/ 3200]\n",
            "loss: 1.358356  [  176/ 3200]\n",
            "loss: 1.367100  [  192/ 3200]\n",
            "loss: 1.344729  [  208/ 3200]\n",
            "loss: 1.349486  [  224/ 3200]\n",
            "loss: 1.348072  [  240/ 3200]\n",
            "loss: 1.372904  [  256/ 3200]\n",
            "loss: 1.360247  [  272/ 3200]\n",
            "loss: 1.351407  [  288/ 3200]\n",
            "loss: 1.348793  [  304/ 3200]\n",
            "loss: 1.352618  [  320/ 3200]\n",
            "loss: 1.360019  [  336/ 3200]\n",
            "loss: 1.358602  [  352/ 3200]\n",
            "loss: 1.354169  [  368/ 3200]\n",
            "loss: 1.358699  [  384/ 3200]\n",
            "loss: 1.352975  [  400/ 3200]\n",
            "loss: 1.363176  [  416/ 3200]\n",
            "loss: 1.366316  [  432/ 3200]\n",
            "loss: 1.346700  [  448/ 3200]\n",
            "loss: 1.357713  [  464/ 3200]\n",
            "loss: 1.371505  [  480/ 3200]\n",
            "loss: 1.347057  [  496/ 3200]\n",
            "loss: 1.344221  [  512/ 3200]\n",
            "loss: 1.357117  [  528/ 3200]\n",
            "loss: 1.352235  [  544/ 3200]\n",
            "loss: 1.377682  [  560/ 3200]\n",
            "loss: 1.340864  [  576/ 3200]\n",
            "loss: 1.357220  [  592/ 3200]\n",
            "loss: 1.349196  [  608/ 3200]\n",
            "loss: 1.359382  [  624/ 3200]\n",
            "loss: 1.373187  [  640/ 3200]\n",
            "loss: 1.347188  [  656/ 3200]\n",
            "loss: 1.355442  [  672/ 3200]\n",
            "loss: 1.369309  [  688/ 3200]\n",
            "loss: 1.359044  [  704/ 3200]\n",
            "loss: 1.358645  [  720/ 3200]\n",
            "loss: 1.354683  [  736/ 3200]\n",
            "loss: 1.357974  [  752/ 3200]\n",
            "loss: 1.349715  [  768/ 3200]\n",
            "loss: 1.354897  [  784/ 3200]\n",
            "loss: 1.355421  [  800/ 3200]\n",
            "loss: 1.349052  [  816/ 3200]\n",
            "loss: 1.364797  [  832/ 3200]\n",
            "loss: 1.356341  [  848/ 3200]\n",
            "loss: 1.338626  [  864/ 3200]\n",
            "loss: 1.347068  [  880/ 3200]\n",
            "loss: 1.354732  [  896/ 3200]\n",
            "loss: 1.350343  [  912/ 3200]\n",
            "loss: 1.360793  [  928/ 3200]\n",
            "loss: 1.355297  [  944/ 3200]\n",
            "loss: 1.346290  [  960/ 3200]\n",
            "loss: 1.349493  [  976/ 3200]\n",
            "loss: 1.355145  [  992/ 3200]\n",
            "loss: 1.347702  [ 1008/ 3200]\n",
            "loss: 1.360654  [ 1024/ 3200]\n",
            "loss: 1.349677  [ 1040/ 3200]\n",
            "loss: 1.355696  [ 1056/ 3200]\n",
            "loss: 1.349691  [ 1072/ 3200]\n",
            "loss: 1.348659  [ 1088/ 3200]\n",
            "loss: 1.354388  [ 1104/ 3200]\n",
            "loss: 1.360019  [ 1120/ 3200]\n",
            "loss: 1.358146  [ 1136/ 3200]\n",
            "loss: 1.357438  [ 1152/ 3200]\n",
            "loss: 1.350413  [ 1168/ 3200]\n",
            "loss: 1.364127  [ 1184/ 3200]\n",
            "loss: 1.355091  [ 1200/ 3200]\n",
            "loss: 1.372653  [ 1216/ 3200]\n",
            "loss: 1.356086  [ 1232/ 3200]\n",
            "loss: 1.377059  [ 1248/ 3200]\n",
            "loss: 1.352087  [ 1264/ 3200]\n",
            "loss: 1.360662  [ 1280/ 3200]\n",
            "loss: 1.365448  [ 1296/ 3200]\n",
            "loss: 1.356394  [ 1312/ 3200]\n",
            "loss: 1.364855  [ 1328/ 3200]\n",
            "loss: 1.350692  [ 1344/ 3200]\n",
            "loss: 1.348848  [ 1360/ 3200]\n",
            "loss: 1.346895  [ 1376/ 3200]\n",
            "loss: 1.359203  [ 1392/ 3200]\n",
            "loss: 1.348303  [ 1408/ 3200]\n",
            "loss: 1.338557  [ 1424/ 3200]\n",
            "loss: 1.355573  [ 1440/ 3200]\n",
            "loss: 1.341313  [ 1456/ 3200]\n",
            "loss: 1.378429  [ 1472/ 3200]\n",
            "loss: 1.349793  [ 1488/ 3200]\n",
            "loss: 1.363498  [ 1504/ 3200]\n",
            "loss: 1.318865  [ 1520/ 3200]\n",
            "loss: 1.323374  [ 1536/ 3200]\n",
            "loss: 1.376732  [ 1552/ 3200]\n",
            "loss: 1.388779  [ 1568/ 3200]\n",
            "loss: 1.337706  [ 1584/ 3200]\n",
            "loss: 1.369221  [ 1600/ 3200]\n",
            "loss: 1.351528  [ 1616/ 3200]\n",
            "loss: 1.345827  [ 1632/ 3200]\n",
            "loss: 1.420181  [ 1648/ 3200]\n",
            "loss: 1.366952  [ 1664/ 3200]\n",
            "loss: 1.362309  [ 1680/ 3200]\n",
            "loss: 1.347231  [ 1696/ 3200]\n",
            "loss: 1.347122  [ 1712/ 3200]\n",
            "loss: 1.332238  [ 1728/ 3200]\n",
            "loss: 1.377303  [ 1744/ 3200]\n",
            "loss: 1.364346  [ 1760/ 3200]\n",
            "loss: 1.361351  [ 1776/ 3200]\n",
            "loss: 1.337470  [ 1792/ 3200]\n",
            "loss: 1.350944  [ 1808/ 3200]\n",
            "loss: 1.343351  [ 1824/ 3200]\n",
            "loss: 1.331117  [ 1840/ 3200]\n",
            "loss: 1.363109  [ 1856/ 3200]\n",
            "loss: 1.352397  [ 1872/ 3200]\n",
            "loss: 1.366530  [ 1888/ 3200]\n",
            "loss: 1.348044  [ 1904/ 3200]\n",
            "loss: 1.352814  [ 1920/ 3200]\n",
            "loss: 1.343752  [ 1936/ 3200]\n",
            "loss: 1.350059  [ 1952/ 3200]\n",
            "loss: 1.345760  [ 1968/ 3200]\n",
            "loss: 1.353618  [ 1984/ 3200]\n",
            "loss: 1.360509  [ 2000/ 3200]\n",
            "loss: 1.357528  [ 2016/ 3200]\n",
            "loss: 1.344696  [ 2032/ 3200]\n",
            "loss: 1.363177  [ 2048/ 3200]\n",
            "loss: 1.342055  [ 2064/ 3200]\n",
            "loss: 1.352524  [ 2080/ 3200]\n",
            "loss: 1.352511  [ 2096/ 3200]\n",
            "loss: 1.351021  [ 2112/ 3200]\n",
            "loss: 1.332046  [ 2128/ 3200]\n",
            "loss: 1.329826  [ 2144/ 3200]\n",
            "loss: 1.395846  [ 2160/ 3200]\n",
            "loss: 1.338799  [ 2176/ 3200]\n",
            "loss: 1.337923  [ 2192/ 3200]\n",
            "loss: 1.327861  [ 2208/ 3200]\n",
            "loss: 1.375095  [ 2224/ 3200]\n",
            "loss: 1.336520  [ 2240/ 3200]\n",
            "loss: 1.370048  [ 2256/ 3200]\n",
            "loss: 1.364867  [ 2272/ 3200]\n",
            "loss: 1.336573  [ 2288/ 3200]\n",
            "loss: 1.321367  [ 2304/ 3200]\n",
            "loss: 1.352986  [ 2320/ 3200]\n",
            "loss: 1.368305  [ 2336/ 3200]\n",
            "loss: 1.343534  [ 2352/ 3200]\n",
            "loss: 1.366133  [ 2368/ 3200]\n",
            "loss: 1.326709  [ 2384/ 3200]\n",
            "loss: 1.355128  [ 2400/ 3200]\n",
            "loss: 1.355430  [ 2416/ 3200]\n",
            "loss: 1.336318  [ 2432/ 3200]\n",
            "loss: 1.336921  [ 2448/ 3200]\n",
            "loss: 1.346959  [ 2464/ 3200]\n",
            "loss: 1.369706  [ 2480/ 3200]\n",
            "loss: 1.358050  [ 2496/ 3200]\n",
            "loss: 1.359466  [ 2512/ 3200]\n",
            "loss: 1.340480  [ 2528/ 3200]\n",
            "loss: 1.346735  [ 2544/ 3200]\n",
            "loss: 1.358780  [ 2560/ 3200]\n",
            "loss: 1.375427  [ 2576/ 3200]\n",
            "loss: 1.367103  [ 2592/ 3200]\n",
            "loss: 1.353291  [ 2608/ 3200]\n",
            "loss: 1.365191  [ 2624/ 3200]\n",
            "loss: 1.351338  [ 2640/ 3200]\n",
            "loss: 1.295092  [ 2656/ 3200]\n",
            "loss: 1.315485  [ 2672/ 3200]\n",
            "loss: 1.400107  [ 2688/ 3200]\n",
            "loss: 1.352927  [ 2704/ 3200]\n",
            "loss: 1.324874  [ 2720/ 3200]\n",
            "loss: 1.360328  [ 2736/ 3200]\n",
            "loss: 1.351854  [ 2752/ 3200]\n",
            "loss: 1.364329  [ 2768/ 3200]\n",
            "loss: 1.338428  [ 2784/ 3200]\n",
            "loss: 1.321161  [ 2800/ 3200]\n",
            "loss: 1.312456  [ 2816/ 3200]\n",
            "loss: 1.324456  [ 2832/ 3200]\n",
            "loss: 1.334986  [ 2848/ 3200]\n",
            "loss: 1.377448  [ 2864/ 3200]\n",
            "loss: 1.356546  [ 2880/ 3200]\n",
            "loss: 1.371656  [ 2896/ 3200]\n",
            "loss: 1.339052  [ 2912/ 3200]\n",
            "loss: 1.325601  [ 2928/ 3200]\n",
            "loss: 1.350816  [ 2944/ 3200]\n",
            "loss: 1.345103  [ 2960/ 3200]\n",
            "loss: 1.371308  [ 2976/ 3200]\n",
            "loss: 1.355391  [ 2992/ 3200]\n",
            "loss: 1.308710  [ 3008/ 3200]\n",
            "loss: 1.328805  [ 3024/ 3200]\n",
            "loss: 1.293862  [ 3040/ 3200]\n",
            "loss: 1.369575  [ 3056/ 3200]\n",
            "loss: 1.324809  [ 3072/ 3200]\n",
            "loss: 1.365030  [ 3088/ 3200]\n",
            "loss: 1.374341  [ 3104/ 3200]\n",
            "loss: 1.338907  [ 3120/ 3200]\n",
            "loss: 1.322664  [ 3136/ 3200]\n",
            "loss: 1.358572  [ 3152/ 3200]\n",
            "loss: 1.318724  [ 3168/ 3200]\n",
            "loss: 1.277199  [ 3184/ 3200]\n",
            "current epoch: 7\n",
            "\n",
            "loss: 1.375861  [    0/ 3200]\n",
            "loss: 1.312958  [   16/ 3200]\n",
            "loss: 1.314924  [   32/ 3200]\n",
            "loss: 1.343305  [   48/ 3200]\n",
            "loss: 1.410709  [   64/ 3200]\n",
            "loss: 1.334989  [   80/ 3200]\n",
            "loss: 1.355508  [   96/ 3200]\n",
            "loss: 1.375985  [  112/ 3200]\n",
            "loss: 1.310538  [  128/ 3200]\n",
            "loss: 1.411163  [  144/ 3200]\n",
            "loss: 1.388700  [  160/ 3200]\n",
            "loss: 1.377074  [  176/ 3200]\n",
            "loss: 1.364297  [  192/ 3200]\n",
            "loss: 1.354552  [  208/ 3200]\n",
            "loss: 1.325824  [  224/ 3200]\n",
            "loss: 1.340064  [  240/ 3200]\n",
            "loss: 1.328028  [  256/ 3200]\n",
            "loss: 1.354328  [  272/ 3200]\n",
            "loss: 1.357397  [  288/ 3200]\n",
            "loss: 1.352589  [  304/ 3200]\n",
            "loss: 1.357918  [  320/ 3200]\n",
            "loss: 1.348943  [  336/ 3200]\n",
            "loss: 1.348384  [  352/ 3200]\n",
            "loss: 1.365408  [  368/ 3200]\n",
            "loss: 1.351677  [  384/ 3200]\n",
            "loss: 1.350827  [  400/ 3200]\n",
            "loss: 1.347617  [  416/ 3200]\n",
            "loss: 1.365867  [  432/ 3200]\n",
            "loss: 1.330129  [  448/ 3200]\n",
            "loss: 1.337328  [  464/ 3200]\n",
            "loss: 1.337811  [  480/ 3200]\n",
            "loss: 1.342010  [  496/ 3200]\n",
            "loss: 1.319047  [  512/ 3200]\n",
            "loss: 1.343542  [  528/ 3200]\n",
            "loss: 1.363269  [  544/ 3200]\n",
            "loss: 1.373488  [  560/ 3200]\n",
            "loss: 1.344183  [  576/ 3200]\n",
            "loss: 1.343104  [  592/ 3200]\n",
            "loss: 1.373459  [  608/ 3200]\n",
            "loss: 1.323703  [  624/ 3200]\n",
            "loss: 1.357032  [  640/ 3200]\n",
            "loss: 1.337928  [  656/ 3200]\n",
            "loss: 1.395422  [  672/ 3200]\n",
            "loss: 1.360846  [  688/ 3200]\n",
            "loss: 1.357962  [  704/ 3200]\n",
            "loss: 1.351395  [  720/ 3200]\n",
            "loss: 1.339491  [  736/ 3200]\n",
            "loss: 1.340597  [  752/ 3200]\n",
            "loss: 1.364403  [  768/ 3200]\n",
            "loss: 1.344935  [  784/ 3200]\n",
            "loss: 1.344950  [  800/ 3200]\n",
            "loss: 1.359493  [  816/ 3200]\n",
            "loss: 1.331092  [  832/ 3200]\n",
            "loss: 1.331241  [  848/ 3200]\n",
            "loss: 1.344534  [  864/ 3200]\n",
            "loss: 1.344944  [  880/ 3200]\n",
            "loss: 1.344678  [  896/ 3200]\n",
            "loss: 1.358468  [  912/ 3200]\n",
            "loss: 1.339488  [  928/ 3200]\n",
            "loss: 1.354681  [  944/ 3200]\n",
            "loss: 1.351292  [  960/ 3200]\n",
            "loss: 1.350571  [  976/ 3200]\n",
            "loss: 1.349546  [  992/ 3200]\n",
            "loss: 1.333346  [ 1008/ 3200]\n",
            "loss: 1.360290  [ 1024/ 3200]\n",
            "loss: 1.298835  [ 1040/ 3200]\n",
            "loss: 1.348849  [ 1056/ 3200]\n",
            "loss: 1.351034  [ 1072/ 3200]\n",
            "loss: 1.351426  [ 1088/ 3200]\n",
            "loss: 1.305119  [ 1104/ 3200]\n",
            "loss: 1.372720  [ 1120/ 3200]\n",
            "loss: 1.330921  [ 1136/ 3200]\n",
            "loss: 1.346474  [ 1152/ 3200]\n",
            "loss: 1.378000  [ 1168/ 3200]\n",
            "loss: 1.360395  [ 1184/ 3200]\n",
            "loss: 1.327952  [ 1200/ 3200]\n",
            "loss: 1.381545  [ 1216/ 3200]\n",
            "loss: 1.365309  [ 1232/ 3200]\n",
            "loss: 1.334333  [ 1248/ 3200]\n",
            "loss: 1.345243  [ 1264/ 3200]\n",
            "loss: 1.320384  [ 1280/ 3200]\n",
            "loss: 1.393189  [ 1296/ 3200]\n",
            "loss: 1.337973  [ 1312/ 3200]\n",
            "loss: 1.333053  [ 1328/ 3200]\n",
            "loss: 1.355809  [ 1344/ 3200]\n",
            "loss: 1.356838  [ 1360/ 3200]\n",
            "loss: 1.360286  [ 1376/ 3200]\n",
            "loss: 1.351608  [ 1392/ 3200]\n",
            "loss: 1.333176  [ 1408/ 3200]\n",
            "loss: 1.355570  [ 1424/ 3200]\n",
            "loss: 1.363604  [ 1440/ 3200]\n",
            "loss: 1.349175  [ 1456/ 3200]\n",
            "loss: 1.322827  [ 1472/ 3200]\n",
            "loss: 1.349374  [ 1488/ 3200]\n",
            "loss: 1.354220  [ 1504/ 3200]\n",
            "loss: 1.340105  [ 1520/ 3200]\n",
            "loss: 1.314370  [ 1536/ 3200]\n",
            "loss: 1.359889  [ 1552/ 3200]\n",
            "loss: 1.336549  [ 1568/ 3200]\n",
            "loss: 1.314501  [ 1584/ 3200]\n",
            "loss: 1.308240  [ 1600/ 3200]\n",
            "loss: 1.346807  [ 1616/ 3200]\n",
            "loss: 1.368079  [ 1632/ 3200]\n",
            "loss: 1.361149  [ 1648/ 3200]\n",
            "loss: 1.384268  [ 1664/ 3200]\n",
            "loss: 1.317017  [ 1680/ 3200]\n",
            "loss: 1.370729  [ 1696/ 3200]\n",
            "loss: 1.309389  [ 1712/ 3200]\n",
            "loss: 1.355958  [ 1728/ 3200]\n",
            "loss: 1.336882  [ 1744/ 3200]\n",
            "loss: 1.314577  [ 1760/ 3200]\n",
            "loss: 1.337930  [ 1776/ 3200]\n",
            "loss: 1.328818  [ 1792/ 3200]\n",
            "loss: 1.321427  [ 1808/ 3200]\n",
            "loss: 1.309590  [ 1824/ 3200]\n",
            "loss: 1.368432  [ 1840/ 3200]\n",
            "loss: 1.369932  [ 1856/ 3200]\n",
            "loss: 1.365971  [ 1872/ 3200]\n",
            "loss: 1.362952  [ 1888/ 3200]\n",
            "loss: 1.338804  [ 1904/ 3200]\n",
            "loss: 1.374786  [ 1920/ 3200]\n",
            "loss: 1.360852  [ 1936/ 3200]\n",
            "loss: 1.354127  [ 1952/ 3200]\n",
            "loss: 1.356462  [ 1968/ 3200]\n",
            "loss: 1.318317  [ 1984/ 3200]\n",
            "loss: 1.332507  [ 2000/ 3200]\n",
            "loss: 1.346179  [ 2016/ 3200]\n",
            "loss: 1.354511  [ 2032/ 3200]\n",
            "loss: 1.359832  [ 2048/ 3200]\n",
            "loss: 1.353667  [ 2064/ 3200]\n",
            "loss: 1.368460  [ 2080/ 3200]\n",
            "loss: 1.335881  [ 2096/ 3200]\n",
            "loss: 1.337910  [ 2112/ 3200]\n",
            "loss: 1.342212  [ 2128/ 3200]\n",
            "loss: 1.325623  [ 2144/ 3200]\n",
            "loss: 1.340211  [ 2160/ 3200]\n",
            "loss: 1.314576  [ 2176/ 3200]\n",
            "loss: 1.311971  [ 2192/ 3200]\n",
            "loss: 1.365074  [ 2208/ 3200]\n",
            "loss: 1.310934  [ 2224/ 3200]\n",
            "loss: 1.395264  [ 2240/ 3200]\n",
            "loss: 1.355537  [ 2256/ 3200]\n",
            "loss: 1.361992  [ 2272/ 3200]\n",
            "loss: 1.323583  [ 2288/ 3200]\n",
            "loss: 1.327009  [ 2304/ 3200]\n",
            "loss: 1.350167  [ 2320/ 3200]\n",
            "loss: 1.344053  [ 2336/ 3200]\n",
            "loss: 1.388087  [ 2352/ 3200]\n",
            "loss: 1.331457  [ 2368/ 3200]\n",
            "loss: 1.370369  [ 2384/ 3200]\n",
            "loss: 1.316570  [ 2400/ 3200]\n",
            "loss: 1.353839  [ 2416/ 3200]\n",
            "loss: 1.389529  [ 2432/ 3200]\n",
            "loss: 1.334247  [ 2448/ 3200]\n",
            "loss: 1.330380  [ 2464/ 3200]\n",
            "loss: 1.359174  [ 2480/ 3200]\n",
            "loss: 1.344285  [ 2496/ 3200]\n",
            "loss: 1.309858  [ 2512/ 3200]\n",
            "loss: 1.343218  [ 2528/ 3200]\n",
            "loss: 1.341549  [ 2544/ 3200]\n",
            "loss: 1.336240  [ 2560/ 3200]\n",
            "loss: 1.362634  [ 2576/ 3200]\n",
            "loss: 1.343594  [ 2592/ 3200]\n",
            "loss: 1.333789  [ 2608/ 3200]\n",
            "loss: 1.286664  [ 2624/ 3200]\n",
            "loss: 1.353462  [ 2640/ 3200]\n",
            "loss: 1.319927  [ 2656/ 3200]\n",
            "loss: 1.333047  [ 2672/ 3200]\n",
            "loss: 1.329593  [ 2688/ 3200]\n",
            "loss: 1.324639  [ 2704/ 3200]\n",
            "loss: 1.377599  [ 2720/ 3200]\n",
            "loss: 1.357005  [ 2736/ 3200]\n",
            "loss: 1.381119  [ 2752/ 3200]\n",
            "loss: 1.326179  [ 2768/ 3200]\n",
            "loss: 1.361078  [ 2784/ 3200]\n",
            "loss: 1.359257  [ 2800/ 3200]\n",
            "loss: 1.301765  [ 2816/ 3200]\n",
            "loss: 1.331962  [ 2832/ 3200]\n",
            "loss: 1.327358  [ 2848/ 3200]\n",
            "loss: 1.329103  [ 2864/ 3200]\n",
            "loss: 1.384151  [ 2880/ 3200]\n",
            "loss: 1.380974  [ 2896/ 3200]\n",
            "loss: 1.335537  [ 2912/ 3200]\n",
            "loss: 1.336728  [ 2928/ 3200]\n",
            "loss: 1.375164  [ 2944/ 3200]\n",
            "loss: 1.353707  [ 2960/ 3200]\n",
            "loss: 1.326664  [ 2976/ 3200]\n",
            "loss: 1.333930  [ 2992/ 3200]\n",
            "loss: 1.379752  [ 3008/ 3200]\n",
            "loss: 1.331030  [ 3024/ 3200]\n",
            "loss: 1.337703  [ 3040/ 3200]\n",
            "loss: 1.358986  [ 3056/ 3200]\n",
            "loss: 1.333715  [ 3072/ 3200]\n",
            "loss: 1.335490  [ 3088/ 3200]\n",
            "loss: 1.300516  [ 3104/ 3200]\n",
            "loss: 1.387778  [ 3120/ 3200]\n",
            "loss: 1.332175  [ 3136/ 3200]\n",
            "loss: 1.328343  [ 3152/ 3200]\n",
            "loss: 1.344630  [ 3168/ 3200]\n",
            "loss: 1.340609  [ 3184/ 3200]\n",
            "current epoch: 8\n",
            "\n",
            "loss: 1.344255  [    0/ 3200]\n",
            "loss: 1.360065  [   16/ 3200]\n",
            "loss: 1.340328  [   32/ 3200]\n",
            "loss: 1.326687  [   48/ 3200]\n",
            "loss: 1.298343  [   64/ 3200]\n",
            "loss: 1.356184  [   80/ 3200]\n",
            "loss: 1.292539  [   96/ 3200]\n",
            "loss: 1.388697  [  112/ 3200]\n",
            "loss: 1.346756  [  128/ 3200]\n",
            "loss: 1.338099  [  144/ 3200]\n",
            "loss: 1.319876  [  160/ 3200]\n",
            "loss: 1.330476  [  176/ 3200]\n",
            "loss: 1.363561  [  192/ 3200]\n",
            "loss: 1.304855  [  208/ 3200]\n",
            "loss: 1.382481  [  224/ 3200]\n",
            "loss: 1.335140  [  240/ 3200]\n",
            "loss: 1.324366  [  256/ 3200]\n",
            "loss: 1.358866  [  272/ 3200]\n",
            "loss: 1.334508  [  288/ 3200]\n",
            "loss: 1.309217  [  304/ 3200]\n",
            "loss: 1.327713  [  320/ 3200]\n",
            "loss: 1.352733  [  336/ 3200]\n",
            "loss: 1.351279  [  352/ 3200]\n",
            "loss: 1.344565  [  368/ 3200]\n",
            "loss: 1.318205  [  384/ 3200]\n",
            "loss: 1.360246  [  400/ 3200]\n",
            "loss: 1.345235  [  416/ 3200]\n",
            "loss: 1.348657  [  432/ 3200]\n",
            "loss: 1.328034  [  448/ 3200]\n",
            "loss: 1.321464  [  464/ 3200]\n",
            "loss: 1.286328  [  480/ 3200]\n",
            "loss: 1.365032  [  496/ 3200]\n",
            "loss: 1.384475  [  512/ 3200]\n",
            "loss: 1.304366  [  528/ 3200]\n",
            "loss: 1.299153  [  544/ 3200]\n",
            "loss: 1.362923  [  560/ 3200]\n",
            "loss: 1.353929  [  576/ 3200]\n",
            "loss: 1.358216  [  592/ 3200]\n",
            "loss: 1.344335  [  608/ 3200]\n",
            "loss: 1.355041  [  624/ 3200]\n",
            "loss: 1.346085  [  640/ 3200]\n",
            "loss: 1.346488  [  656/ 3200]\n",
            "loss: 1.352774  [  672/ 3200]\n",
            "loss: 1.353452  [  688/ 3200]\n",
            "loss: 1.363551  [  704/ 3200]\n",
            "loss: 1.353385  [  720/ 3200]\n",
            "loss: 1.340304  [  736/ 3200]\n",
            "loss: 1.335523  [  752/ 3200]\n",
            "loss: 1.328667  [  768/ 3200]\n",
            "loss: 1.362564  [  784/ 3200]\n",
            "loss: 1.348048  [  800/ 3200]\n",
            "loss: 1.317703  [  816/ 3200]\n",
            "loss: 1.298822  [  832/ 3200]\n",
            "loss: 1.333363  [  848/ 3200]\n",
            "loss: 1.353868  [  864/ 3200]\n",
            "loss: 1.343791  [  880/ 3200]\n",
            "loss: 1.347121  [  896/ 3200]\n",
            "loss: 1.329812  [  912/ 3200]\n",
            "loss: 1.327927  [  928/ 3200]\n",
            "loss: 1.354209  [  944/ 3200]\n",
            "loss: 1.367563  [  960/ 3200]\n",
            "loss: 1.322569  [  976/ 3200]\n",
            "loss: 1.270356  [  992/ 3200]\n",
            "loss: 1.316884  [ 1008/ 3200]\n",
            "loss: 1.362975  [ 1024/ 3200]\n",
            "loss: 1.312150  [ 1040/ 3200]\n",
            "loss: 1.320754  [ 1056/ 3200]\n",
            "loss: 1.316729  [ 1072/ 3200]\n",
            "loss: 1.336774  [ 1088/ 3200]\n",
            "loss: 1.359937  [ 1104/ 3200]\n",
            "loss: 1.313833  [ 1120/ 3200]\n",
            "loss: 1.345041  [ 1136/ 3200]\n",
            "loss: 1.299871  [ 1152/ 3200]\n",
            "loss: 1.329595  [ 1168/ 3200]\n",
            "loss: 1.391094  [ 1184/ 3200]\n",
            "loss: 1.346436  [ 1200/ 3200]\n",
            "loss: 1.301143  [ 1216/ 3200]\n",
            "loss: 1.352377  [ 1232/ 3200]\n",
            "loss: 1.292060  [ 1248/ 3200]\n",
            "loss: 1.361985  [ 1264/ 3200]\n",
            "loss: 1.348133  [ 1280/ 3200]\n",
            "loss: 1.357204  [ 1296/ 3200]\n",
            "loss: 1.324336  [ 1312/ 3200]\n",
            "loss: 1.325001  [ 1328/ 3200]\n",
            "loss: 1.328498  [ 1344/ 3200]\n",
            "loss: 1.324721  [ 1360/ 3200]\n",
            "loss: 1.326752  [ 1376/ 3200]\n",
            "loss: 1.296798  [ 1392/ 3200]\n",
            "loss: 1.364612  [ 1408/ 3200]\n",
            "loss: 1.325684  [ 1424/ 3200]\n",
            "loss: 1.344603  [ 1440/ 3200]\n",
            "loss: 1.372593  [ 1456/ 3200]\n",
            "loss: 1.335344  [ 1472/ 3200]\n",
            "loss: 1.341658  [ 1488/ 3200]\n",
            "loss: 1.333761  [ 1504/ 3200]\n",
            "loss: 1.347099  [ 1520/ 3200]\n",
            "loss: 1.344239  [ 1536/ 3200]\n",
            "loss: 1.313114  [ 1552/ 3200]\n",
            "loss: 1.362132  [ 1568/ 3200]\n",
            "loss: 1.347176  [ 1584/ 3200]\n",
            "loss: 1.350760  [ 1600/ 3200]\n",
            "loss: 1.335644  [ 1616/ 3200]\n",
            "loss: 1.333270  [ 1632/ 3200]\n",
            "loss: 1.348398  [ 1648/ 3200]\n",
            "loss: 1.337510  [ 1664/ 3200]\n",
            "loss: 1.359734  [ 1680/ 3200]\n",
            "loss: 1.343404  [ 1696/ 3200]\n",
            "loss: 1.330037  [ 1712/ 3200]\n",
            "loss: 1.370942  [ 1728/ 3200]\n",
            "loss: 1.322544  [ 1744/ 3200]\n",
            "loss: 1.396665  [ 1760/ 3200]\n",
            "loss: 1.327566  [ 1776/ 3200]\n",
            "loss: 1.335511  [ 1792/ 3200]\n",
            "loss: 1.314184  [ 1808/ 3200]\n",
            "loss: 1.303979  [ 1824/ 3200]\n",
            "loss: 1.372457  [ 1840/ 3200]\n",
            "loss: 1.345608  [ 1856/ 3200]\n",
            "loss: 1.359548  [ 1872/ 3200]\n",
            "loss: 1.334685  [ 1888/ 3200]\n",
            "loss: 1.352759  [ 1904/ 3200]\n",
            "loss: 1.366016  [ 1920/ 3200]\n",
            "loss: 1.358392  [ 1936/ 3200]\n",
            "loss: 1.310469  [ 1952/ 3200]\n",
            "loss: 1.328124  [ 1968/ 3200]\n",
            "loss: 1.341658  [ 1984/ 3200]\n",
            "loss: 1.333752  [ 2000/ 3200]\n",
            "loss: 1.343494  [ 2016/ 3200]\n",
            "loss: 1.386767  [ 2032/ 3200]\n",
            "loss: 1.363601  [ 2048/ 3200]\n",
            "loss: 1.338435  [ 2064/ 3200]\n",
            "loss: 1.310633  [ 2080/ 3200]\n",
            "loss: 1.347160  [ 2096/ 3200]\n",
            "loss: 1.331406  [ 2112/ 3200]\n",
            "loss: 1.306957  [ 2128/ 3200]\n",
            "loss: 1.359654  [ 2144/ 3200]\n",
            "loss: 1.370375  [ 2160/ 3200]\n",
            "loss: 1.337645  [ 2176/ 3200]\n",
            "loss: 1.315413  [ 2192/ 3200]\n",
            "loss: 1.337155  [ 2208/ 3200]\n",
            "loss: 1.327584  [ 2224/ 3200]\n",
            "loss: 1.336700  [ 2240/ 3200]\n",
            "loss: 1.361036  [ 2256/ 3200]\n",
            "loss: 1.334912  [ 2272/ 3200]\n",
            "loss: 1.348060  [ 2288/ 3200]\n",
            "loss: 1.311361  [ 2304/ 3200]\n",
            "loss: 1.334257  [ 2320/ 3200]\n",
            "loss: 1.347592  [ 2336/ 3200]\n",
            "loss: 1.322476  [ 2352/ 3200]\n",
            "loss: 1.324180  [ 2368/ 3200]\n",
            "loss: 1.340684  [ 2384/ 3200]\n",
            "loss: 1.354523  [ 2400/ 3200]\n",
            "loss: 1.305110  [ 2416/ 3200]\n",
            "loss: 1.333909  [ 2432/ 3200]\n",
            "loss: 1.321587  [ 2448/ 3200]\n",
            "loss: 1.404018  [ 2464/ 3200]\n",
            "loss: 1.317100  [ 2480/ 3200]\n",
            "loss: 1.299478  [ 2496/ 3200]\n",
            "loss: 1.340320  [ 2512/ 3200]\n",
            "loss: 1.349987  [ 2528/ 3200]\n",
            "loss: 1.342521  [ 2544/ 3200]\n",
            "loss: 1.355792  [ 2560/ 3200]\n",
            "loss: 1.337112  [ 2576/ 3200]\n",
            "loss: 1.351155  [ 2592/ 3200]\n",
            "loss: 1.324860  [ 2608/ 3200]\n",
            "loss: 1.322508  [ 2624/ 3200]\n",
            "loss: 1.304522  [ 2640/ 3200]\n",
            "loss: 1.314876  [ 2656/ 3200]\n",
            "loss: 1.356421  [ 2672/ 3200]\n",
            "loss: 1.366419  [ 2688/ 3200]\n",
            "loss: 1.336302  [ 2704/ 3200]\n",
            "loss: 1.337449  [ 2720/ 3200]\n",
            "loss: 1.333503  [ 2736/ 3200]\n",
            "loss: 1.345299  [ 2752/ 3200]\n",
            "loss: 1.352927  [ 2768/ 3200]\n",
            "loss: 1.321275  [ 2784/ 3200]\n",
            "loss: 1.371663  [ 2800/ 3200]\n",
            "loss: 1.343868  [ 2816/ 3200]\n",
            "loss: 1.338388  [ 2832/ 3200]\n",
            "loss: 1.336939  [ 2848/ 3200]\n",
            "loss: 1.327221  [ 2864/ 3200]\n",
            "loss: 1.304509  [ 2880/ 3200]\n",
            "loss: 1.309472  [ 2896/ 3200]\n",
            "loss: 1.327805  [ 2912/ 3200]\n",
            "loss: 1.317134  [ 2928/ 3200]\n",
            "loss: 1.340119  [ 2944/ 3200]\n",
            "loss: 1.285319  [ 2960/ 3200]\n",
            "loss: 1.343148  [ 2976/ 3200]\n",
            "loss: 1.327671  [ 2992/ 3200]\n",
            "loss: 1.368860  [ 3008/ 3200]\n",
            "loss: 1.334775  [ 3024/ 3200]\n",
            "loss: 1.261266  [ 3040/ 3200]\n",
            "loss: 1.355107  [ 3056/ 3200]\n",
            "loss: 1.295027  [ 3072/ 3200]\n",
            "loss: 1.366151  [ 3088/ 3200]\n",
            "loss: 1.382855  [ 3104/ 3200]\n",
            "loss: 1.336767  [ 3120/ 3200]\n",
            "loss: 1.356746  [ 3136/ 3200]\n",
            "loss: 1.359548  [ 3152/ 3200]\n",
            "loss: 1.319781  [ 3168/ 3200]\n",
            "loss: 1.330407  [ 3184/ 3200]\n",
            "current epoch: 9\n",
            "\n",
            "loss: 1.395863  [    0/ 3200]\n",
            "loss: 1.303601  [   16/ 3200]\n",
            "loss: 1.318142  [   32/ 3200]\n",
            "loss: 1.312773  [   48/ 3200]\n",
            "loss: 1.362441  [   64/ 3200]\n",
            "loss: 1.358877  [   80/ 3200]\n",
            "loss: 1.278260  [   96/ 3200]\n",
            "loss: 1.388679  [  112/ 3200]\n",
            "loss: 1.305136  [  128/ 3200]\n",
            "loss: 1.312744  [  144/ 3200]\n",
            "loss: 1.357418  [  160/ 3200]\n",
            "loss: 1.361425  [  176/ 3200]\n",
            "loss: 1.294065  [  192/ 3200]\n",
            "loss: 1.367728  [  208/ 3200]\n",
            "loss: 1.333721  [  224/ 3200]\n",
            "loss: 1.301020  [  240/ 3200]\n",
            "loss: 1.388904  [  256/ 3200]\n",
            "loss: 1.404339  [  272/ 3200]\n",
            "loss: 1.340288  [  288/ 3200]\n",
            "loss: 1.333779  [  304/ 3200]\n",
            "loss: 1.376332  [  320/ 3200]\n",
            "loss: 1.299515  [  336/ 3200]\n",
            "loss: 1.356180  [  352/ 3200]\n",
            "loss: 1.346654  [  368/ 3200]\n",
            "loss: 1.349655  [  384/ 3200]\n",
            "loss: 1.319357  [  400/ 3200]\n",
            "loss: 1.349039  [  416/ 3200]\n",
            "loss: 1.322234  [  432/ 3200]\n",
            "loss: 1.344877  [  448/ 3200]\n",
            "loss: 1.337297  [  464/ 3200]\n",
            "loss: 1.331746  [  480/ 3200]\n",
            "loss: 1.327045  [  496/ 3200]\n",
            "loss: 1.310059  [  512/ 3200]\n",
            "loss: 1.335434  [  528/ 3200]\n",
            "loss: 1.318477  [  544/ 3200]\n",
            "loss: 1.317555  [  560/ 3200]\n",
            "loss: 1.335148  [  576/ 3200]\n",
            "loss: 1.322047  [  592/ 3200]\n",
            "loss: 1.322954  [  608/ 3200]\n",
            "loss: 1.324678  [  624/ 3200]\n",
            "loss: 1.313557  [  640/ 3200]\n",
            "loss: 1.333259  [  656/ 3200]\n",
            "loss: 1.324294  [  672/ 3200]\n",
            "loss: 1.353800  [  688/ 3200]\n",
            "loss: 1.348889  [  704/ 3200]\n",
            "loss: 1.336608  [  720/ 3200]\n",
            "loss: 1.291365  [  736/ 3200]\n",
            "loss: 1.303571  [  752/ 3200]\n",
            "loss: 1.367658  [  768/ 3200]\n",
            "loss: 1.366396  [  784/ 3200]\n",
            "loss: 1.359313  [  800/ 3200]\n",
            "loss: 1.305694  [  816/ 3200]\n",
            "loss: 1.356143  [  832/ 3200]\n",
            "loss: 1.291113  [  848/ 3200]\n",
            "loss: 1.308837  [  864/ 3200]\n",
            "loss: 1.239322  [  880/ 3200]\n",
            "loss: 1.395857  [  896/ 3200]\n",
            "loss: 1.345472  [  912/ 3200]\n",
            "loss: 1.323465  [  928/ 3200]\n",
            "loss: 1.290386  [  944/ 3200]\n",
            "loss: 1.338950  [  960/ 3200]\n",
            "loss: 1.374439  [  976/ 3200]\n",
            "loss: 1.353173  [  992/ 3200]\n",
            "loss: 1.317384  [ 1008/ 3200]\n",
            "loss: 1.323595  [ 1024/ 3200]\n",
            "loss: 1.315666  [ 1040/ 3200]\n",
            "loss: 1.349926  [ 1056/ 3200]\n",
            "loss: 1.319795  [ 1072/ 3200]\n",
            "loss: 1.315056  [ 1088/ 3200]\n",
            "loss: 1.294669  [ 1104/ 3200]\n",
            "loss: 1.345021  [ 1120/ 3200]\n",
            "loss: 1.309533  [ 1136/ 3200]\n",
            "loss: 1.352124  [ 1152/ 3200]\n",
            "loss: 1.345089  [ 1168/ 3200]\n",
            "loss: 1.407329  [ 1184/ 3200]\n",
            "loss: 1.305273  [ 1200/ 3200]\n",
            "loss: 1.366784  [ 1216/ 3200]\n",
            "loss: 1.363921  [ 1232/ 3200]\n",
            "loss: 1.372517  [ 1248/ 3200]\n",
            "loss: 1.314178  [ 1264/ 3200]\n",
            "loss: 1.349483  [ 1280/ 3200]\n",
            "loss: 1.322781  [ 1296/ 3200]\n",
            "loss: 1.294033  [ 1312/ 3200]\n",
            "loss: 1.377623  [ 1328/ 3200]\n",
            "loss: 1.349239  [ 1344/ 3200]\n",
            "loss: 1.379208  [ 1360/ 3200]\n",
            "loss: 1.331430  [ 1376/ 3200]\n",
            "loss: 1.376742  [ 1392/ 3200]\n",
            "loss: 1.329970  [ 1408/ 3200]\n",
            "loss: 1.358523  [ 1424/ 3200]\n",
            "loss: 1.315892  [ 1440/ 3200]\n",
            "loss: 1.300885  [ 1456/ 3200]\n",
            "loss: 1.331077  [ 1472/ 3200]\n",
            "loss: 1.316535  [ 1488/ 3200]\n",
            "loss: 1.295072  [ 1504/ 3200]\n",
            "loss: 1.329633  [ 1520/ 3200]\n",
            "loss: 1.332126  [ 1536/ 3200]\n",
            "loss: 1.329303  [ 1552/ 3200]\n",
            "loss: 1.320582  [ 1568/ 3200]\n",
            "loss: 1.317374  [ 1584/ 3200]\n",
            "loss: 1.278917  [ 1600/ 3200]\n",
            "loss: 1.328576  [ 1616/ 3200]\n",
            "loss: 1.345681  [ 1632/ 3200]\n",
            "loss: 1.356916  [ 1648/ 3200]\n",
            "loss: 1.335933  [ 1664/ 3200]\n",
            "loss: 1.310216  [ 1680/ 3200]\n",
            "loss: 1.292181  [ 1696/ 3200]\n",
            "loss: 1.288147  [ 1712/ 3200]\n",
            "loss: 1.331593  [ 1728/ 3200]\n",
            "loss: 1.362280  [ 1744/ 3200]\n",
            "loss: 1.304465  [ 1760/ 3200]\n",
            "loss: 1.333145  [ 1776/ 3200]\n",
            "loss: 1.305500  [ 1792/ 3200]\n",
            "loss: 1.277691  [ 1808/ 3200]\n",
            "loss: 1.401322  [ 1824/ 3200]\n",
            "loss: 1.315663  [ 1840/ 3200]\n",
            "loss: 1.321239  [ 1856/ 3200]\n",
            "loss: 1.329489  [ 1872/ 3200]\n",
            "loss: 1.354253  [ 1888/ 3200]\n",
            "loss: 1.328278  [ 1904/ 3200]\n",
            "loss: 1.305034  [ 1920/ 3200]\n",
            "loss: 1.307561  [ 1936/ 3200]\n",
            "loss: 1.323613  [ 1952/ 3200]\n",
            "loss: 1.313571  [ 1968/ 3200]\n",
            "loss: 1.317973  [ 1984/ 3200]\n",
            "loss: 1.338282  [ 2000/ 3200]\n",
            "loss: 1.327507  [ 2016/ 3200]\n",
            "loss: 1.346603  [ 2032/ 3200]\n",
            "loss: 1.328679  [ 2048/ 3200]\n",
            "loss: 1.344747  [ 2064/ 3200]\n",
            "loss: 1.294460  [ 2080/ 3200]\n",
            "loss: 1.323415  [ 2096/ 3200]\n",
            "loss: 1.314838  [ 2112/ 3200]\n",
            "loss: 1.333881  [ 2128/ 3200]\n",
            "loss: 1.336380  [ 2144/ 3200]\n",
            "loss: 1.292690  [ 2160/ 3200]\n",
            "loss: 1.338376  [ 2176/ 3200]\n",
            "loss: 1.365263  [ 2192/ 3200]\n",
            "loss: 1.340103  [ 2208/ 3200]\n",
            "loss: 1.302810  [ 2224/ 3200]\n",
            "loss: 1.307137  [ 2240/ 3200]\n",
            "loss: 1.335243  [ 2256/ 3200]\n",
            "loss: 1.293095  [ 2272/ 3200]\n",
            "loss: 1.322538  [ 2288/ 3200]\n",
            "loss: 1.323652  [ 2304/ 3200]\n",
            "loss: 1.328559  [ 2320/ 3200]\n",
            "loss: 1.302062  [ 2336/ 3200]\n",
            "loss: 1.324821  [ 2352/ 3200]\n",
            "loss: 1.306970  [ 2368/ 3200]\n",
            "loss: 1.314093  [ 2384/ 3200]\n",
            "loss: 1.349554  [ 2400/ 3200]\n",
            "loss: 1.336813  [ 2416/ 3200]\n",
            "loss: 1.325529  [ 2432/ 3200]\n",
            "loss: 1.339455  [ 2448/ 3200]\n",
            "loss: 1.302100  [ 2464/ 3200]\n",
            "loss: 1.368038  [ 2480/ 3200]\n",
            "loss: 1.320384  [ 2496/ 3200]\n",
            "loss: 1.323960  [ 2512/ 3200]\n",
            "loss: 1.300297  [ 2528/ 3200]\n",
            "loss: 1.313433  [ 2544/ 3200]\n",
            "loss: 1.334675  [ 2560/ 3200]\n",
            "loss: 1.317985  [ 2576/ 3200]\n",
            "loss: 1.323486  [ 2592/ 3200]\n",
            "loss: 1.337640  [ 2608/ 3200]\n",
            "loss: 1.326231  [ 2624/ 3200]\n",
            "loss: 1.328845  [ 2640/ 3200]\n",
            "loss: 1.309257  [ 2656/ 3200]\n",
            "loss: 1.300080  [ 2672/ 3200]\n",
            "loss: 1.302236  [ 2688/ 3200]\n",
            "loss: 1.359362  [ 2704/ 3200]\n",
            "loss: 1.318855  [ 2720/ 3200]\n",
            "loss: 1.342700  [ 2736/ 3200]\n",
            "loss: 1.315616  [ 2752/ 3200]\n",
            "loss: 1.301906  [ 2768/ 3200]\n",
            "loss: 1.305468  [ 2784/ 3200]\n",
            "loss: 1.363739  [ 2800/ 3200]\n",
            "loss: 1.337616  [ 2816/ 3200]\n",
            "loss: 1.307039  [ 2832/ 3200]\n",
            "loss: 1.317636  [ 2848/ 3200]\n",
            "loss: 1.359083  [ 2864/ 3200]\n",
            "loss: 1.290082  [ 2880/ 3200]\n",
            "loss: 1.311341  [ 2896/ 3200]\n",
            "loss: 1.326962  [ 2912/ 3200]\n",
            "loss: 1.268226  [ 2928/ 3200]\n",
            "loss: 1.371754  [ 2944/ 3200]\n",
            "loss: 1.309263  [ 2960/ 3200]\n",
            "loss: 1.345410  [ 2976/ 3200]\n",
            "loss: 1.309563  [ 2992/ 3200]\n",
            "loss: 1.291979  [ 3008/ 3200]\n",
            "loss: 1.330155  [ 3024/ 3200]\n",
            "loss: 1.353340  [ 3040/ 3200]\n",
            "loss: 1.334567  [ 3056/ 3200]\n",
            "loss: 1.295735  [ 3072/ 3200]\n",
            "loss: 1.331195  [ 3088/ 3200]\n",
            "loss: 1.344456  [ 3104/ 3200]\n",
            "loss: 1.333785  [ 3120/ 3200]\n",
            "loss: 1.337295  [ 3136/ 3200]\n",
            "loss: 1.340908  [ 3152/ 3200]\n",
            "loss: 1.374809  [ 3168/ 3200]\n",
            "loss: 1.318987  [ 3184/ 3200]\n",
            "current epoch: 10\n",
            "\n",
            "loss: 1.298459  [    0/ 3200]\n",
            "loss: 1.342296  [   16/ 3200]\n",
            "loss: 1.320931  [   32/ 3200]\n",
            "loss: 1.333545  [   48/ 3200]\n",
            "loss: 1.322230  [   64/ 3200]\n",
            "loss: 1.341382  [   80/ 3200]\n",
            "loss: 1.310795  [   96/ 3200]\n",
            "loss: 1.333393  [  112/ 3200]\n",
            "loss: 1.328291  [  128/ 3200]\n",
            "loss: 1.335663  [  144/ 3200]\n",
            "loss: 1.317122  [  160/ 3200]\n",
            "loss: 1.358448  [  176/ 3200]\n",
            "loss: 1.316848  [  192/ 3200]\n",
            "loss: 1.297588  [  208/ 3200]\n",
            "loss: 1.295100  [  224/ 3200]\n",
            "loss: 1.333651  [  240/ 3200]\n",
            "loss: 1.365381  [  256/ 3200]\n",
            "loss: 1.354148  [  272/ 3200]\n",
            "loss: 1.366623  [  288/ 3200]\n",
            "loss: 1.331470  [  304/ 3200]\n",
            "loss: 1.325847  [  320/ 3200]\n",
            "loss: 1.300591  [  336/ 3200]\n",
            "loss: 1.319787  [  352/ 3200]\n",
            "loss: 1.295323  [  368/ 3200]\n",
            "loss: 1.346552  [  384/ 3200]\n",
            "loss: 1.361953  [  400/ 3200]\n",
            "loss: 1.352207  [  416/ 3200]\n",
            "loss: 1.306163  [  432/ 3200]\n",
            "loss: 1.333076  [  448/ 3200]\n",
            "loss: 1.293781  [  464/ 3200]\n",
            "loss: 1.318525  [  480/ 3200]\n",
            "loss: 1.317003  [  496/ 3200]\n",
            "loss: 1.286001  [  512/ 3200]\n",
            "loss: 1.370730  [  528/ 3200]\n",
            "loss: 1.333731  [  544/ 3200]\n",
            "loss: 1.341700  [  560/ 3200]\n",
            "loss: 1.341105  [  576/ 3200]\n",
            "loss: 1.310293  [  592/ 3200]\n",
            "loss: 1.332615  [  608/ 3200]\n",
            "loss: 1.275844  [  624/ 3200]\n",
            "loss: 1.321545  [  640/ 3200]\n",
            "loss: 1.290715  [  656/ 3200]\n",
            "loss: 1.348322  [  672/ 3200]\n",
            "loss: 1.332625  [  688/ 3200]\n",
            "loss: 1.320218  [  704/ 3200]\n",
            "loss: 1.330892  [  720/ 3200]\n",
            "loss: 1.317505  [  736/ 3200]\n",
            "loss: 1.386027  [  752/ 3200]\n",
            "loss: 1.329275  [  768/ 3200]\n",
            "loss: 1.354784  [  784/ 3200]\n",
            "loss: 1.327073  [  800/ 3200]\n",
            "loss: 1.326330  [  816/ 3200]\n",
            "loss: 1.348162  [  832/ 3200]\n",
            "loss: 1.341871  [  848/ 3200]\n",
            "loss: 1.312790  [  864/ 3200]\n",
            "loss: 1.309945  [  880/ 3200]\n",
            "loss: 1.345709  [  896/ 3200]\n",
            "loss: 1.309946  [  912/ 3200]\n",
            "loss: 1.290111  [  928/ 3200]\n",
            "loss: 1.303912  [  944/ 3200]\n",
            "loss: 1.387400  [  960/ 3200]\n",
            "loss: 1.328432  [  976/ 3200]\n",
            "loss: 1.298382  [  992/ 3200]\n",
            "loss: 1.297576  [ 1008/ 3200]\n",
            "loss: 1.336228  [ 1024/ 3200]\n",
            "loss: 1.274817  [ 1040/ 3200]\n",
            "loss: 1.287082  [ 1056/ 3200]\n",
            "loss: 1.311790  [ 1072/ 3200]\n",
            "loss: 1.326503  [ 1088/ 3200]\n",
            "loss: 1.371183  [ 1104/ 3200]\n",
            "loss: 1.339051  [ 1120/ 3200]\n",
            "loss: 1.347848  [ 1136/ 3200]\n",
            "loss: 1.348031  [ 1152/ 3200]\n",
            "loss: 1.298499  [ 1168/ 3200]\n",
            "loss: 1.287259  [ 1184/ 3200]\n",
            "loss: 1.337834  [ 1200/ 3200]\n",
            "loss: 1.320897  [ 1216/ 3200]\n",
            "loss: 1.337320  [ 1232/ 3200]\n",
            "loss: 1.276812  [ 1248/ 3200]\n",
            "loss: 1.305651  [ 1264/ 3200]\n",
            "loss: 1.329492  [ 1280/ 3200]\n",
            "loss: 1.340825  [ 1296/ 3200]\n",
            "loss: 1.285387  [ 1312/ 3200]\n",
            "loss: 1.336449  [ 1328/ 3200]\n",
            "loss: 1.322995  [ 1344/ 3200]\n",
            "loss: 1.239909  [ 1360/ 3200]\n",
            "loss: 1.308145  [ 1376/ 3200]\n",
            "loss: 1.321735  [ 1392/ 3200]\n",
            "loss: 1.322723  [ 1408/ 3200]\n",
            "loss: 1.263304  [ 1424/ 3200]\n",
            "loss: 1.309340  [ 1440/ 3200]\n",
            "loss: 1.291510  [ 1456/ 3200]\n",
            "loss: 1.312276  [ 1472/ 3200]\n",
            "loss: 1.303282  [ 1488/ 3200]\n",
            "loss: 1.296121  [ 1504/ 3200]\n",
            "loss: 1.339972  [ 1520/ 3200]\n",
            "loss: 1.326560  [ 1536/ 3200]\n",
            "loss: 1.297656  [ 1552/ 3200]\n",
            "loss: 1.332320  [ 1568/ 3200]\n",
            "loss: 1.309747  [ 1584/ 3200]\n",
            "loss: 1.315600  [ 1600/ 3200]\n",
            "loss: 1.377195  [ 1616/ 3200]\n",
            "loss: 1.302785  [ 1632/ 3200]\n",
            "loss: 1.318764  [ 1648/ 3200]\n",
            "loss: 1.300774  [ 1664/ 3200]\n",
            "loss: 1.352037  [ 1680/ 3200]\n",
            "loss: 1.316684  [ 1696/ 3200]\n",
            "loss: 1.288749  [ 1712/ 3200]\n",
            "loss: 1.356923  [ 1728/ 3200]\n",
            "loss: 1.280072  [ 1744/ 3200]\n",
            "loss: 1.315946  [ 1760/ 3200]\n",
            "loss: 1.242544  [ 1776/ 3200]\n",
            "loss: 1.328866  [ 1792/ 3200]\n",
            "loss: 1.286051  [ 1808/ 3200]\n",
            "loss: 1.320922  [ 1824/ 3200]\n",
            "loss: 1.297560  [ 1840/ 3200]\n",
            "loss: 1.364773  [ 1856/ 3200]\n",
            "loss: 1.287324  [ 1872/ 3200]\n",
            "loss: 1.314519  [ 1888/ 3200]\n",
            "loss: 1.358422  [ 1904/ 3200]\n",
            "loss: 1.278820  [ 1920/ 3200]\n",
            "loss: 1.265808  [ 1936/ 3200]\n",
            "loss: 1.312337  [ 1952/ 3200]\n",
            "loss: 1.279324  [ 1968/ 3200]\n",
            "loss: 1.329620  [ 1984/ 3200]\n",
            "loss: 1.318847  [ 2000/ 3200]\n",
            "loss: 1.355611  [ 2016/ 3200]\n",
            "loss: 1.364702  [ 2032/ 3200]\n",
            "loss: 1.308381  [ 2048/ 3200]\n",
            "loss: 1.267626  [ 2064/ 3200]\n",
            "loss: 1.355134  [ 2080/ 3200]\n",
            "loss: 1.362397  [ 2096/ 3200]\n",
            "loss: 1.351253  [ 2112/ 3200]\n",
            "loss: 1.285520  [ 2128/ 3200]\n",
            "loss: 1.336397  [ 2144/ 3200]\n",
            "loss: 1.343191  [ 2160/ 3200]\n",
            "loss: 1.358162  [ 2176/ 3200]\n",
            "loss: 1.317033  [ 2192/ 3200]\n",
            "loss: 1.289602  [ 2208/ 3200]\n",
            "loss: 1.362461  [ 2224/ 3200]\n",
            "loss: 1.313169  [ 2240/ 3200]\n",
            "loss: 1.322403  [ 2256/ 3200]\n",
            "loss: 1.295996  [ 2272/ 3200]\n",
            "loss: 1.297872  [ 2288/ 3200]\n",
            "loss: 1.294033  [ 2304/ 3200]\n",
            "loss: 1.305917  [ 2320/ 3200]\n",
            "loss: 1.311661  [ 2336/ 3200]\n",
            "loss: 1.305059  [ 2352/ 3200]\n",
            "loss: 1.355156  [ 2368/ 3200]\n",
            "loss: 1.336270  [ 2384/ 3200]\n",
            "loss: 1.337454  [ 2400/ 3200]\n",
            "loss: 1.351840  [ 2416/ 3200]\n",
            "loss: 1.338709  [ 2432/ 3200]\n",
            "loss: 1.325892  [ 2448/ 3200]\n",
            "loss: 1.310537  [ 2464/ 3200]\n",
            "loss: 1.302625  [ 2480/ 3200]\n",
            "loss: 1.350073  [ 2496/ 3200]\n",
            "loss: 1.331690  [ 2512/ 3200]\n",
            "loss: 1.297510  [ 2528/ 3200]\n",
            "loss: 1.315945  [ 2544/ 3200]\n",
            "loss: 1.258055  [ 2560/ 3200]\n",
            "loss: 1.355001  [ 2576/ 3200]\n",
            "loss: 1.298184  [ 2592/ 3200]\n",
            "loss: 1.337698  [ 2608/ 3200]\n",
            "loss: 1.254303  [ 2624/ 3200]\n",
            "loss: 1.357731  [ 2640/ 3200]\n",
            "loss: 1.293674  [ 2656/ 3200]\n",
            "loss: 1.308801  [ 2672/ 3200]\n",
            "loss: 1.326059  [ 2688/ 3200]\n",
            "loss: 1.356423  [ 2704/ 3200]\n",
            "loss: 1.274620  [ 2720/ 3200]\n",
            "loss: 1.290479  [ 2736/ 3200]\n",
            "loss: 1.305534  [ 2752/ 3200]\n",
            "loss: 1.298752  [ 2768/ 3200]\n",
            "loss: 1.247881  [ 2784/ 3200]\n",
            "loss: 1.275803  [ 2800/ 3200]\n",
            "loss: 1.304181  [ 2816/ 3200]\n",
            "loss: 1.360431  [ 2832/ 3200]\n",
            "loss: 1.314106  [ 2848/ 3200]\n",
            "loss: 1.404607  [ 2864/ 3200]\n",
            "loss: 1.315270  [ 2880/ 3200]\n",
            "loss: 1.310081  [ 2896/ 3200]\n",
            "loss: 1.323551  [ 2912/ 3200]\n",
            "loss: 1.337528  [ 2928/ 3200]\n",
            "loss: 1.279375  [ 2944/ 3200]\n",
            "loss: 1.293937  [ 2960/ 3200]\n",
            "loss: 1.291264  [ 2976/ 3200]\n",
            "loss: 1.364282  [ 2992/ 3200]\n",
            "loss: 1.339235  [ 3008/ 3200]\n",
            "loss: 1.312583  [ 3024/ 3200]\n",
            "loss: 1.235990  [ 3040/ 3200]\n",
            "loss: 1.394560  [ 3056/ 3200]\n",
            "loss: 1.354402  [ 3072/ 3200]\n",
            "loss: 1.242985  [ 3088/ 3200]\n",
            "loss: 1.344688  [ 3104/ 3200]\n",
            "loss: 1.296767  [ 3120/ 3200]\n",
            "loss: 1.308567  [ 3136/ 3200]\n",
            "loss: 1.307435  [ 3152/ 3200]\n",
            "loss: 1.321719  [ 3168/ 3200]\n",
            "loss: 1.309369  [ 3184/ 3200]\n",
            "current epoch: 11\n",
            "\n",
            "loss: 1.312648  [    0/ 3200]\n",
            "loss: 1.349110  [   16/ 3200]\n",
            "loss: 1.284896  [   32/ 3200]\n",
            "loss: 1.307263  [   48/ 3200]\n",
            "loss: 1.304226  [   64/ 3200]\n",
            "loss: 1.302635  [   80/ 3200]\n",
            "loss: 1.375130  [   96/ 3200]\n",
            "loss: 1.335381  [  112/ 3200]\n",
            "loss: 1.287754  [  128/ 3200]\n",
            "loss: 1.357552  [  144/ 3200]\n",
            "loss: 1.291677  [  160/ 3200]\n",
            "loss: 1.343384  [  176/ 3200]\n",
            "loss: 1.315782  [  192/ 3200]\n",
            "loss: 1.290709  [  208/ 3200]\n",
            "loss: 1.302582  [  224/ 3200]\n",
            "loss: 1.366643  [  240/ 3200]\n",
            "loss: 1.326288  [  256/ 3200]\n",
            "loss: 1.320851  [  272/ 3200]\n",
            "loss: 1.337952  [  288/ 3200]\n",
            "loss: 1.316766  [  304/ 3200]\n",
            "loss: 1.346470  [  320/ 3200]\n",
            "loss: 1.295701  [  336/ 3200]\n",
            "loss: 1.364198  [  352/ 3200]\n",
            "loss: 1.306739  [  368/ 3200]\n",
            "loss: 1.306575  [  384/ 3200]\n",
            "loss: 1.316755  [  400/ 3200]\n",
            "loss: 1.263654  [  416/ 3200]\n",
            "loss: 1.259928  [  432/ 3200]\n",
            "loss: 1.269179  [  448/ 3200]\n",
            "loss: 1.269991  [  464/ 3200]\n",
            "loss: 1.314709  [  480/ 3200]\n",
            "loss: 1.357286  [  496/ 3200]\n",
            "loss: 1.308668  [  512/ 3200]\n",
            "loss: 1.376250  [  528/ 3200]\n",
            "loss: 1.326410  [  544/ 3200]\n",
            "loss: 1.320423  [  560/ 3200]\n",
            "loss: 1.284173  [  576/ 3200]\n",
            "loss: 1.333146  [  592/ 3200]\n",
            "loss: 1.249917  [  608/ 3200]\n",
            "loss: 1.305157  [  624/ 3200]\n",
            "loss: 1.365865  [  640/ 3200]\n",
            "loss: 1.299187  [  656/ 3200]\n",
            "loss: 1.332112  [  672/ 3200]\n",
            "loss: 1.231603  [  688/ 3200]\n",
            "loss: 1.374255  [  704/ 3200]\n",
            "loss: 1.344350  [  720/ 3200]\n",
            "loss: 1.280932  [  736/ 3200]\n",
            "loss: 1.323817  [  752/ 3200]\n",
            "loss: 1.297577  [  768/ 3200]\n",
            "loss: 1.324045  [  784/ 3200]\n",
            "loss: 1.301349  [  800/ 3200]\n",
            "loss: 1.332429  [  816/ 3200]\n",
            "loss: 1.307722  [  832/ 3200]\n",
            "loss: 1.317330  [  848/ 3200]\n",
            "loss: 1.326778  [  864/ 3200]\n",
            "loss: 1.349022  [  880/ 3200]\n",
            "loss: 1.262883  [  896/ 3200]\n",
            "loss: 1.349312  [  912/ 3200]\n",
            "loss: 1.320094  [  928/ 3200]\n",
            "loss: 1.311709  [  944/ 3200]\n",
            "loss: 1.313563  [  960/ 3200]\n",
            "loss: 1.267463  [  976/ 3200]\n",
            "loss: 1.333099  [  992/ 3200]\n",
            "loss: 1.338793  [ 1008/ 3200]\n",
            "loss: 1.319476  [ 1024/ 3200]\n",
            "loss: 1.310066  [ 1040/ 3200]\n",
            "loss: 1.254790  [ 1056/ 3200]\n",
            "loss: 1.289574  [ 1072/ 3200]\n",
            "loss: 1.282974  [ 1088/ 3200]\n",
            "loss: 1.307452  [ 1104/ 3200]\n",
            "loss: 1.276894  [ 1120/ 3200]\n",
            "loss: 1.353415  [ 1136/ 3200]\n",
            "loss: 1.302241  [ 1152/ 3200]\n",
            "loss: 1.337553  [ 1168/ 3200]\n",
            "loss: 1.320125  [ 1184/ 3200]\n",
            "loss: 1.313147  [ 1200/ 3200]\n",
            "loss: 1.305284  [ 1216/ 3200]\n",
            "loss: 1.332476  [ 1232/ 3200]\n",
            "loss: 1.279984  [ 1248/ 3200]\n",
            "loss: 1.311826  [ 1264/ 3200]\n",
            "loss: 1.268529  [ 1280/ 3200]\n",
            "loss: 1.396719  [ 1296/ 3200]\n",
            "loss: 1.299767  [ 1312/ 3200]\n",
            "loss: 1.306205  [ 1328/ 3200]\n",
            "loss: 1.302048  [ 1344/ 3200]\n",
            "loss: 1.347366  [ 1360/ 3200]\n",
            "loss: 1.296659  [ 1376/ 3200]\n",
            "loss: 1.289182  [ 1392/ 3200]\n",
            "loss: 1.301130  [ 1408/ 3200]\n",
            "loss: 1.330031  [ 1424/ 3200]\n",
            "loss: 1.333544  [ 1440/ 3200]\n",
            "loss: 1.328006  [ 1456/ 3200]\n",
            "loss: 1.327117  [ 1472/ 3200]\n",
            "loss: 1.282643  [ 1488/ 3200]\n",
            "loss: 1.350933  [ 1504/ 3200]\n",
            "loss: 1.266394  [ 1520/ 3200]\n",
            "loss: 1.313289  [ 1536/ 3200]\n",
            "loss: 1.326671  [ 1552/ 3200]\n",
            "loss: 1.411569  [ 1568/ 3200]\n",
            "loss: 1.305722  [ 1584/ 3200]\n",
            "loss: 1.296113  [ 1600/ 3200]\n",
            "loss: 1.254570  [ 1616/ 3200]\n",
            "loss: 1.328641  [ 1632/ 3200]\n",
            "loss: 1.269892  [ 1648/ 3200]\n",
            "loss: 1.269381  [ 1664/ 3200]\n",
            "loss: 1.294262  [ 1680/ 3200]\n",
            "loss: 1.338091  [ 1696/ 3200]\n",
            "loss: 1.303878  [ 1712/ 3200]\n",
            "loss: 1.322657  [ 1728/ 3200]\n",
            "loss: 1.279641  [ 1744/ 3200]\n",
            "loss: 1.236452  [ 1760/ 3200]\n",
            "loss: 1.319903  [ 1776/ 3200]\n",
            "loss: 1.220479  [ 1792/ 3200]\n",
            "loss: 1.302593  [ 1808/ 3200]\n",
            "loss: 1.326807  [ 1824/ 3200]\n",
            "loss: 1.275934  [ 1840/ 3200]\n",
            "loss: 1.265956  [ 1856/ 3200]\n",
            "loss: 1.260558  [ 1872/ 3200]\n",
            "loss: 1.310412  [ 1888/ 3200]\n",
            "loss: 1.274310  [ 1904/ 3200]\n",
            "loss: 1.341755  [ 1920/ 3200]\n",
            "loss: 1.307182  [ 1936/ 3200]\n",
            "loss: 1.262974  [ 1952/ 3200]\n",
            "loss: 1.279042  [ 1968/ 3200]\n",
            "loss: 1.330153  [ 1984/ 3200]\n",
            "loss: 1.303653  [ 2000/ 3200]\n",
            "loss: 1.309184  [ 2016/ 3200]\n",
            "loss: 1.292663  [ 2032/ 3200]\n",
            "loss: 1.294850  [ 2048/ 3200]\n",
            "loss: 1.239709  [ 2064/ 3200]\n",
            "loss: 1.235276  [ 2080/ 3200]\n",
            "loss: 1.221156  [ 2096/ 3200]\n",
            "loss: 1.335325  [ 2112/ 3200]\n",
            "loss: 1.383182  [ 2128/ 3200]\n",
            "loss: 1.265119  [ 2144/ 3200]\n",
            "loss: 1.284406  [ 2160/ 3200]\n",
            "loss: 1.348439  [ 2176/ 3200]\n",
            "loss: 1.413958  [ 2192/ 3200]\n",
            "loss: 1.285696  [ 2208/ 3200]\n",
            "loss: 1.316580  [ 2224/ 3200]\n",
            "loss: 1.298933  [ 2240/ 3200]\n",
            "loss: 1.261829  [ 2256/ 3200]\n",
            "loss: 1.347487  [ 2272/ 3200]\n",
            "loss: 1.277948  [ 2288/ 3200]\n",
            "loss: 1.316887  [ 2304/ 3200]\n",
            "loss: 1.337724  [ 2320/ 3200]\n",
            "loss: 1.332218  [ 2336/ 3200]\n",
            "loss: 1.254336  [ 2352/ 3200]\n",
            "loss: 1.343563  [ 2368/ 3200]\n",
            "loss: 1.281001  [ 2384/ 3200]\n",
            "loss: 1.353602  [ 2400/ 3200]\n",
            "loss: 1.343036  [ 2416/ 3200]\n",
            "loss: 1.338368  [ 2432/ 3200]\n",
            "loss: 1.285803  [ 2448/ 3200]\n",
            "loss: 1.276463  [ 2464/ 3200]\n",
            "loss: 1.333878  [ 2480/ 3200]\n",
            "loss: 1.357794  [ 2496/ 3200]\n",
            "loss: 1.273281  [ 2512/ 3200]\n",
            "loss: 1.276837  [ 2528/ 3200]\n",
            "loss: 1.282827  [ 2544/ 3200]\n",
            "loss: 1.304329  [ 2560/ 3200]\n",
            "loss: 1.341273  [ 2576/ 3200]\n",
            "loss: 1.310663  [ 2592/ 3200]\n",
            "loss: 1.270420  [ 2608/ 3200]\n",
            "loss: 1.290591  [ 2624/ 3200]\n",
            "loss: 1.288308  [ 2640/ 3200]\n",
            "loss: 1.314897  [ 2656/ 3200]\n",
            "loss: 1.286660  [ 2672/ 3200]\n",
            "loss: 1.296046  [ 2688/ 3200]\n",
            "loss: 1.235771  [ 2704/ 3200]\n",
            "loss: 1.375055  [ 2720/ 3200]\n",
            "loss: 1.291832  [ 2736/ 3200]\n",
            "loss: 1.230795  [ 2752/ 3200]\n",
            "loss: 1.347735  [ 2768/ 3200]\n",
            "loss: 1.341817  [ 2784/ 3200]\n",
            "loss: 1.288872  [ 2800/ 3200]\n",
            "loss: 1.327137  [ 2816/ 3200]\n",
            "loss: 1.310649  [ 2832/ 3200]\n",
            "loss: 1.308287  [ 2848/ 3200]\n",
            "loss: 1.299779  [ 2864/ 3200]\n",
            "loss: 1.351593  [ 2880/ 3200]\n",
            "loss: 1.312909  [ 2896/ 3200]\n",
            "loss: 1.221607  [ 2912/ 3200]\n",
            "loss: 1.207088  [ 2928/ 3200]\n",
            "loss: 1.301356  [ 2944/ 3200]\n",
            "loss: 1.303416  [ 2960/ 3200]\n",
            "loss: 1.314085  [ 2976/ 3200]\n",
            "loss: 1.296712  [ 2992/ 3200]\n",
            "loss: 1.249514  [ 3008/ 3200]\n",
            "loss: 1.312597  [ 3024/ 3200]\n",
            "loss: 1.292453  [ 3040/ 3200]\n",
            "loss: 1.261304  [ 3056/ 3200]\n",
            "loss: 1.310626  [ 3072/ 3200]\n",
            "loss: 1.307260  [ 3088/ 3200]\n",
            "loss: 1.286637  [ 3104/ 3200]\n",
            "loss: 1.316248  [ 3120/ 3200]\n",
            "loss: 1.361968  [ 3136/ 3200]\n",
            "loss: 1.286419  [ 3152/ 3200]\n",
            "loss: 1.343574  [ 3168/ 3200]\n",
            "loss: 1.333761  [ 3184/ 3200]\n",
            "current epoch: 12\n",
            "\n",
            "loss: 1.305858  [    0/ 3200]\n",
            "loss: 1.294361  [   16/ 3200]\n",
            "loss: 1.267951  [   32/ 3200]\n",
            "loss: 1.208630  [   48/ 3200]\n",
            "loss: 1.288095  [   64/ 3200]\n",
            "loss: 1.313204  [   80/ 3200]\n",
            "loss: 1.260839  [   96/ 3200]\n",
            "loss: 1.286002  [  112/ 3200]\n",
            "loss: 1.298244  [  128/ 3200]\n",
            "loss: 1.300561  [  144/ 3200]\n",
            "loss: 1.324379  [  160/ 3200]\n",
            "loss: 1.288378  [  176/ 3200]\n",
            "loss: 1.322324  [  192/ 3200]\n",
            "loss: 1.268993  [  208/ 3200]\n",
            "loss: 1.355953  [  224/ 3200]\n",
            "loss: 1.304527  [  240/ 3200]\n",
            "loss: 1.349494  [  256/ 3200]\n",
            "loss: 1.310340  [  272/ 3200]\n",
            "loss: 1.340986  [  288/ 3200]\n",
            "loss: 1.229811  [  304/ 3200]\n",
            "loss: 1.355103  [  320/ 3200]\n",
            "loss: 1.290034  [  336/ 3200]\n",
            "loss: 1.290518  [  352/ 3200]\n",
            "loss: 1.322895  [  368/ 3200]\n",
            "loss: 1.281754  [  384/ 3200]\n",
            "loss: 1.353790  [  400/ 3200]\n",
            "loss: 1.361497  [  416/ 3200]\n",
            "loss: 1.293261  [  432/ 3200]\n",
            "loss: 1.305291  [  448/ 3200]\n",
            "loss: 1.271846  [  464/ 3200]\n",
            "loss: 1.296500  [  480/ 3200]\n",
            "loss: 1.221747  [  496/ 3200]\n",
            "loss: 1.312945  [  512/ 3200]\n",
            "loss: 1.361510  [  528/ 3200]\n",
            "loss: 1.275371  [  544/ 3200]\n",
            "loss: 1.318278  [  560/ 3200]\n",
            "loss: 1.295828  [  576/ 3200]\n",
            "loss: 1.342047  [  592/ 3200]\n",
            "loss: 1.276336  [  608/ 3200]\n",
            "loss: 1.300511  [  624/ 3200]\n",
            "loss: 1.303436  [  640/ 3200]\n",
            "loss: 1.275413  [  656/ 3200]\n",
            "loss: 1.273620  [  672/ 3200]\n",
            "loss: 1.338694  [  688/ 3200]\n",
            "loss: 1.349497  [  704/ 3200]\n",
            "loss: 1.290602  [  720/ 3200]\n",
            "loss: 1.287688  [  736/ 3200]\n",
            "loss: 1.284525  [  752/ 3200]\n",
            "loss: 1.270247  [  768/ 3200]\n",
            "loss: 1.324957  [  784/ 3200]\n",
            "loss: 1.277283  [  800/ 3200]\n",
            "loss: 1.270221  [  816/ 3200]\n",
            "loss: 1.248462  [  832/ 3200]\n",
            "loss: 1.257378  [  848/ 3200]\n",
            "loss: 1.262805  [  864/ 3200]\n",
            "loss: 1.345388  [  880/ 3200]\n",
            "loss: 1.294769  [  896/ 3200]\n",
            "loss: 1.278804  [  912/ 3200]\n",
            "loss: 1.232080  [  928/ 3200]\n",
            "loss: 1.284291  [  944/ 3200]\n",
            "loss: 1.334705  [  960/ 3200]\n",
            "loss: 1.267129  [  976/ 3200]\n",
            "loss: 1.254120  [  992/ 3200]\n",
            "loss: 1.223992  [ 1008/ 3200]\n",
            "loss: 1.275757  [ 1024/ 3200]\n",
            "loss: 1.219326  [ 1040/ 3200]\n",
            "loss: 1.286558  [ 1056/ 3200]\n",
            "loss: 1.313182  [ 1072/ 3200]\n",
            "loss: 1.247129  [ 1088/ 3200]\n",
            "loss: 1.267928  [ 1104/ 3200]\n",
            "loss: 1.197368  [ 1120/ 3200]\n",
            "loss: 1.248101  [ 1136/ 3200]\n",
            "loss: 1.268994  [ 1152/ 3200]\n",
            "loss: 1.301943  [ 1168/ 3200]\n",
            "loss: 1.178949  [ 1184/ 3200]\n",
            "loss: 1.326728  [ 1200/ 3200]\n",
            "loss: 1.242002  [ 1216/ 3200]\n",
            "loss: 1.261307  [ 1232/ 3200]\n",
            "loss: 1.294986  [ 1248/ 3200]\n",
            "loss: 1.350234  [ 1264/ 3200]\n",
            "loss: 1.223431  [ 1280/ 3200]\n",
            "loss: 1.346811  [ 1296/ 3200]\n",
            "loss: 1.345354  [ 1312/ 3200]\n",
            "loss: 1.277922  [ 1328/ 3200]\n",
            "loss: 1.267582  [ 1344/ 3200]\n",
            "loss: 1.321139  [ 1360/ 3200]\n",
            "loss: 1.334430  [ 1376/ 3200]\n",
            "loss: 1.297477  [ 1392/ 3200]\n",
            "loss: 1.315898  [ 1408/ 3200]\n",
            "loss: 1.339512  [ 1424/ 3200]\n",
            "loss: 1.300816  [ 1440/ 3200]\n",
            "loss: 1.270564  [ 1456/ 3200]\n",
            "loss: 1.223882  [ 1472/ 3200]\n",
            "loss: 1.401718  [ 1488/ 3200]\n",
            "loss: 1.217689  [ 1504/ 3200]\n",
            "loss: 1.236652  [ 1520/ 3200]\n",
            "loss: 1.297505  [ 1536/ 3200]\n",
            "loss: 1.320279  [ 1552/ 3200]\n",
            "loss: 1.335629  [ 1568/ 3200]\n",
            "loss: 1.287743  [ 1584/ 3200]\n",
            "loss: 1.277536  [ 1600/ 3200]\n",
            "loss: 1.293556  [ 1616/ 3200]\n",
            "loss: 1.287056  [ 1632/ 3200]\n",
            "loss: 1.361065  [ 1648/ 3200]\n",
            "loss: 1.295907  [ 1664/ 3200]\n",
            "loss: 1.311869  [ 1680/ 3200]\n",
            "loss: 1.325271  [ 1696/ 3200]\n",
            "loss: 1.325121  [ 1712/ 3200]\n",
            "loss: 1.315068  [ 1728/ 3200]\n",
            "loss: 1.291236  [ 1744/ 3200]\n",
            "loss: 1.271286  [ 1760/ 3200]\n",
            "loss: 1.315832  [ 1776/ 3200]\n",
            "loss: 1.319431  [ 1792/ 3200]\n",
            "loss: 1.272681  [ 1808/ 3200]\n",
            "loss: 1.296766  [ 1824/ 3200]\n",
            "loss: 1.212243  [ 1840/ 3200]\n",
            "loss: 1.326303  [ 1856/ 3200]\n",
            "loss: 1.288269  [ 1872/ 3200]\n",
            "loss: 1.265412  [ 1888/ 3200]\n",
            "loss: 1.309471  [ 1904/ 3200]\n",
            "loss: 1.294409  [ 1920/ 3200]\n",
            "loss: 1.271011  [ 1936/ 3200]\n",
            "loss: 1.272600  [ 1952/ 3200]\n",
            "loss: 1.299008  [ 1968/ 3200]\n",
            "loss: 1.254544  [ 1984/ 3200]\n",
            "loss: 1.279078  [ 2000/ 3200]\n",
            "loss: 1.258498  [ 2016/ 3200]\n",
            "loss: 1.324683  [ 2032/ 3200]\n",
            "loss: 1.309829  [ 2048/ 3200]\n",
            "loss: 1.287501  [ 2064/ 3200]\n",
            "loss: 1.292511  [ 2080/ 3200]\n",
            "loss: 1.381298  [ 2096/ 3200]\n",
            "loss: 1.273082  [ 2112/ 3200]\n",
            "loss: 1.260960  [ 2128/ 3200]\n",
            "loss: 1.224778  [ 2144/ 3200]\n",
            "loss: 1.251962  [ 2160/ 3200]\n",
            "loss: 1.301577  [ 2176/ 3200]\n",
            "loss: 1.319790  [ 2192/ 3200]\n",
            "loss: 1.288981  [ 2208/ 3200]\n",
            "loss: 1.270712  [ 2224/ 3200]\n",
            "loss: 1.300221  [ 2240/ 3200]\n",
            "loss: 1.276489  [ 2256/ 3200]\n",
            "loss: 1.246347  [ 2272/ 3200]\n",
            "loss: 1.249738  [ 2288/ 3200]\n",
            "loss: 1.260573  [ 2304/ 3200]\n",
            "loss: 1.319854  [ 2320/ 3200]\n",
            "loss: 1.429603  [ 2336/ 3200]\n",
            "loss: 1.270670  [ 2352/ 3200]\n",
            "loss: 1.253053  [ 2368/ 3200]\n",
            "loss: 1.227400  [ 2384/ 3200]\n",
            "loss: 1.290937  [ 2400/ 3200]\n",
            "loss: 1.280435  [ 2416/ 3200]\n",
            "loss: 1.211926  [ 2432/ 3200]\n",
            "loss: 1.298710  [ 2448/ 3200]\n",
            "loss: 1.297503  [ 2464/ 3200]\n",
            "loss: 1.284261  [ 2480/ 3200]\n",
            "loss: 1.314141  [ 2496/ 3200]\n",
            "loss: 1.303607  [ 2512/ 3200]\n",
            "loss: 1.281244  [ 2528/ 3200]\n",
            "loss: 1.347447  [ 2544/ 3200]\n",
            "loss: 1.284482  [ 2560/ 3200]\n",
            "loss: 1.319505  [ 2576/ 3200]\n",
            "loss: 1.300391  [ 2592/ 3200]\n",
            "loss: 1.333937  [ 2608/ 3200]\n",
            "loss: 1.292915  [ 2624/ 3200]\n",
            "loss: 1.300479  [ 2640/ 3200]\n",
            "loss: 1.317002  [ 2656/ 3200]\n",
            "loss: 1.277359  [ 2672/ 3200]\n",
            "loss: 1.254568  [ 2688/ 3200]\n",
            "loss: 1.326840  [ 2704/ 3200]\n",
            "loss: 1.279661  [ 2720/ 3200]\n",
            "loss: 1.273173  [ 2736/ 3200]\n",
            "loss: 1.396822  [ 2752/ 3200]\n",
            "loss: 1.295804  [ 2768/ 3200]\n",
            "loss: 1.309403  [ 2784/ 3200]\n",
            "loss: 1.301373  [ 2800/ 3200]\n",
            "loss: 1.283740  [ 2816/ 3200]\n",
            "loss: 1.278941  [ 2832/ 3200]\n",
            "loss: 1.316005  [ 2848/ 3200]\n",
            "loss: 1.286590  [ 2864/ 3200]\n",
            "loss: 1.283495  [ 2880/ 3200]\n",
            "loss: 1.295907  [ 2896/ 3200]\n",
            "loss: 1.286352  [ 2912/ 3200]\n",
            "loss: 1.325764  [ 2928/ 3200]\n",
            "loss: 1.261920  [ 2944/ 3200]\n",
            "loss: 1.332733  [ 2960/ 3200]\n",
            "loss: 1.313899  [ 2976/ 3200]\n",
            "loss: 1.328327  [ 2992/ 3200]\n",
            "loss: 1.308737  [ 3008/ 3200]\n",
            "loss: 1.302372  [ 3024/ 3200]\n",
            "loss: 1.288407  [ 3040/ 3200]\n",
            "loss: 1.253931  [ 3056/ 3200]\n",
            "loss: 1.252863  [ 3072/ 3200]\n",
            "loss: 1.369705  [ 3088/ 3200]\n",
            "loss: 1.325530  [ 3104/ 3200]\n",
            "loss: 1.282031  [ 3120/ 3200]\n",
            "loss: 1.281112  [ 3136/ 3200]\n",
            "loss: 1.261360  [ 3152/ 3200]\n",
            "loss: 1.282242  [ 3168/ 3200]\n",
            "loss: 1.266572  [ 3184/ 3200]\n",
            "current epoch: 13\n",
            "\n",
            "loss: 1.283993  [    0/ 3200]\n",
            "loss: 1.334921  [   16/ 3200]\n",
            "loss: 1.293342  [   32/ 3200]\n",
            "loss: 1.273485  [   48/ 3200]\n",
            "loss: 1.240676  [   64/ 3200]\n",
            "loss: 1.309758  [   80/ 3200]\n",
            "loss: 1.287276  [   96/ 3200]\n",
            "loss: 1.263778  [  112/ 3200]\n",
            "loss: 1.302422  [  128/ 3200]\n",
            "loss: 1.294717  [  144/ 3200]\n",
            "loss: 1.305131  [  160/ 3200]\n",
            "loss: 1.257357  [  176/ 3200]\n",
            "loss: 1.254272  [  192/ 3200]\n",
            "loss: 1.231837  [  208/ 3200]\n",
            "loss: 1.220610  [  224/ 3200]\n",
            "loss: 1.171109  [  240/ 3200]\n",
            "loss: 1.254653  [  256/ 3200]\n",
            "loss: 1.364882  [  272/ 3200]\n",
            "loss: 1.317416  [  288/ 3200]\n",
            "loss: 1.227789  [  304/ 3200]\n",
            "loss: 1.339216  [  320/ 3200]\n",
            "loss: 1.175875  [  336/ 3200]\n",
            "loss: 1.241191  [  352/ 3200]\n",
            "loss: 1.243886  [  368/ 3200]\n",
            "loss: 1.300005  [  384/ 3200]\n",
            "loss: 1.314067  [  400/ 3200]\n",
            "loss: 1.349316  [  416/ 3200]\n",
            "loss: 1.282251  [  432/ 3200]\n",
            "loss: 1.274939  [  448/ 3200]\n",
            "loss: 1.190280  [  464/ 3200]\n",
            "loss: 1.267180  [  480/ 3200]\n",
            "loss: 1.406033  [  496/ 3200]\n",
            "loss: 1.260458  [  512/ 3200]\n",
            "loss: 1.288683  [  528/ 3200]\n",
            "loss: 1.328706  [  544/ 3200]\n",
            "loss: 1.253569  [  560/ 3200]\n",
            "loss: 1.267177  [  576/ 3200]\n",
            "loss: 1.315856  [  592/ 3200]\n",
            "loss: 1.267448  [  608/ 3200]\n",
            "loss: 1.274863  [  624/ 3200]\n",
            "loss: 1.379491  [  640/ 3200]\n",
            "loss: 1.295321  [  656/ 3200]\n",
            "loss: 1.291919  [  672/ 3200]\n",
            "loss: 1.315994  [  688/ 3200]\n",
            "loss: 1.191513  [  704/ 3200]\n",
            "loss: 1.211562  [  720/ 3200]\n",
            "loss: 1.300069  [  736/ 3200]\n",
            "loss: 1.293216  [  752/ 3200]\n",
            "loss: 1.307701  [  768/ 3200]\n",
            "loss: 1.335193  [  784/ 3200]\n",
            "loss: 1.370246  [  800/ 3200]\n",
            "loss: 1.277651  [  816/ 3200]\n",
            "loss: 1.211734  [  832/ 3200]\n",
            "loss: 1.264640  [  848/ 3200]\n",
            "loss: 1.297660  [  864/ 3200]\n",
            "loss: 1.343944  [  880/ 3200]\n",
            "loss: 1.242776  [  896/ 3200]\n",
            "loss: 1.255003  [  912/ 3200]\n",
            "loss: 1.296616  [  928/ 3200]\n",
            "loss: 1.316919  [  944/ 3200]\n",
            "loss: 1.271648  [  960/ 3200]\n",
            "loss: 1.312155  [  976/ 3200]\n",
            "loss: 1.264733  [  992/ 3200]\n",
            "loss: 1.272776  [ 1008/ 3200]\n",
            "loss: 1.298943  [ 1024/ 3200]\n",
            "loss: 1.299786  [ 1040/ 3200]\n",
            "loss: 1.262485  [ 1056/ 3200]\n",
            "loss: 1.271128  [ 1072/ 3200]\n",
            "loss: 1.314543  [ 1088/ 3200]\n",
            "loss: 1.241441  [ 1104/ 3200]\n",
            "loss: 1.275761  [ 1120/ 3200]\n",
            "loss: 1.253322  [ 1136/ 3200]\n",
            "loss: 1.270820  [ 1152/ 3200]\n",
            "loss: 1.318324  [ 1168/ 3200]\n",
            "loss: 1.256670  [ 1184/ 3200]\n",
            "loss: 1.350733  [ 1200/ 3200]\n",
            "loss: 1.283758  [ 1216/ 3200]\n",
            "loss: 1.286529  [ 1232/ 3200]\n",
            "loss: 1.297401  [ 1248/ 3200]\n",
            "loss: 1.288921  [ 1264/ 3200]\n",
            "loss: 1.256647  [ 1280/ 3200]\n",
            "loss: 1.250393  [ 1296/ 3200]\n",
            "loss: 1.299168  [ 1312/ 3200]\n",
            "loss: 1.284821  [ 1328/ 3200]\n",
            "loss: 1.218770  [ 1344/ 3200]\n",
            "loss: 1.350249  [ 1360/ 3200]\n",
            "loss: 1.328828  [ 1376/ 3200]\n",
            "loss: 1.350933  [ 1392/ 3200]\n",
            "loss: 1.296812  [ 1408/ 3200]\n",
            "loss: 1.351934  [ 1424/ 3200]\n",
            "loss: 1.243610  [ 1440/ 3200]\n",
            "loss: 1.313074  [ 1456/ 3200]\n",
            "loss: 1.334107  [ 1472/ 3200]\n",
            "loss: 1.339616  [ 1488/ 3200]\n",
            "loss: 1.281588  [ 1504/ 3200]\n",
            "loss: 1.296908  [ 1520/ 3200]\n",
            "loss: 1.280988  [ 1536/ 3200]\n",
            "loss: 1.293276  [ 1552/ 3200]\n",
            "loss: 1.268542  [ 1568/ 3200]\n",
            "loss: 1.247812  [ 1584/ 3200]\n",
            "loss: 1.359308  [ 1600/ 3200]\n",
            "loss: 1.232734  [ 1616/ 3200]\n",
            "loss: 1.318809  [ 1632/ 3200]\n",
            "loss: 1.238776  [ 1648/ 3200]\n",
            "loss: 1.265955  [ 1664/ 3200]\n",
            "loss: 1.274159  [ 1680/ 3200]\n",
            "loss: 1.267280  [ 1696/ 3200]\n",
            "loss: 1.228951  [ 1712/ 3200]\n",
            "loss: 1.229724  [ 1728/ 3200]\n",
            "loss: 1.302471  [ 1744/ 3200]\n",
            "loss: 1.187806  [ 1760/ 3200]\n",
            "loss: 1.244390  [ 1776/ 3200]\n",
            "loss: 1.287196  [ 1792/ 3200]\n",
            "loss: 1.287750  [ 1808/ 3200]\n",
            "loss: 1.292946  [ 1824/ 3200]\n",
            "loss: 1.262999  [ 1840/ 3200]\n",
            "loss: 1.316954  [ 1856/ 3200]\n",
            "loss: 1.329450  [ 1872/ 3200]\n",
            "loss: 1.292456  [ 1888/ 3200]\n",
            "loss: 1.310856  [ 1904/ 3200]\n",
            "loss: 1.295017  [ 1920/ 3200]\n",
            "loss: 1.232296  [ 1936/ 3200]\n",
            "loss: 1.312601  [ 1952/ 3200]\n",
            "loss: 1.266134  [ 1968/ 3200]\n",
            "loss: 1.235554  [ 1984/ 3200]\n",
            "loss: 1.264296  [ 2000/ 3200]\n",
            "loss: 1.302012  [ 2016/ 3200]\n",
            "loss: 1.334961  [ 2032/ 3200]\n",
            "loss: 1.334421  [ 2048/ 3200]\n",
            "loss: 1.291749  [ 2064/ 3200]\n",
            "loss: 1.298232  [ 2080/ 3200]\n",
            "loss: 1.261981  [ 2096/ 3200]\n",
            "loss: 1.286166  [ 2112/ 3200]\n",
            "loss: 1.296557  [ 2128/ 3200]\n",
            "loss: 1.225374  [ 2144/ 3200]\n",
            "loss: 1.228455  [ 2160/ 3200]\n",
            "loss: 1.297416  [ 2176/ 3200]\n",
            "loss: 1.282928  [ 2192/ 3200]\n",
            "loss: 1.279058  [ 2208/ 3200]\n",
            "loss: 1.323126  [ 2224/ 3200]\n",
            "loss: 1.265843  [ 2240/ 3200]\n",
            "loss: 1.282768  [ 2256/ 3200]\n",
            "loss: 1.250046  [ 2272/ 3200]\n",
            "loss: 1.219129  [ 2288/ 3200]\n",
            "loss: 1.284987  [ 2304/ 3200]\n",
            "loss: 1.278013  [ 2320/ 3200]\n",
            "loss: 1.316790  [ 2336/ 3200]\n",
            "loss: 1.293754  [ 2352/ 3200]\n",
            "loss: 1.272222  [ 2368/ 3200]\n",
            "loss: 1.239179  [ 2384/ 3200]\n",
            "loss: 1.263914  [ 2400/ 3200]\n",
            "loss: 1.233564  [ 2416/ 3200]\n",
            "loss: 1.295530  [ 2432/ 3200]\n",
            "loss: 1.281034  [ 2448/ 3200]\n",
            "loss: 1.242179  [ 2464/ 3200]\n",
            "loss: 1.350369  [ 2480/ 3200]\n",
            "loss: 1.283654  [ 2496/ 3200]\n",
            "loss: 1.231897  [ 2512/ 3200]\n",
            "loss: 1.255215  [ 2528/ 3200]\n",
            "loss: 1.276772  [ 2544/ 3200]\n",
            "loss: 1.207962  [ 2560/ 3200]\n",
            "loss: 1.274581  [ 2576/ 3200]\n",
            "loss: 1.229394  [ 2592/ 3200]\n",
            "loss: 1.260666  [ 2608/ 3200]\n",
            "loss: 1.329075  [ 2624/ 3200]\n",
            "loss: 1.247141  [ 2640/ 3200]\n",
            "loss: 1.261140  [ 2656/ 3200]\n",
            "loss: 1.330830  [ 2672/ 3200]\n",
            "loss: 1.283653  [ 2688/ 3200]\n",
            "loss: 1.244667  [ 2704/ 3200]\n",
            "loss: 1.269200  [ 2720/ 3200]\n",
            "loss: 1.294272  [ 2736/ 3200]\n",
            "loss: 1.274521  [ 2752/ 3200]\n",
            "loss: 1.267722  [ 2768/ 3200]\n",
            "loss: 1.281227  [ 2784/ 3200]\n",
            "loss: 1.270765  [ 2800/ 3200]\n",
            "loss: 1.293012  [ 2816/ 3200]\n",
            "loss: 1.300633  [ 2832/ 3200]\n",
            "loss: 1.320257  [ 2848/ 3200]\n",
            "loss: 1.263848  [ 2864/ 3200]\n",
            "loss: 1.294054  [ 2880/ 3200]\n",
            "loss: 1.229194  [ 2896/ 3200]\n",
            "loss: 1.284935  [ 2912/ 3200]\n",
            "loss: 1.255512  [ 2928/ 3200]\n",
            "loss: 1.280718  [ 2944/ 3200]\n",
            "loss: 1.232715  [ 2960/ 3200]\n",
            "loss: 1.287336  [ 2976/ 3200]\n",
            "loss: 1.220497  [ 2992/ 3200]\n",
            "loss: 1.303385  [ 3008/ 3200]\n",
            "loss: 1.304844  [ 3024/ 3200]\n",
            "loss: 1.259012  [ 3040/ 3200]\n",
            "loss: 1.284873  [ 3056/ 3200]\n",
            "loss: 1.298607  [ 3072/ 3200]\n",
            "loss: 1.227333  [ 3088/ 3200]\n",
            "loss: 1.241799  [ 3104/ 3200]\n",
            "loss: 1.193374  [ 3120/ 3200]\n",
            "loss: 1.278197  [ 3136/ 3200]\n",
            "loss: 1.265914  [ 3152/ 3200]\n",
            "loss: 1.194988  [ 3168/ 3200]\n",
            "loss: 1.228017  [ 3184/ 3200]\n",
            "current epoch: 14\n",
            "\n",
            "loss: 1.226413  [    0/ 3200]\n",
            "loss: 1.343719  [   16/ 3200]\n",
            "loss: 1.245386  [   32/ 3200]\n",
            "loss: 1.278410  [   48/ 3200]\n",
            "loss: 1.295298  [   64/ 3200]\n",
            "loss: 1.267369  [   80/ 3200]\n",
            "loss: 1.349679  [   96/ 3200]\n",
            "loss: 1.339238  [  112/ 3200]\n",
            "loss: 1.278046  [  128/ 3200]\n",
            "loss: 1.231699  [  144/ 3200]\n",
            "loss: 1.187250  [  160/ 3200]\n",
            "loss: 1.263948  [  176/ 3200]\n",
            "loss: 1.252268  [  192/ 3200]\n",
            "loss: 1.251724  [  208/ 3200]\n",
            "loss: 1.205840  [  224/ 3200]\n",
            "loss: 1.323198  [  240/ 3200]\n",
            "loss: 1.350506  [  256/ 3200]\n",
            "loss: 1.148351  [  272/ 3200]\n",
            "loss: 1.249407  [  288/ 3200]\n",
            "loss: 1.233672  [  304/ 3200]\n",
            "loss: 1.294158  [  320/ 3200]\n",
            "loss: 1.317962  [  336/ 3200]\n",
            "loss: 1.098243  [  352/ 3200]\n",
            "loss: 1.192187  [  368/ 3200]\n",
            "loss: 1.280844  [  384/ 3200]\n",
            "loss: 1.339463  [  400/ 3200]\n",
            "loss: 1.208533  [  416/ 3200]\n",
            "loss: 1.257319  [  432/ 3200]\n",
            "loss: 1.227433  [  448/ 3200]\n",
            "loss: 1.275663  [  464/ 3200]\n",
            "loss: 1.226117  [  480/ 3200]\n",
            "loss: 1.137879  [  496/ 3200]\n",
            "loss: 1.309040  [  512/ 3200]\n",
            "loss: 1.333458  [  528/ 3200]\n",
            "loss: 1.370906  [  544/ 3200]\n",
            "loss: 1.171094  [  560/ 3200]\n",
            "loss: 1.324757  [  576/ 3200]\n",
            "loss: 1.277872  [  592/ 3200]\n",
            "loss: 1.376539  [  608/ 3200]\n",
            "loss: 1.261857  [  624/ 3200]\n",
            "loss: 1.235086  [  640/ 3200]\n",
            "loss: 1.337534  [  656/ 3200]\n",
            "loss: 1.307425  [  672/ 3200]\n",
            "loss: 1.245229  [  688/ 3200]\n",
            "loss: 1.323887  [  704/ 3200]\n",
            "loss: 1.241462  [  720/ 3200]\n",
            "loss: 1.264912  [  736/ 3200]\n",
            "loss: 1.325915  [  752/ 3200]\n",
            "loss: 1.202540  [  768/ 3200]\n",
            "loss: 1.309870  [  784/ 3200]\n",
            "loss: 1.215545  [  800/ 3200]\n",
            "loss: 1.202102  [  816/ 3200]\n",
            "loss: 1.273509  [  832/ 3200]\n",
            "loss: 1.252285  [  848/ 3200]\n",
            "loss: 1.203584  [  864/ 3200]\n",
            "loss: 1.259722  [  880/ 3200]\n",
            "loss: 1.286169  [  896/ 3200]\n",
            "loss: 1.211282  [  912/ 3200]\n",
            "loss: 1.225804  [  928/ 3200]\n",
            "loss: 1.290707  [  944/ 3200]\n",
            "loss: 1.246601  [  960/ 3200]\n",
            "loss: 1.181988  [  976/ 3200]\n",
            "loss: 1.342872  [  992/ 3200]\n",
            "loss: 1.267424  [ 1008/ 3200]\n",
            "loss: 1.232178  [ 1024/ 3200]\n",
            "loss: 1.174951  [ 1040/ 3200]\n",
            "loss: 1.257366  [ 1056/ 3200]\n",
            "loss: 1.293065  [ 1072/ 3200]\n",
            "loss: 1.312445  [ 1088/ 3200]\n",
            "loss: 1.180888  [ 1104/ 3200]\n",
            "loss: 1.327966  [ 1120/ 3200]\n",
            "loss: 1.262225  [ 1136/ 3200]\n",
            "loss: 1.174749  [ 1152/ 3200]\n",
            "loss: 1.238628  [ 1168/ 3200]\n",
            "loss: 1.326826  [ 1184/ 3200]\n",
            "loss: 1.303428  [ 1200/ 3200]\n",
            "loss: 1.271562  [ 1216/ 3200]\n",
            "loss: 1.249166  [ 1232/ 3200]\n",
            "loss: 1.363333  [ 1248/ 3200]\n",
            "loss: 1.314933  [ 1264/ 3200]\n",
            "loss: 1.199476  [ 1280/ 3200]\n",
            "loss: 1.233785  [ 1296/ 3200]\n",
            "loss: 1.245127  [ 1312/ 3200]\n",
            "loss: 1.249718  [ 1328/ 3200]\n",
            "loss: 1.324207  [ 1344/ 3200]\n",
            "loss: 1.202587  [ 1360/ 3200]\n",
            "loss: 1.325654  [ 1376/ 3200]\n",
            "loss: 1.384768  [ 1392/ 3200]\n",
            "loss: 1.293865  [ 1408/ 3200]\n",
            "loss: 1.277412  [ 1424/ 3200]\n",
            "loss: 1.272767  [ 1440/ 3200]\n",
            "loss: 1.320049  [ 1456/ 3200]\n",
            "loss: 1.282420  [ 1472/ 3200]\n",
            "loss: 1.249912  [ 1488/ 3200]\n",
            "loss: 1.232624  [ 1504/ 3200]\n",
            "loss: 1.237199  [ 1520/ 3200]\n",
            "loss: 1.239227  [ 1536/ 3200]\n",
            "loss: 1.286465  [ 1552/ 3200]\n",
            "loss: 1.183318  [ 1568/ 3200]\n",
            "loss: 1.200557  [ 1584/ 3200]\n",
            "loss: 1.219677  [ 1600/ 3200]\n",
            "loss: 1.303346  [ 1616/ 3200]\n",
            "loss: 1.233517  [ 1632/ 3200]\n",
            "loss: 1.317881  [ 1648/ 3200]\n",
            "loss: 1.266404  [ 1664/ 3200]\n",
            "loss: 1.320894  [ 1680/ 3200]\n",
            "loss: 1.221681  [ 1696/ 3200]\n",
            "loss: 1.345785  [ 1712/ 3200]\n",
            "loss: 1.251589  [ 1728/ 3200]\n",
            "loss: 1.258820  [ 1744/ 3200]\n",
            "loss: 1.215268  [ 1760/ 3200]\n",
            "loss: 1.280354  [ 1776/ 3200]\n",
            "loss: 1.261393  [ 1792/ 3200]\n",
            "loss: 1.249073  [ 1808/ 3200]\n",
            "loss: 1.210875  [ 1824/ 3200]\n",
            "loss: 1.298587  [ 1840/ 3200]\n",
            "loss: 1.254342  [ 1856/ 3200]\n",
            "loss: 1.244057  [ 1872/ 3200]\n",
            "loss: 1.180164  [ 1888/ 3200]\n",
            "loss: 1.210583  [ 1904/ 3200]\n",
            "loss: 1.164893  [ 1920/ 3200]\n",
            "loss: 1.372264  [ 1936/ 3200]\n",
            "loss: 1.223323  [ 1952/ 3200]\n",
            "loss: 1.315150  [ 1968/ 3200]\n",
            "loss: 1.228161  [ 1984/ 3200]\n",
            "loss: 1.347883  [ 2000/ 3200]\n",
            "loss: 1.216589  [ 2016/ 3200]\n",
            "loss: 1.217492  [ 2032/ 3200]\n",
            "loss: 1.333017  [ 2048/ 3200]\n",
            "loss: 1.281157  [ 2064/ 3200]\n",
            "loss: 1.247254  [ 2080/ 3200]\n",
            "loss: 1.284006  [ 2096/ 3200]\n",
            "loss: 1.271049  [ 2112/ 3200]\n",
            "loss: 1.167286  [ 2128/ 3200]\n",
            "loss: 1.297469  [ 2144/ 3200]\n",
            "loss: 1.240543  [ 2160/ 3200]\n",
            "loss: 1.296341  [ 2176/ 3200]\n",
            "loss: 1.260039  [ 2192/ 3200]\n",
            "loss: 1.315884  [ 2208/ 3200]\n",
            "loss: 1.300117  [ 2224/ 3200]\n",
            "loss: 1.327045  [ 2240/ 3200]\n",
            "loss: 1.241974  [ 2256/ 3200]\n",
            "loss: 1.235512  [ 2272/ 3200]\n",
            "loss: 1.206185  [ 2288/ 3200]\n",
            "loss: 1.257994  [ 2304/ 3200]\n",
            "loss: 1.255847  [ 2320/ 3200]\n",
            "loss: 1.285631  [ 2336/ 3200]\n",
            "loss: 1.254615  [ 2352/ 3200]\n",
            "loss: 1.274018  [ 2368/ 3200]\n",
            "loss: 1.174803  [ 2384/ 3200]\n",
            "loss: 1.312383  [ 2400/ 3200]\n",
            "loss: 1.259530  [ 2416/ 3200]\n",
            "loss: 1.294250  [ 2432/ 3200]\n",
            "loss: 1.265588  [ 2448/ 3200]\n",
            "loss: 1.205073  [ 2464/ 3200]\n",
            "loss: 1.236163  [ 2480/ 3200]\n",
            "loss: 1.226535  [ 2496/ 3200]\n",
            "loss: 1.163883  [ 2512/ 3200]\n",
            "loss: 1.316786  [ 2528/ 3200]\n",
            "loss: 1.225495  [ 2544/ 3200]\n",
            "loss: 1.287432  [ 2560/ 3200]\n",
            "loss: 1.307138  [ 2576/ 3200]\n",
            "loss: 1.325981  [ 2592/ 3200]\n",
            "loss: 1.358671  [ 2608/ 3200]\n",
            "loss: 1.241061  [ 2624/ 3200]\n",
            "loss: 1.270116  [ 2640/ 3200]\n",
            "loss: 1.267630  [ 2656/ 3200]\n",
            "loss: 1.199358  [ 2672/ 3200]\n",
            "loss: 1.155037  [ 2688/ 3200]\n",
            "loss: 1.172380  [ 2704/ 3200]\n",
            "loss: 1.374066  [ 2720/ 3200]\n",
            "loss: 1.313940  [ 2736/ 3200]\n",
            "loss: 1.249627  [ 2752/ 3200]\n",
            "loss: 1.313600  [ 2768/ 3200]\n",
            "loss: 1.267647  [ 2784/ 3200]\n",
            "loss: 1.297602  [ 2800/ 3200]\n",
            "loss: 1.190590  [ 2816/ 3200]\n",
            "loss: 1.237769  [ 2832/ 3200]\n",
            "loss: 1.290904  [ 2848/ 3200]\n",
            "loss: 1.284280  [ 2864/ 3200]\n",
            "loss: 1.263515  [ 2880/ 3200]\n",
            "loss: 1.296094  [ 2896/ 3200]\n",
            "loss: 1.355927  [ 2912/ 3200]\n",
            "loss: 1.195462  [ 2928/ 3200]\n",
            "loss: 1.237890  [ 2944/ 3200]\n",
            "loss: 1.207248  [ 2960/ 3200]\n",
            "loss: 1.281083  [ 2976/ 3200]\n",
            "loss: 1.301853  [ 2992/ 3200]\n",
            "loss: 1.214368  [ 3008/ 3200]\n",
            "loss: 1.305871  [ 3024/ 3200]\n",
            "loss: 1.203444  [ 3040/ 3200]\n",
            "loss: 1.271048  [ 3056/ 3200]\n",
            "loss: 1.280670  [ 3072/ 3200]\n",
            "loss: 1.233203  [ 3088/ 3200]\n",
            "loss: 1.189355  [ 3104/ 3200]\n",
            "loss: 1.313973  [ 3120/ 3200]\n",
            "loss: 1.150059  [ 3136/ 3200]\n",
            "loss: 1.230676  [ 3152/ 3200]\n",
            "loss: 1.239634  [ 3168/ 3200]\n",
            "loss: 1.254286  [ 3184/ 3200]\n",
            "current epoch: 15\n",
            "\n",
            "loss: 1.137086  [    0/ 3200]\n",
            "loss: 1.232879  [   16/ 3200]\n",
            "loss: 1.205303  [   32/ 3200]\n",
            "loss: 1.153479  [   48/ 3200]\n",
            "loss: 1.227021  [   64/ 3200]\n",
            "loss: 1.271192  [   80/ 3200]\n",
            "loss: 1.228005  [   96/ 3200]\n",
            "loss: 1.254791  [  112/ 3200]\n",
            "loss: 1.209845  [  128/ 3200]\n",
            "loss: 1.269408  [  144/ 3200]\n",
            "loss: 1.224312  [  160/ 3200]\n",
            "loss: 1.325161  [  176/ 3200]\n",
            "loss: 1.280390  [  192/ 3200]\n",
            "loss: 1.209762  [  208/ 3200]\n",
            "loss: 1.159462  [  224/ 3200]\n",
            "loss: 1.285082  [  240/ 3200]\n",
            "loss: 1.269302  [  256/ 3200]\n",
            "loss: 1.241923  [  272/ 3200]\n",
            "loss: 1.277835  [  288/ 3200]\n",
            "loss: 1.293606  [  304/ 3200]\n",
            "loss: 1.296654  [  320/ 3200]\n",
            "loss: 1.321356  [  336/ 3200]\n",
            "loss: 1.354772  [  352/ 3200]\n",
            "loss: 1.412336  [  368/ 3200]\n",
            "loss: 1.230733  [  384/ 3200]\n",
            "loss: 1.193147  [  400/ 3200]\n",
            "loss: 1.284645  [  416/ 3200]\n",
            "loss: 1.251930  [  432/ 3200]\n",
            "loss: 1.193026  [  448/ 3200]\n",
            "loss: 1.254797  [  464/ 3200]\n",
            "loss: 1.301700  [  480/ 3200]\n",
            "loss: 1.319827  [  496/ 3200]\n",
            "loss: 1.252098  [  512/ 3200]\n",
            "loss: 1.215834  [  528/ 3200]\n",
            "loss: 1.321673  [  544/ 3200]\n",
            "loss: 1.228441  [  560/ 3200]\n",
            "loss: 1.147228  [  576/ 3200]\n",
            "loss: 1.315123  [  592/ 3200]\n",
            "loss: 1.111203  [  608/ 3200]\n",
            "loss: 1.271677  [  624/ 3200]\n",
            "loss: 1.162473  [  640/ 3200]\n",
            "loss: 1.319022  [  656/ 3200]\n",
            "loss: 1.262270  [  672/ 3200]\n",
            "loss: 1.197846  [  688/ 3200]\n",
            "loss: 1.319253  [  704/ 3200]\n",
            "loss: 1.218820  [  720/ 3200]\n",
            "loss: 1.246793  [  736/ 3200]\n",
            "loss: 1.277215  [  752/ 3200]\n",
            "loss: 1.234878  [  768/ 3200]\n",
            "loss: 1.197734  [  784/ 3200]\n",
            "loss: 1.376506  [  800/ 3200]\n",
            "loss: 1.176439  [  816/ 3200]\n",
            "loss: 1.192770  [  832/ 3200]\n",
            "loss: 1.229673  [  848/ 3200]\n",
            "loss: 1.231512  [  864/ 3200]\n",
            "loss: 1.291944  [  880/ 3200]\n",
            "loss: 1.346648  [  896/ 3200]\n",
            "loss: 1.239993  [  912/ 3200]\n",
            "loss: 1.294996  [  928/ 3200]\n",
            "loss: 1.198954  [  944/ 3200]\n",
            "loss: 1.174830  [  960/ 3200]\n",
            "loss: 1.263888  [  976/ 3200]\n",
            "loss: 1.330354  [  992/ 3200]\n",
            "loss: 1.350317  [ 1008/ 3200]\n",
            "loss: 1.140162  [ 1024/ 3200]\n",
            "loss: 1.186520  [ 1040/ 3200]\n",
            "loss: 1.281371  [ 1056/ 3200]\n",
            "loss: 1.184960  [ 1072/ 3200]\n",
            "loss: 1.243313  [ 1088/ 3200]\n",
            "loss: 1.266710  [ 1104/ 3200]\n",
            "loss: 1.219626  [ 1120/ 3200]\n",
            "loss: 1.293025  [ 1136/ 3200]\n",
            "loss: 1.162607  [ 1152/ 3200]\n",
            "loss: 1.216578  [ 1168/ 3200]\n",
            "loss: 1.230540  [ 1184/ 3200]\n",
            "loss: 1.332518  [ 1200/ 3200]\n",
            "loss: 1.212476  [ 1216/ 3200]\n",
            "loss: 1.251338  [ 1232/ 3200]\n",
            "loss: 1.177865  [ 1248/ 3200]\n",
            "loss: 1.092087  [ 1264/ 3200]\n",
            "loss: 1.202128  [ 1280/ 3200]\n",
            "loss: 1.304346  [ 1296/ 3200]\n",
            "loss: 1.296902  [ 1312/ 3200]\n",
            "loss: 1.233289  [ 1328/ 3200]\n",
            "loss: 1.170332  [ 1344/ 3200]\n",
            "loss: 1.304769  [ 1360/ 3200]\n",
            "loss: 1.267566  [ 1376/ 3200]\n",
            "loss: 1.232773  [ 1392/ 3200]\n",
            "loss: 1.263354  [ 1408/ 3200]\n",
            "loss: 1.255323  [ 1424/ 3200]\n",
            "loss: 1.323936  [ 1440/ 3200]\n",
            "loss: 1.196912  [ 1456/ 3200]\n",
            "loss: 1.278872  [ 1472/ 3200]\n",
            "loss: 1.249900  [ 1488/ 3200]\n",
            "loss: 1.202022  [ 1504/ 3200]\n",
            "loss: 1.267652  [ 1520/ 3200]\n",
            "loss: 1.252002  [ 1536/ 3200]\n",
            "loss: 1.213727  [ 1552/ 3200]\n",
            "loss: 1.190264  [ 1568/ 3200]\n",
            "loss: 1.291257  [ 1584/ 3200]\n",
            "loss: 1.307336  [ 1600/ 3200]\n",
            "loss: 1.248509  [ 1616/ 3200]\n",
            "loss: 1.265355  [ 1632/ 3200]\n",
            "loss: 1.225757  [ 1648/ 3200]\n",
            "loss: 1.238034  [ 1664/ 3200]\n",
            "loss: 1.271593  [ 1680/ 3200]\n",
            "loss: 1.216649  [ 1696/ 3200]\n",
            "loss: 1.247550  [ 1712/ 3200]\n",
            "loss: 1.249194  [ 1728/ 3200]\n",
            "loss: 1.274149  [ 1744/ 3200]\n",
            "loss: 1.258340  [ 1760/ 3200]\n",
            "loss: 1.261512  [ 1776/ 3200]\n",
            "loss: 1.223336  [ 1792/ 3200]\n",
            "loss: 1.188261  [ 1808/ 3200]\n",
            "loss: 1.307090  [ 1824/ 3200]\n",
            "loss: 1.246097  [ 1840/ 3200]\n",
            "loss: 1.268702  [ 1856/ 3200]\n",
            "loss: 1.193747  [ 1872/ 3200]\n",
            "loss: 1.158925  [ 1888/ 3200]\n",
            "loss: 1.172584  [ 1904/ 3200]\n",
            "loss: 1.241077  [ 1920/ 3200]\n",
            "loss: 1.274759  [ 1936/ 3200]\n",
            "loss: 1.229840  [ 1952/ 3200]\n",
            "loss: 1.254647  [ 1968/ 3200]\n",
            "loss: 1.299372  [ 1984/ 3200]\n",
            "loss: 1.191936  [ 2000/ 3200]\n",
            "loss: 1.174673  [ 2016/ 3200]\n",
            "loss: 1.226682  [ 2032/ 3200]\n",
            "loss: 1.372224  [ 2048/ 3200]\n",
            "loss: 1.296099  [ 2064/ 3200]\n",
            "loss: 1.249203  [ 2080/ 3200]\n",
            "loss: 1.259803  [ 2096/ 3200]\n",
            "loss: 1.170459  [ 2112/ 3200]\n",
            "loss: 1.186519  [ 2128/ 3200]\n",
            "loss: 1.207957  [ 2144/ 3200]\n",
            "loss: 1.283748  [ 2160/ 3200]\n",
            "loss: 1.318135  [ 2176/ 3200]\n",
            "loss: 1.167431  [ 2192/ 3200]\n",
            "loss: 1.270534  [ 2208/ 3200]\n",
            "loss: 1.205786  [ 2224/ 3200]\n",
            "loss: 1.287894  [ 2240/ 3200]\n",
            "loss: 1.217592  [ 2256/ 3200]\n",
            "loss: 1.278426  [ 2272/ 3200]\n",
            "loss: 1.133751  [ 2288/ 3200]\n",
            "loss: 1.205120  [ 2304/ 3200]\n",
            "loss: 1.314864  [ 2320/ 3200]\n",
            "loss: 1.140076  [ 2336/ 3200]\n",
            "loss: 1.240781  [ 2352/ 3200]\n",
            "loss: 1.165640  [ 2368/ 3200]\n",
            "loss: 1.231518  [ 2384/ 3200]\n",
            "loss: 1.228696  [ 2400/ 3200]\n",
            "loss: 1.136385  [ 2416/ 3200]\n",
            "loss: 1.243831  [ 2432/ 3200]\n",
            "loss: 1.302428  [ 2448/ 3200]\n",
            "loss: 1.147639  [ 2464/ 3200]\n",
            "loss: 1.388577  [ 2480/ 3200]\n",
            "loss: 1.205286  [ 2496/ 3200]\n",
            "loss: 1.266161  [ 2512/ 3200]\n",
            "loss: 1.294727  [ 2528/ 3200]\n",
            "loss: 1.211972  [ 2544/ 3200]\n",
            "loss: 1.213583  [ 2560/ 3200]\n",
            "loss: 1.189905  [ 2576/ 3200]\n",
            "loss: 1.249251  [ 2592/ 3200]\n",
            "loss: 1.232865  [ 2608/ 3200]\n",
            "loss: 1.279244  [ 2624/ 3200]\n",
            "loss: 1.211344  [ 2640/ 3200]\n",
            "loss: 1.354213  [ 2656/ 3200]\n",
            "loss: 1.246683  [ 2672/ 3200]\n",
            "loss: 1.250358  [ 2688/ 3200]\n",
            "loss: 1.264066  [ 2704/ 3200]\n",
            "loss: 1.262475  [ 2720/ 3200]\n",
            "loss: 1.180947  [ 2736/ 3200]\n",
            "loss: 1.257222  [ 2752/ 3200]\n",
            "loss: 1.208241  [ 2768/ 3200]\n",
            "loss: 1.247171  [ 2784/ 3200]\n",
            "loss: 1.253726  [ 2800/ 3200]\n",
            "loss: 1.203700  [ 2816/ 3200]\n",
            "loss: 1.255021  [ 2832/ 3200]\n",
            "loss: 1.283555  [ 2848/ 3200]\n",
            "loss: 1.250755  [ 2864/ 3200]\n",
            "loss: 1.199672  [ 2880/ 3200]\n",
            "loss: 1.185004  [ 2896/ 3200]\n",
            "loss: 1.222463  [ 2912/ 3200]\n",
            "loss: 1.295140  [ 2928/ 3200]\n",
            "loss: 1.293978  [ 2944/ 3200]\n",
            "loss: 1.240290  [ 2960/ 3200]\n",
            "loss: 1.323079  [ 2976/ 3200]\n",
            "loss: 1.190026  [ 2992/ 3200]\n",
            "loss: 1.271201  [ 3008/ 3200]\n",
            "loss: 1.209471  [ 3024/ 3200]\n",
            "loss: 1.282311  [ 3040/ 3200]\n",
            "loss: 1.218797  [ 3056/ 3200]\n",
            "loss: 1.259656  [ 3072/ 3200]\n",
            "loss: 1.277066  [ 3088/ 3200]\n",
            "loss: 1.119638  [ 3104/ 3200]\n",
            "loss: 1.224583  [ 3120/ 3200]\n",
            "loss: 1.339347  [ 3136/ 3200]\n",
            "loss: 1.221596  [ 3152/ 3200]\n",
            "loss: 1.254285  [ 3168/ 3200]\n",
            "loss: 1.202172  [ 3184/ 3200]\n",
            "current epoch: 16\n",
            "\n",
            "loss: 1.176999  [    0/ 3200]\n",
            "loss: 1.265580  [   16/ 3200]\n",
            "loss: 1.185074  [   32/ 3200]\n",
            "loss: 1.334606  [   48/ 3200]\n",
            "loss: 1.249294  [   64/ 3200]\n",
            "loss: 1.189985  [   80/ 3200]\n",
            "loss: 1.284777  [   96/ 3200]\n",
            "loss: 1.302270  [  112/ 3200]\n",
            "loss: 1.199838  [  128/ 3200]\n",
            "loss: 1.243592  [  144/ 3200]\n",
            "loss: 1.287247  [  160/ 3200]\n",
            "loss: 1.262845  [  176/ 3200]\n",
            "loss: 1.286558  [  192/ 3200]\n",
            "loss: 1.107699  [  208/ 3200]\n",
            "loss: 1.293532  [  224/ 3200]\n",
            "loss: 1.145732  [  240/ 3200]\n",
            "loss: 1.258810  [  256/ 3200]\n",
            "loss: 1.245357  [  272/ 3200]\n",
            "loss: 1.268561  [  288/ 3200]\n",
            "loss: 1.263114  [  304/ 3200]\n",
            "loss: 1.214102  [  320/ 3200]\n",
            "loss: 1.135047  [  336/ 3200]\n",
            "loss: 1.314775  [  352/ 3200]\n",
            "loss: 1.286464  [  368/ 3200]\n",
            "loss: 1.190471  [  384/ 3200]\n",
            "loss: 1.173968  [  400/ 3200]\n",
            "loss: 1.178997  [  416/ 3200]\n",
            "loss: 1.244176  [  432/ 3200]\n",
            "loss: 1.246089  [  448/ 3200]\n",
            "loss: 1.128836  [  464/ 3200]\n",
            "loss: 1.267074  [  480/ 3200]\n",
            "loss: 1.218795  [  496/ 3200]\n",
            "loss: 1.153443  [  512/ 3200]\n",
            "loss: 1.105245  [  528/ 3200]\n",
            "loss: 1.241060  [  544/ 3200]\n",
            "loss: 1.249898  [  560/ 3200]\n",
            "loss: 1.279832  [  576/ 3200]\n",
            "loss: 1.198011  [  592/ 3200]\n",
            "loss: 1.180905  [  608/ 3200]\n",
            "loss: 1.244811  [  624/ 3200]\n",
            "loss: 1.190609  [  640/ 3200]\n",
            "loss: 1.249998  [  656/ 3200]\n",
            "loss: 1.128734  [  672/ 3200]\n",
            "loss: 1.231258  [  688/ 3200]\n",
            "loss: 1.254250  [  704/ 3200]\n",
            "loss: 1.288257  [  720/ 3200]\n",
            "loss: 1.277869  [  736/ 3200]\n",
            "loss: 1.282300  [  752/ 3200]\n",
            "loss: 1.262510  [  768/ 3200]\n",
            "loss: 1.241139  [  784/ 3200]\n",
            "loss: 1.247576  [  800/ 3200]\n",
            "loss: 1.181257  [  816/ 3200]\n",
            "loss: 1.316614  [  832/ 3200]\n",
            "loss: 1.335989  [  848/ 3200]\n",
            "loss: 1.273161  [  864/ 3200]\n",
            "loss: 1.252028  [  880/ 3200]\n",
            "loss: 1.116543  [  896/ 3200]\n",
            "loss: 1.220361  [  912/ 3200]\n",
            "loss: 1.303289  [  928/ 3200]\n",
            "loss: 1.248569  [  944/ 3200]\n",
            "loss: 1.217802  [  960/ 3200]\n",
            "loss: 1.337783  [  976/ 3200]\n",
            "loss: 1.193100  [  992/ 3200]\n",
            "loss: 1.164578  [ 1008/ 3200]\n",
            "loss: 1.250254  [ 1024/ 3200]\n",
            "loss: 1.207796  [ 1040/ 3200]\n",
            "loss: 1.244181  [ 1056/ 3200]\n",
            "loss: 1.268891  [ 1072/ 3200]\n",
            "loss: 1.326847  [ 1088/ 3200]\n",
            "loss: 1.209093  [ 1104/ 3200]\n",
            "loss: 1.213292  [ 1120/ 3200]\n",
            "loss: 1.220474  [ 1136/ 3200]\n",
            "loss: 1.228871  [ 1152/ 3200]\n",
            "loss: 1.271481  [ 1168/ 3200]\n",
            "loss: 1.258749  [ 1184/ 3200]\n",
            "loss: 1.307329  [ 1200/ 3200]\n",
            "loss: 1.275214  [ 1216/ 3200]\n",
            "loss: 1.151606  [ 1232/ 3200]\n",
            "loss: 1.267204  [ 1248/ 3200]\n",
            "loss: 1.223892  [ 1264/ 3200]\n",
            "loss: 1.236663  [ 1280/ 3200]\n",
            "loss: 1.180312  [ 1296/ 3200]\n",
            "loss: 1.268423  [ 1312/ 3200]\n",
            "loss: 1.218909  [ 1328/ 3200]\n",
            "loss: 1.237857  [ 1344/ 3200]\n",
            "loss: 1.269105  [ 1360/ 3200]\n",
            "loss: 1.249114  [ 1376/ 3200]\n",
            "loss: 1.223864  [ 1392/ 3200]\n",
            "loss: 1.241794  [ 1408/ 3200]\n",
            "loss: 1.299300  [ 1424/ 3200]\n",
            "loss: 1.251032  [ 1440/ 3200]\n",
            "loss: 1.186831  [ 1456/ 3200]\n",
            "loss: 1.282340  [ 1472/ 3200]\n",
            "loss: 1.134438  [ 1488/ 3200]\n",
            "loss: 1.133073  [ 1504/ 3200]\n",
            "loss: 1.324671  [ 1520/ 3200]\n",
            "loss: 1.218433  [ 1536/ 3200]\n",
            "loss: 1.218082  [ 1552/ 3200]\n",
            "loss: 1.331959  [ 1568/ 3200]\n",
            "loss: 1.149051  [ 1584/ 3200]\n",
            "loss: 1.349118  [ 1600/ 3200]\n",
            "loss: 1.196336  [ 1616/ 3200]\n",
            "loss: 1.102236  [ 1632/ 3200]\n",
            "loss: 1.133367  [ 1648/ 3200]\n",
            "loss: 1.113283  [ 1664/ 3200]\n",
            "loss: 1.262375  [ 1680/ 3200]\n",
            "loss: 1.235166  [ 1696/ 3200]\n",
            "loss: 1.217579  [ 1712/ 3200]\n",
            "loss: 1.212823  [ 1728/ 3200]\n",
            "loss: 1.212438  [ 1744/ 3200]\n",
            "loss: 1.215441  [ 1760/ 3200]\n",
            "loss: 1.203994  [ 1776/ 3200]\n",
            "loss: 1.166850  [ 1792/ 3200]\n",
            "loss: 1.158157  [ 1808/ 3200]\n",
            "loss: 1.274689  [ 1824/ 3200]\n",
            "loss: 1.319332  [ 1840/ 3200]\n",
            "loss: 1.141210  [ 1856/ 3200]\n",
            "loss: 1.289484  [ 1872/ 3200]\n",
            "loss: 1.192305  [ 1888/ 3200]\n",
            "loss: 1.218832  [ 1904/ 3200]\n",
            "loss: 1.227474  [ 1920/ 3200]\n",
            "loss: 1.224115  [ 1936/ 3200]\n",
            "loss: 1.202508  [ 1952/ 3200]\n",
            "loss: 1.256655  [ 1968/ 3200]\n",
            "loss: 1.271843  [ 1984/ 3200]\n",
            "loss: 1.191188  [ 2000/ 3200]\n",
            "loss: 1.323292  [ 2016/ 3200]\n",
            "loss: 1.234543  [ 2032/ 3200]\n",
            "loss: 1.206083  [ 2048/ 3200]\n",
            "loss: 1.276638  [ 2064/ 3200]\n",
            "loss: 1.216126  [ 2080/ 3200]\n",
            "loss: 1.223989  [ 2096/ 3200]\n",
            "loss: 1.233660  [ 2112/ 3200]\n",
            "loss: 1.246274  [ 2128/ 3200]\n",
            "loss: 1.172953  [ 2144/ 3200]\n",
            "loss: 1.099440  [ 2160/ 3200]\n",
            "loss: 1.287998  [ 2176/ 3200]\n",
            "loss: 1.192811  [ 2192/ 3200]\n",
            "loss: 1.200741  [ 2208/ 3200]\n",
            "loss: 1.157641  [ 2224/ 3200]\n",
            "loss: 1.231110  [ 2240/ 3200]\n",
            "loss: 1.197633  [ 2256/ 3200]\n",
            "loss: 1.190100  [ 2272/ 3200]\n",
            "loss: 1.267706  [ 2288/ 3200]\n",
            "loss: 1.243168  [ 2304/ 3200]\n",
            "loss: 1.070671  [ 2320/ 3200]\n",
            "loss: 1.216385  [ 2336/ 3200]\n",
            "loss: 1.235229  [ 2352/ 3200]\n",
            "loss: 1.208554  [ 2368/ 3200]\n",
            "loss: 1.193264  [ 2384/ 3200]\n",
            "loss: 1.222647  [ 2400/ 3200]\n",
            "loss: 1.305682  [ 2416/ 3200]\n",
            "loss: 1.123497  [ 2432/ 3200]\n",
            "loss: 1.318703  [ 2448/ 3200]\n",
            "loss: 1.206919  [ 2464/ 3200]\n",
            "loss: 1.145566  [ 2480/ 3200]\n",
            "loss: 1.292039  [ 2496/ 3200]\n",
            "loss: 1.260565  [ 2512/ 3200]\n",
            "loss: 1.246521  [ 2528/ 3200]\n",
            "loss: 1.265983  [ 2544/ 3200]\n",
            "loss: 1.257012  [ 2560/ 3200]\n",
            "loss: 1.181995  [ 2576/ 3200]\n",
            "loss: 1.156178  [ 2592/ 3200]\n",
            "loss: 1.221209  [ 2608/ 3200]\n",
            "loss: 1.208661  [ 2624/ 3200]\n",
            "loss: 1.318794  [ 2640/ 3200]\n",
            "loss: 1.219063  [ 2656/ 3200]\n",
            "loss: 1.175949  [ 2672/ 3200]\n",
            "loss: 1.145067  [ 2688/ 3200]\n",
            "loss: 1.120599  [ 2704/ 3200]\n",
            "loss: 1.278522  [ 2720/ 3200]\n",
            "loss: 1.203528  [ 2736/ 3200]\n",
            "loss: 1.264085  [ 2752/ 3200]\n",
            "loss: 1.242200  [ 2768/ 3200]\n",
            "loss: 1.285043  [ 2784/ 3200]\n",
            "loss: 1.284681  [ 2800/ 3200]\n",
            "loss: 1.124860  [ 2816/ 3200]\n",
            "loss: 1.194037  [ 2832/ 3200]\n",
            "loss: 1.164364  [ 2848/ 3200]\n",
            "loss: 1.294056  [ 2864/ 3200]\n",
            "loss: 1.144360  [ 2880/ 3200]\n",
            "loss: 1.173319  [ 2896/ 3200]\n",
            "loss: 1.250872  [ 2912/ 3200]\n",
            "loss: 1.141369  [ 2928/ 3200]\n",
            "loss: 1.203768  [ 2944/ 3200]\n",
            "loss: 1.160403  [ 2960/ 3200]\n",
            "loss: 1.265105  [ 2976/ 3200]\n",
            "loss: 1.311018  [ 2992/ 3200]\n",
            "loss: 1.240144  [ 3008/ 3200]\n",
            "loss: 1.264008  [ 3024/ 3200]\n",
            "loss: 1.118312  [ 3040/ 3200]\n",
            "loss: 1.200251  [ 3056/ 3200]\n",
            "loss: 1.147244  [ 3072/ 3200]\n",
            "loss: 1.059018  [ 3088/ 3200]\n",
            "loss: 1.096824  [ 3104/ 3200]\n",
            "loss: 1.253809  [ 3120/ 3200]\n",
            "loss: 1.149918  [ 3136/ 3200]\n",
            "loss: 1.197714  [ 3152/ 3200]\n",
            "loss: 1.304803  [ 3168/ 3200]\n",
            "loss: 1.317368  [ 3184/ 3200]\n",
            "current epoch: 17\n",
            "\n",
            "loss: 1.098367  [    0/ 3200]\n",
            "loss: 1.260880  [   16/ 3200]\n",
            "loss: 1.171957  [   32/ 3200]\n",
            "loss: 1.231381  [   48/ 3200]\n",
            "loss: 1.207290  [   64/ 3200]\n",
            "loss: 1.268204  [   80/ 3200]\n",
            "loss: 1.241579  [   96/ 3200]\n",
            "loss: 1.209627  [  112/ 3200]\n",
            "loss: 1.195321  [  128/ 3200]\n",
            "loss: 1.096338  [  144/ 3200]\n",
            "loss: 1.321612  [  160/ 3200]\n",
            "loss: 1.235434  [  176/ 3200]\n",
            "loss: 1.179365  [  192/ 3200]\n",
            "loss: 1.282174  [  208/ 3200]\n",
            "loss: 1.213792  [  224/ 3200]\n",
            "loss: 1.268401  [  240/ 3200]\n",
            "loss: 1.233871  [  256/ 3200]\n",
            "loss: 1.123547  [  272/ 3200]\n",
            "loss: 1.233767  [  288/ 3200]\n",
            "loss: 1.168935  [  304/ 3200]\n",
            "loss: 1.148963  [  320/ 3200]\n",
            "loss: 1.198614  [  336/ 3200]\n",
            "loss: 1.189461  [  352/ 3200]\n",
            "loss: 1.240716  [  368/ 3200]\n",
            "loss: 1.261871  [  384/ 3200]\n",
            "loss: 1.172789  [  400/ 3200]\n",
            "loss: 1.193422  [  416/ 3200]\n",
            "loss: 1.225555  [  432/ 3200]\n",
            "loss: 1.172116  [  448/ 3200]\n",
            "loss: 1.127809  [  464/ 3200]\n",
            "loss: 1.221961  [  480/ 3200]\n",
            "loss: 1.171619  [  496/ 3200]\n",
            "loss: 1.300534  [  512/ 3200]\n",
            "loss: 1.105514  [  528/ 3200]\n",
            "loss: 1.175474  [  544/ 3200]\n",
            "loss: 1.179793  [  560/ 3200]\n",
            "loss: 1.339545  [  576/ 3200]\n",
            "loss: 1.348721  [  592/ 3200]\n",
            "loss: 1.198389  [  608/ 3200]\n",
            "loss: 1.275111  [  624/ 3200]\n",
            "loss: 1.281360  [  640/ 3200]\n",
            "loss: 1.210071  [  656/ 3200]\n",
            "loss: 1.195781  [  672/ 3200]\n",
            "loss: 1.196830  [  688/ 3200]\n",
            "loss: 1.189288  [  704/ 3200]\n",
            "loss: 1.158807  [  720/ 3200]\n",
            "loss: 1.059945  [  736/ 3200]\n",
            "loss: 1.205504  [  752/ 3200]\n",
            "loss: 1.147332  [  768/ 3200]\n",
            "loss: 1.205495  [  784/ 3200]\n",
            "loss: 1.155891  [  800/ 3200]\n",
            "loss: 1.178866  [  816/ 3200]\n",
            "loss: 1.231749  [  832/ 3200]\n",
            "loss: 1.228820  [  848/ 3200]\n",
            "loss: 1.333973  [  864/ 3200]\n",
            "loss: 1.202043  [  880/ 3200]\n",
            "loss: 1.342719  [  896/ 3200]\n",
            "loss: 1.174199  [  912/ 3200]\n",
            "loss: 1.327047  [  928/ 3200]\n",
            "loss: 1.192305  [  944/ 3200]\n",
            "loss: 1.252489  [  960/ 3200]\n",
            "loss: 1.259340  [  976/ 3200]\n",
            "loss: 1.192321  [  992/ 3200]\n",
            "loss: 1.224164  [ 1008/ 3200]\n",
            "loss: 1.209593  [ 1024/ 3200]\n",
            "loss: 1.293901  [ 1040/ 3200]\n",
            "loss: 1.107393  [ 1056/ 3200]\n",
            "loss: 1.136784  [ 1072/ 3200]\n",
            "loss: 1.204735  [ 1088/ 3200]\n",
            "loss: 1.181827  [ 1104/ 3200]\n",
            "loss: 1.272609  [ 1120/ 3200]\n",
            "loss: 1.234902  [ 1136/ 3200]\n",
            "loss: 1.109070  [ 1152/ 3200]\n",
            "loss: 1.191390  [ 1168/ 3200]\n",
            "loss: 1.266715  [ 1184/ 3200]\n",
            "loss: 1.221781  [ 1200/ 3200]\n",
            "loss: 1.214287  [ 1216/ 3200]\n",
            "loss: 1.172359  [ 1232/ 3200]\n",
            "loss: 1.223872  [ 1248/ 3200]\n",
            "loss: 1.058181  [ 1264/ 3200]\n",
            "loss: 1.203301  [ 1280/ 3200]\n",
            "loss: 1.112507  [ 1296/ 3200]\n",
            "loss: 1.262172  [ 1312/ 3200]\n",
            "loss: 1.181259  [ 1328/ 3200]\n",
            "loss: 1.130356  [ 1344/ 3200]\n",
            "loss: 1.184485  [ 1360/ 3200]\n",
            "loss: 1.275792  [ 1376/ 3200]\n",
            "loss: 1.178117  [ 1392/ 3200]\n",
            "loss: 1.298357  [ 1408/ 3200]\n",
            "loss: 1.242251  [ 1424/ 3200]\n",
            "loss: 1.227384  [ 1440/ 3200]\n",
            "loss: 1.192902  [ 1456/ 3200]\n",
            "loss: 1.304193  [ 1472/ 3200]\n",
            "loss: 1.218966  [ 1488/ 3200]\n",
            "loss: 1.283198  [ 1504/ 3200]\n",
            "loss: 1.024221  [ 1520/ 3200]\n",
            "loss: 1.094838  [ 1536/ 3200]\n",
            "loss: 1.173782  [ 1552/ 3200]\n",
            "loss: 1.226259  [ 1568/ 3200]\n",
            "loss: 1.271714  [ 1584/ 3200]\n",
            "loss: 1.224959  [ 1600/ 3200]\n",
            "loss: 1.179769  [ 1616/ 3200]\n",
            "loss: 1.284518  [ 1632/ 3200]\n",
            "loss: 1.127733  [ 1648/ 3200]\n",
            "loss: 1.264469  [ 1664/ 3200]\n",
            "loss: 1.173757  [ 1680/ 3200]\n",
            "loss: 1.232848  [ 1696/ 3200]\n",
            "loss: 1.065435  [ 1712/ 3200]\n",
            "loss: 1.336426  [ 1728/ 3200]\n",
            "loss: 1.275798  [ 1744/ 3200]\n",
            "loss: 1.135015  [ 1760/ 3200]\n",
            "loss: 1.316069  [ 1776/ 3200]\n",
            "loss: 1.213406  [ 1792/ 3200]\n",
            "loss: 1.277580  [ 1808/ 3200]\n",
            "loss: 1.230443  [ 1824/ 3200]\n",
            "loss: 1.110162  [ 1840/ 3200]\n",
            "loss: 1.203162  [ 1856/ 3200]\n",
            "loss: 1.260493  [ 1872/ 3200]\n",
            "loss: 1.166784  [ 1888/ 3200]\n",
            "loss: 1.257323  [ 1904/ 3200]\n",
            "loss: 1.205010  [ 1920/ 3200]\n",
            "loss: 1.109616  [ 1936/ 3200]\n",
            "loss: 1.215828  [ 1952/ 3200]\n",
            "loss: 1.166529  [ 1968/ 3200]\n",
            "loss: 1.131433  [ 1984/ 3200]\n",
            "loss: 1.152287  [ 2000/ 3200]\n",
            "loss: 1.167214  [ 2016/ 3200]\n",
            "loss: 1.213071  [ 2032/ 3200]\n",
            "loss: 1.098153  [ 2048/ 3200]\n",
            "loss: 1.211600  [ 2064/ 3200]\n",
            "loss: 1.231286  [ 2080/ 3200]\n",
            "loss: 1.233501  [ 2096/ 3200]\n",
            "loss: 1.219146  [ 2112/ 3200]\n",
            "loss: 1.200548  [ 2128/ 3200]\n",
            "loss: 1.183482  [ 2144/ 3200]\n",
            "loss: 1.138924  [ 2160/ 3200]\n",
            "loss: 1.200170  [ 2176/ 3200]\n",
            "loss: 1.252786  [ 2192/ 3200]\n",
            "loss: 1.240316  [ 2208/ 3200]\n",
            "loss: 1.176439  [ 2224/ 3200]\n",
            "loss: 1.301260  [ 2240/ 3200]\n",
            "loss: 1.235231  [ 2256/ 3200]\n",
            "loss: 1.257357  [ 2272/ 3200]\n",
            "loss: 1.187292  [ 2288/ 3200]\n",
            "loss: 1.262046  [ 2304/ 3200]\n",
            "loss: 1.222221  [ 2320/ 3200]\n",
            "loss: 1.231129  [ 2336/ 3200]\n",
            "loss: 1.260477  [ 2352/ 3200]\n",
            "loss: 1.179239  [ 2368/ 3200]\n",
            "loss: 1.156301  [ 2384/ 3200]\n",
            "loss: 1.162013  [ 2400/ 3200]\n",
            "loss: 1.158048  [ 2416/ 3200]\n",
            "loss: 1.186210  [ 2432/ 3200]\n",
            "loss: 1.038239  [ 2448/ 3200]\n",
            "loss: 1.189402  [ 2464/ 3200]\n",
            "loss: 1.230253  [ 2480/ 3200]\n",
            "loss: 1.295159  [ 2496/ 3200]\n",
            "loss: 1.119832  [ 2512/ 3200]\n",
            "loss: 1.282978  [ 2528/ 3200]\n",
            "loss: 1.179656  [ 2544/ 3200]\n",
            "loss: 1.185550  [ 2560/ 3200]\n",
            "loss: 1.267843  [ 2576/ 3200]\n",
            "loss: 1.147096  [ 2592/ 3200]\n",
            "loss: 1.287079  [ 2608/ 3200]\n",
            "loss: 1.104949  [ 2624/ 3200]\n",
            "loss: 1.178930  [ 2640/ 3200]\n",
            "loss: 1.176336  [ 2656/ 3200]\n",
            "loss: 1.201642  [ 2672/ 3200]\n",
            "loss: 1.095363  [ 2688/ 3200]\n",
            "loss: 1.211864  [ 2704/ 3200]\n",
            "loss: 1.263560  [ 2720/ 3200]\n",
            "loss: 1.187870  [ 2736/ 3200]\n",
            "loss: 1.334049  [ 2752/ 3200]\n",
            "loss: 1.364348  [ 2768/ 3200]\n",
            "loss: 1.186921  [ 2784/ 3200]\n",
            "loss: 1.090601  [ 2800/ 3200]\n",
            "loss: 1.283706  [ 2816/ 3200]\n",
            "loss: 1.155160  [ 2832/ 3200]\n",
            "loss: 1.166935  [ 2848/ 3200]\n",
            "loss: 1.189283  [ 2864/ 3200]\n",
            "loss: 1.241107  [ 2880/ 3200]\n",
            "loss: 1.111827  [ 2896/ 3200]\n",
            "loss: 1.203090  [ 2912/ 3200]\n",
            "loss: 1.193905  [ 2928/ 3200]\n",
            "loss: 1.039489  [ 2944/ 3200]\n",
            "loss: 1.191438  [ 2960/ 3200]\n",
            "loss: 1.217230  [ 2976/ 3200]\n",
            "loss: 1.143724  [ 2992/ 3200]\n",
            "loss: 1.253017  [ 3008/ 3200]\n",
            "loss: 1.173746  [ 3024/ 3200]\n",
            "loss: 1.118383  [ 3040/ 3200]\n",
            "loss: 1.095669  [ 3056/ 3200]\n",
            "loss: 1.107898  [ 3072/ 3200]\n",
            "loss: 1.257378  [ 3088/ 3200]\n",
            "loss: 1.139353  [ 3104/ 3200]\n",
            "loss: 1.241545  [ 3120/ 3200]\n",
            "loss: 1.233029  [ 3136/ 3200]\n",
            "loss: 1.232479  [ 3152/ 3200]\n",
            "loss: 1.122759  [ 3168/ 3200]\n",
            "loss: 1.204226  [ 3184/ 3200]\n",
            "current epoch: 18\n",
            "\n",
            "loss: 1.194960  [    0/ 3200]\n",
            "loss: 1.220947  [   16/ 3200]\n",
            "loss: 1.170206  [   32/ 3200]\n",
            "loss: 1.099040  [   48/ 3200]\n",
            "loss: 1.274444  [   64/ 3200]\n",
            "loss: 1.164584  [   80/ 3200]\n",
            "loss: 1.258838  [   96/ 3200]\n",
            "loss: 1.329568  [  112/ 3200]\n",
            "loss: 1.220296  [  128/ 3200]\n",
            "loss: 1.160836  [  144/ 3200]\n",
            "loss: 1.153523  [  160/ 3200]\n",
            "loss: 1.188920  [  176/ 3200]\n",
            "loss: 1.147478  [  192/ 3200]\n",
            "loss: 1.250701  [  208/ 3200]\n",
            "loss: 1.097973  [  224/ 3200]\n",
            "loss: 1.201064  [  240/ 3200]\n",
            "loss: 1.190246  [  256/ 3200]\n",
            "loss: 1.145518  [  272/ 3200]\n",
            "loss: 1.169849  [  288/ 3200]\n",
            "loss: 1.090987  [  304/ 3200]\n",
            "loss: 1.335463  [  320/ 3200]\n",
            "loss: 1.155011  [  336/ 3200]\n",
            "loss: 1.272647  [  352/ 3200]\n",
            "loss: 1.166266  [  368/ 3200]\n",
            "loss: 1.170285  [  384/ 3200]\n",
            "loss: 1.226774  [  400/ 3200]\n",
            "loss: 1.169327  [  416/ 3200]\n",
            "loss: 1.047912  [  432/ 3200]\n",
            "loss: 1.131270  [  448/ 3200]\n",
            "loss: 1.193261  [  464/ 3200]\n",
            "loss: 1.167043  [  480/ 3200]\n",
            "loss: 1.302617  [  496/ 3200]\n",
            "loss: 1.202513  [  512/ 3200]\n",
            "loss: 1.200067  [  528/ 3200]\n",
            "loss: 1.135699  [  544/ 3200]\n",
            "loss: 1.223211  [  560/ 3200]\n",
            "loss: 1.280429  [  576/ 3200]\n",
            "loss: 1.221721  [  592/ 3200]\n",
            "loss: 1.215227  [  608/ 3200]\n",
            "loss: 1.162295  [  624/ 3200]\n",
            "loss: 1.278369  [  640/ 3200]\n",
            "loss: 1.317847  [  656/ 3200]\n",
            "loss: 1.218489  [  672/ 3200]\n",
            "loss: 1.416931  [  688/ 3200]\n",
            "loss: 1.216510  [  704/ 3200]\n",
            "loss: 1.279203  [  720/ 3200]\n",
            "loss: 1.214570  [  736/ 3200]\n",
            "loss: 1.078377  [  752/ 3200]\n",
            "loss: 1.136634  [  768/ 3200]\n",
            "loss: 1.127221  [  784/ 3200]\n",
            "loss: 1.258557  [  800/ 3200]\n",
            "loss: 1.236703  [  816/ 3200]\n",
            "loss: 1.194794  [  832/ 3200]\n",
            "loss: 1.238368  [  848/ 3200]\n",
            "loss: 1.098152  [  864/ 3200]\n",
            "loss: 1.290461  [  880/ 3200]\n",
            "loss: 1.187478  [  896/ 3200]\n",
            "loss: 1.188357  [  912/ 3200]\n",
            "loss: 1.140254  [  928/ 3200]\n",
            "loss: 1.227926  [  944/ 3200]\n",
            "loss: 1.179991  [  960/ 3200]\n",
            "loss: 1.139140  [  976/ 3200]\n",
            "loss: 1.142315  [  992/ 3200]\n",
            "loss: 1.186314  [ 1008/ 3200]\n",
            "loss: 1.152466  [ 1024/ 3200]\n",
            "loss: 1.269291  [ 1040/ 3200]\n",
            "loss: 1.162868  [ 1056/ 3200]\n",
            "loss: 1.013835  [ 1072/ 3200]\n",
            "loss: 1.270409  [ 1088/ 3200]\n",
            "loss: 1.188088  [ 1104/ 3200]\n",
            "loss: 1.196031  [ 1120/ 3200]\n",
            "loss: 1.318461  [ 1136/ 3200]\n",
            "loss: 1.233141  [ 1152/ 3200]\n",
            "loss: 1.111496  [ 1168/ 3200]\n",
            "loss: 1.191692  [ 1184/ 3200]\n",
            "loss: 1.117420  [ 1200/ 3200]\n",
            "loss: 1.182897  [ 1216/ 3200]\n",
            "loss: 1.177715  [ 1232/ 3200]\n",
            "loss: 1.092515  [ 1248/ 3200]\n",
            "loss: 1.189592  [ 1264/ 3200]\n",
            "loss: 1.011332  [ 1280/ 3200]\n",
            "loss: 1.344246  [ 1296/ 3200]\n",
            "loss: 1.151577  [ 1312/ 3200]\n",
            "loss: 1.073859  [ 1328/ 3200]\n",
            "loss: 1.234338  [ 1344/ 3200]\n",
            "loss: 1.255095  [ 1360/ 3200]\n",
            "loss: 1.015804  [ 1376/ 3200]\n",
            "loss: 1.237123  [ 1392/ 3200]\n",
            "loss: 1.251942  [ 1408/ 3200]\n",
            "loss: 1.159084  [ 1424/ 3200]\n",
            "loss: 1.158511  [ 1440/ 3200]\n",
            "loss: 1.162409  [ 1456/ 3200]\n",
            "loss: 1.178328  [ 1472/ 3200]\n",
            "loss: 1.280848  [ 1488/ 3200]\n",
            "loss: 1.240150  [ 1504/ 3200]\n",
            "loss: 1.266973  [ 1520/ 3200]\n",
            "loss: 1.188418  [ 1536/ 3200]\n",
            "loss: 1.196180  [ 1552/ 3200]\n",
            "loss: 1.131836  [ 1568/ 3200]\n",
            "loss: 1.275254  [ 1584/ 3200]\n",
            "loss: 1.190573  [ 1600/ 3200]\n",
            "loss: 1.221506  [ 1616/ 3200]\n",
            "loss: 1.169347  [ 1632/ 3200]\n",
            "loss: 1.037554  [ 1648/ 3200]\n",
            "loss: 1.096879  [ 1664/ 3200]\n",
            "loss: 1.224153  [ 1680/ 3200]\n",
            "loss: 1.279321  [ 1696/ 3200]\n",
            "loss: 1.289387  [ 1712/ 3200]\n",
            "loss: 1.397553  [ 1728/ 3200]\n",
            "loss: 1.219313  [ 1744/ 3200]\n",
            "loss: 1.250067  [ 1760/ 3200]\n",
            "loss: 1.207151  [ 1776/ 3200]\n",
            "loss: 1.207979  [ 1792/ 3200]\n",
            "loss: 1.281212  [ 1808/ 3200]\n",
            "loss: 1.303354  [ 1824/ 3200]\n",
            "loss: 1.122947  [ 1840/ 3200]\n",
            "loss: 1.162329  [ 1856/ 3200]\n",
            "loss: 1.212586  [ 1872/ 3200]\n",
            "loss: 1.151376  [ 1888/ 3200]\n",
            "loss: 1.194956  [ 1904/ 3200]\n",
            "loss: 1.220450  [ 1920/ 3200]\n",
            "loss: 1.214489  [ 1936/ 3200]\n",
            "loss: 1.230962  [ 1952/ 3200]\n",
            "loss: 1.026527  [ 1968/ 3200]\n",
            "loss: 1.199295  [ 1984/ 3200]\n",
            "loss: 1.164686  [ 2000/ 3200]\n",
            "loss: 1.272541  [ 2016/ 3200]\n",
            "loss: 1.222489  [ 2032/ 3200]\n",
            "loss: 1.246311  [ 2048/ 3200]\n",
            "loss: 1.204531  [ 2064/ 3200]\n",
            "loss: 1.024582  [ 2080/ 3200]\n",
            "loss: 1.030111  [ 2096/ 3200]\n",
            "loss: 1.133299  [ 2112/ 3200]\n",
            "loss: 1.075117  [ 2128/ 3200]\n",
            "loss: 1.163006  [ 2144/ 3200]\n",
            "loss: 1.202325  [ 2160/ 3200]\n",
            "loss: 1.206844  [ 2176/ 3200]\n",
            "loss: 1.005158  [ 2192/ 3200]\n",
            "loss: 1.171443  [ 2208/ 3200]\n",
            "loss: 1.205083  [ 2224/ 3200]\n",
            "loss: 1.150175  [ 2240/ 3200]\n",
            "loss: 1.201985  [ 2256/ 3200]\n",
            "loss: 1.219163  [ 2272/ 3200]\n",
            "loss: 1.153597  [ 2288/ 3200]\n",
            "loss: 1.247808  [ 2304/ 3200]\n",
            "loss: 1.206946  [ 2320/ 3200]\n",
            "loss: 1.224794  [ 2336/ 3200]\n",
            "loss: 1.316795  [ 2352/ 3200]\n",
            "loss: 1.364118  [ 2368/ 3200]\n",
            "loss: 1.262888  [ 2384/ 3200]\n",
            "loss: 1.264418  [ 2400/ 3200]\n",
            "loss: 1.005084  [ 2416/ 3200]\n",
            "loss: 1.199312  [ 2432/ 3200]\n",
            "loss: 1.166013  [ 2448/ 3200]\n",
            "loss: 1.279060  [ 2464/ 3200]\n",
            "loss: 1.121446  [ 2480/ 3200]\n",
            "loss: 1.014937  [ 2496/ 3200]\n",
            "loss: 1.169677  [ 2512/ 3200]\n",
            "loss: 1.181868  [ 2528/ 3200]\n",
            "loss: 1.080618  [ 2544/ 3200]\n",
            "loss: 1.160565  [ 2560/ 3200]\n",
            "loss: 1.203539  [ 2576/ 3200]\n",
            "loss: 1.218645  [ 2592/ 3200]\n",
            "loss: 1.161941  [ 2608/ 3200]\n",
            "loss: 1.202434  [ 2624/ 3200]\n",
            "loss: 1.256647  [ 2640/ 3200]\n",
            "loss: 1.187068  [ 2656/ 3200]\n",
            "loss: 1.174318  [ 2672/ 3200]\n",
            "loss: 1.175660  [ 2688/ 3200]\n",
            "loss: 1.192020  [ 2704/ 3200]\n",
            "loss: 1.159289  [ 2720/ 3200]\n",
            "loss: 1.154613  [ 2736/ 3200]\n",
            "loss: 1.179152  [ 2752/ 3200]\n",
            "loss: 1.123075  [ 2768/ 3200]\n",
            "loss: 1.077617  [ 2784/ 3200]\n",
            "loss: 1.238207  [ 2800/ 3200]\n",
            "loss: 1.189021  [ 2816/ 3200]\n",
            "loss: 1.130906  [ 2832/ 3200]\n",
            "loss: 1.131407  [ 2848/ 3200]\n",
            "loss: 1.338783  [ 2864/ 3200]\n",
            "loss: 1.025422  [ 2880/ 3200]\n",
            "loss: 1.123541  [ 2896/ 3200]\n",
            "loss: 1.097657  [ 2912/ 3200]\n",
            "loss: 1.127385  [ 2928/ 3200]\n",
            "loss: 1.154915  [ 2944/ 3200]\n",
            "loss: 1.208726  [ 2960/ 3200]\n",
            "loss: 1.054653  [ 2976/ 3200]\n",
            "loss: 1.017187  [ 2992/ 3200]\n",
            "loss: 1.205771  [ 3008/ 3200]\n",
            "loss: 1.185029  [ 3024/ 3200]\n",
            "loss: 1.293729  [ 3040/ 3200]\n",
            "loss: 1.036419  [ 3056/ 3200]\n",
            "loss: 1.146974  [ 3072/ 3200]\n",
            "loss: 1.286146  [ 3088/ 3200]\n",
            "loss: 1.193747  [ 3104/ 3200]\n",
            "loss: 1.104822  [ 3120/ 3200]\n",
            "loss: 1.255524  [ 3136/ 3200]\n",
            "loss: 1.119112  [ 3152/ 3200]\n",
            "loss: 1.209114  [ 3168/ 3200]\n",
            "loss: 1.118700  [ 3184/ 3200]\n",
            "current epoch: 19\n",
            "\n",
            "loss: 0.984259  [    0/ 3200]\n",
            "loss: 1.138259  [   16/ 3200]\n",
            "loss: 1.090703  [   32/ 3200]\n",
            "loss: 1.126306  [   48/ 3200]\n",
            "loss: 1.079903  [   64/ 3200]\n",
            "loss: 1.066153  [   80/ 3200]\n",
            "loss: 1.016472  [   96/ 3200]\n",
            "loss: 1.082918  [  112/ 3200]\n",
            "loss: 1.234945  [  128/ 3200]\n",
            "loss: 1.310387  [  144/ 3200]\n",
            "loss: 1.201822  [  160/ 3200]\n",
            "loss: 1.115227  [  176/ 3200]\n",
            "loss: 1.140090  [  192/ 3200]\n",
            "loss: 1.191904  [  208/ 3200]\n",
            "loss: 1.033268  [  224/ 3200]\n",
            "loss: 0.998860  [  240/ 3200]\n",
            "loss: 0.921016  [  256/ 3200]\n",
            "loss: 1.211260  [  272/ 3200]\n",
            "loss: 1.260118  [  288/ 3200]\n",
            "loss: 1.287914  [  304/ 3200]\n",
            "loss: 1.111576  [  320/ 3200]\n",
            "loss: 1.145911  [  336/ 3200]\n",
            "loss: 1.096549  [  352/ 3200]\n",
            "loss: 1.291173  [  368/ 3200]\n",
            "loss: 1.210617  [  384/ 3200]\n",
            "loss: 1.245345  [  400/ 3200]\n",
            "loss: 1.108919  [  416/ 3200]\n",
            "loss: 1.186550  [  432/ 3200]\n",
            "loss: 1.009066  [  448/ 3200]\n",
            "loss: 1.036708  [  464/ 3200]\n",
            "loss: 1.208300  [  480/ 3200]\n",
            "loss: 1.165939  [  496/ 3200]\n",
            "loss: 1.058097  [  512/ 3200]\n",
            "loss: 1.238419  [  528/ 3200]\n",
            "loss: 1.064960  [  544/ 3200]\n",
            "loss: 1.116445  [  560/ 3200]\n",
            "loss: 1.181731  [  576/ 3200]\n",
            "loss: 1.216189  [  592/ 3200]\n",
            "loss: 1.185895  [  608/ 3200]\n",
            "loss: 1.165075  [  624/ 3200]\n",
            "loss: 1.188770  [  640/ 3200]\n",
            "loss: 1.219395  [  656/ 3200]\n",
            "loss: 1.073351  [  672/ 3200]\n",
            "loss: 1.143432  [  688/ 3200]\n",
            "loss: 1.276876  [  704/ 3200]\n",
            "loss: 1.109746  [  720/ 3200]\n",
            "loss: 1.176947  [  736/ 3200]\n",
            "loss: 1.273889  [  752/ 3200]\n",
            "loss: 1.231596  [  768/ 3200]\n",
            "loss: 1.148196  [  784/ 3200]\n",
            "loss: 1.150760  [  800/ 3200]\n",
            "loss: 1.252281  [  816/ 3200]\n",
            "loss: 1.224254  [  832/ 3200]\n",
            "loss: 1.114621  [  848/ 3200]\n",
            "loss: 1.225561  [  864/ 3200]\n",
            "loss: 1.131287  [  880/ 3200]\n",
            "loss: 1.075968  [  896/ 3200]\n",
            "loss: 1.131299  [  912/ 3200]\n",
            "loss: 1.083504  [  928/ 3200]\n",
            "loss: 1.260581  [  944/ 3200]\n",
            "loss: 1.156032  [  960/ 3200]\n",
            "loss: 1.067975  [  976/ 3200]\n",
            "loss: 1.165324  [  992/ 3200]\n",
            "loss: 1.127020  [ 1008/ 3200]\n",
            "loss: 1.239961  [ 1024/ 3200]\n",
            "loss: 1.098742  [ 1040/ 3200]\n",
            "loss: 1.220800  [ 1056/ 3200]\n",
            "loss: 1.268821  [ 1072/ 3200]\n",
            "loss: 1.325931  [ 1088/ 3200]\n",
            "loss: 1.274669  [ 1104/ 3200]\n",
            "loss: 1.119979  [ 1120/ 3200]\n",
            "loss: 1.214451  [ 1136/ 3200]\n",
            "loss: 1.065604  [ 1152/ 3200]\n",
            "loss: 1.311648  [ 1168/ 3200]\n",
            "loss: 1.193811  [ 1184/ 3200]\n",
            "loss: 1.055773  [ 1200/ 3200]\n",
            "loss: 0.942404  [ 1216/ 3200]\n",
            "loss: 1.143042  [ 1232/ 3200]\n",
            "loss: 1.112821  [ 1248/ 3200]\n",
            "loss: 1.269446  [ 1264/ 3200]\n",
            "loss: 1.313221  [ 1280/ 3200]\n",
            "loss: 1.141634  [ 1296/ 3200]\n",
            "loss: 1.151881  [ 1312/ 3200]\n",
            "loss: 1.109723  [ 1328/ 3200]\n",
            "loss: 1.156525  [ 1344/ 3200]\n",
            "loss: 1.205341  [ 1360/ 3200]\n",
            "loss: 1.165356  [ 1376/ 3200]\n",
            "loss: 1.344098  [ 1392/ 3200]\n",
            "loss: 1.194565  [ 1408/ 3200]\n",
            "loss: 1.236582  [ 1424/ 3200]\n",
            "loss: 1.055487  [ 1440/ 3200]\n",
            "loss: 1.227844  [ 1456/ 3200]\n",
            "loss: 1.178995  [ 1472/ 3200]\n",
            "loss: 1.098875  [ 1488/ 3200]\n",
            "loss: 1.230189  [ 1504/ 3200]\n",
            "loss: 1.254939  [ 1520/ 3200]\n",
            "loss: 1.119657  [ 1536/ 3200]\n",
            "loss: 1.118708  [ 1552/ 3200]\n",
            "loss: 1.202994  [ 1568/ 3200]\n",
            "loss: 1.318302  [ 1584/ 3200]\n",
            "loss: 1.260094  [ 1600/ 3200]\n",
            "loss: 1.080065  [ 1616/ 3200]\n",
            "loss: 1.228440  [ 1632/ 3200]\n",
            "loss: 1.199032  [ 1648/ 3200]\n",
            "loss: 1.168596  [ 1664/ 3200]\n",
            "loss: 1.286268  [ 1680/ 3200]\n",
            "loss: 1.202999  [ 1696/ 3200]\n",
            "loss: 1.284860  [ 1712/ 3200]\n",
            "loss: 1.100233  [ 1728/ 3200]\n",
            "loss: 1.275430  [ 1744/ 3200]\n",
            "loss: 1.229772  [ 1760/ 3200]\n",
            "loss: 1.176359  [ 1776/ 3200]\n",
            "loss: 1.120720  [ 1792/ 3200]\n",
            "loss: 1.098570  [ 1808/ 3200]\n",
            "loss: 1.077340  [ 1824/ 3200]\n",
            "loss: 1.224110  [ 1840/ 3200]\n",
            "loss: 1.250826  [ 1856/ 3200]\n",
            "loss: 1.303300  [ 1872/ 3200]\n",
            "loss: 1.143719  [ 1888/ 3200]\n",
            "loss: 1.170499  [ 1904/ 3200]\n",
            "loss: 1.151233  [ 1920/ 3200]\n",
            "loss: 1.107088  [ 1936/ 3200]\n",
            "loss: 1.188866  [ 1952/ 3200]\n",
            "loss: 1.046278  [ 1968/ 3200]\n",
            "loss: 1.240832  [ 1984/ 3200]\n",
            "loss: 1.156561  [ 2000/ 3200]\n",
            "loss: 1.147177  [ 2016/ 3200]\n",
            "loss: 1.151762  [ 2032/ 3200]\n",
            "loss: 1.163976  [ 2048/ 3200]\n",
            "loss: 1.264087  [ 2064/ 3200]\n",
            "loss: 1.092866  [ 2080/ 3200]\n",
            "loss: 1.222031  [ 2096/ 3200]\n",
            "loss: 1.188817  [ 2112/ 3200]\n",
            "loss: 1.097065  [ 2128/ 3200]\n",
            "loss: 1.038230  [ 2144/ 3200]\n",
            "loss: 1.192769  [ 2160/ 3200]\n",
            "loss: 1.108992  [ 2176/ 3200]\n",
            "loss: 1.094076  [ 2192/ 3200]\n",
            "loss: 1.187246  [ 2208/ 3200]\n",
            "loss: 0.991477  [ 2224/ 3200]\n",
            "loss: 1.070297  [ 2240/ 3200]\n",
            "loss: 1.132217  [ 2256/ 3200]\n",
            "loss: 1.202633  [ 2272/ 3200]\n",
            "loss: 1.171948  [ 2288/ 3200]\n",
            "loss: 1.041166  [ 2304/ 3200]\n",
            "loss: 1.147640  [ 2320/ 3200]\n",
            "loss: 1.143886  [ 2336/ 3200]\n",
            "loss: 1.309260  [ 2352/ 3200]\n",
            "loss: 1.119494  [ 2368/ 3200]\n",
            "loss: 1.226337  [ 2384/ 3200]\n",
            "loss: 1.100242  [ 2400/ 3200]\n",
            "loss: 1.106760  [ 2416/ 3200]\n",
            "loss: 1.165595  [ 2432/ 3200]\n",
            "loss: 1.213255  [ 2448/ 3200]\n",
            "loss: 1.037060  [ 2464/ 3200]\n",
            "loss: 1.286542  [ 2480/ 3200]\n",
            "loss: 1.246984  [ 2496/ 3200]\n",
            "loss: 1.067165  [ 2512/ 3200]\n",
            "loss: 1.308740  [ 2528/ 3200]\n",
            "loss: 1.148708  [ 2544/ 3200]\n",
            "loss: 1.287858  [ 2560/ 3200]\n",
            "loss: 1.120054  [ 2576/ 3200]\n",
            "loss: 1.151997  [ 2592/ 3200]\n",
            "loss: 1.161367  [ 2608/ 3200]\n",
            "loss: 1.232327  [ 2624/ 3200]\n",
            "loss: 1.123427  [ 2640/ 3200]\n",
            "loss: 1.165719  [ 2656/ 3200]\n",
            "loss: 1.204633  [ 2672/ 3200]\n",
            "loss: 1.192006  [ 2688/ 3200]\n",
            "loss: 1.092533  [ 2704/ 3200]\n",
            "loss: 1.142337  [ 2720/ 3200]\n",
            "loss: 1.015664  [ 2736/ 3200]\n",
            "loss: 1.145024  [ 2752/ 3200]\n",
            "loss: 1.188026  [ 2768/ 3200]\n",
            "loss: 1.226251  [ 2784/ 3200]\n",
            "loss: 1.123013  [ 2800/ 3200]\n",
            "loss: 1.167112  [ 2816/ 3200]\n",
            "loss: 1.117466  [ 2832/ 3200]\n",
            "loss: 1.278132  [ 2848/ 3200]\n",
            "loss: 0.950672  [ 2864/ 3200]\n",
            "loss: 1.079240  [ 2880/ 3200]\n",
            "loss: 1.198324  [ 2896/ 3200]\n",
            "loss: 1.256835  [ 2912/ 3200]\n",
            "loss: 1.362361  [ 2928/ 3200]\n",
            "loss: 1.146682  [ 2944/ 3200]\n",
            "loss: 1.323853  [ 2960/ 3200]\n",
            "loss: 1.153903  [ 2976/ 3200]\n",
            "loss: 1.160700  [ 2992/ 3200]\n",
            "loss: 1.176246  [ 3008/ 3200]\n",
            "loss: 1.091321  [ 3024/ 3200]\n",
            "loss: 1.182269  [ 3040/ 3200]\n",
            "loss: 1.107393  [ 3056/ 3200]\n",
            "loss: 1.332468  [ 3072/ 3200]\n",
            "loss: 1.112866  [ 3088/ 3200]\n",
            "loss: 1.138913  [ 3104/ 3200]\n",
            "loss: 1.182172  [ 3120/ 3200]\n",
            "loss: 1.149631  [ 3136/ 3200]\n",
            "loss: 1.159936  [ 3152/ 3200]\n",
            "loss: 1.087133  [ 3168/ 3200]\n",
            "loss: 1.085125  [ 3184/ 3200]\n",
            "current epoch: 20\n",
            "\n",
            "loss: 1.117695  [    0/ 3200]\n",
            "loss: 1.176222  [   16/ 3200]\n",
            "loss: 1.104306  [   32/ 3200]\n",
            "loss: 1.058477  [   48/ 3200]\n",
            "loss: 1.124420  [   64/ 3200]\n",
            "loss: 1.261581  [   80/ 3200]\n",
            "loss: 1.183921  [   96/ 3200]\n",
            "loss: 1.169333  [  112/ 3200]\n",
            "loss: 1.154593  [  128/ 3200]\n",
            "loss: 1.200219  [  144/ 3200]\n",
            "loss: 1.254130  [  160/ 3200]\n",
            "loss: 1.236809  [  176/ 3200]\n",
            "loss: 1.054879  [  192/ 3200]\n",
            "loss: 1.124399  [  208/ 3200]\n",
            "loss: 1.205010  [  224/ 3200]\n",
            "loss: 1.273648  [  240/ 3200]\n",
            "loss: 1.198110  [  256/ 3200]\n",
            "loss: 1.178579  [  272/ 3200]\n",
            "loss: 1.249205  [  288/ 3200]\n",
            "loss: 1.074827  [  304/ 3200]\n",
            "loss: 1.256666  [  320/ 3200]\n",
            "loss: 1.173307  [  336/ 3200]\n",
            "loss: 1.235388  [  352/ 3200]\n",
            "loss: 1.098864  [  368/ 3200]\n",
            "loss: 1.192457  [  384/ 3200]\n",
            "loss: 1.155425  [  400/ 3200]\n",
            "loss: 1.280728  [  416/ 3200]\n",
            "loss: 1.295590  [  432/ 3200]\n",
            "loss: 1.127627  [  448/ 3200]\n",
            "loss: 1.291240  [  464/ 3200]\n",
            "loss: 1.072877  [  480/ 3200]\n",
            "loss: 1.112368  [  496/ 3200]\n",
            "loss: 1.089455  [  512/ 3200]\n",
            "loss: 1.220170  [  528/ 3200]\n",
            "loss: 1.030057  [  544/ 3200]\n",
            "loss: 1.200065  [  560/ 3200]\n",
            "loss: 1.095467  [  576/ 3200]\n",
            "loss: 1.235824  [  592/ 3200]\n",
            "loss: 1.116108  [  608/ 3200]\n",
            "loss: 1.232387  [  624/ 3200]\n",
            "loss: 1.196492  [  640/ 3200]\n",
            "loss: 1.147027  [  656/ 3200]\n",
            "loss: 1.133919  [  672/ 3200]\n",
            "loss: 1.285274  [  688/ 3200]\n",
            "loss: 1.143148  [  704/ 3200]\n",
            "loss: 1.225871  [  720/ 3200]\n",
            "loss: 1.352255  [  736/ 3200]\n",
            "loss: 1.214646  [  752/ 3200]\n",
            "loss: 1.070700  [  768/ 3200]\n",
            "loss: 1.067047  [  784/ 3200]\n",
            "loss: 0.994336  [  800/ 3200]\n",
            "loss: 1.276956  [  816/ 3200]\n",
            "loss: 1.161716  [  832/ 3200]\n",
            "loss: 1.039445  [  848/ 3200]\n",
            "loss: 1.083014  [  864/ 3200]\n",
            "loss: 1.121442  [  880/ 3200]\n",
            "loss: 1.037232  [  896/ 3200]\n",
            "loss: 1.362470  [  912/ 3200]\n",
            "loss: 0.978222  [  928/ 3200]\n",
            "loss: 1.277381  [  944/ 3200]\n",
            "loss: 1.261753  [  960/ 3200]\n",
            "loss: 1.232889  [  976/ 3200]\n",
            "loss: 1.149534  [  992/ 3200]\n",
            "loss: 1.121604  [ 1008/ 3200]\n",
            "loss: 1.108824  [ 1024/ 3200]\n",
            "loss: 1.232600  [ 1040/ 3200]\n",
            "loss: 1.233622  [ 1056/ 3200]\n",
            "loss: 1.137272  [ 1072/ 3200]\n",
            "loss: 1.192903  [ 1088/ 3200]\n",
            "loss: 1.083924  [ 1104/ 3200]\n",
            "loss: 1.230201  [ 1120/ 3200]\n",
            "loss: 1.195272  [ 1136/ 3200]\n",
            "loss: 1.090221  [ 1152/ 3200]\n",
            "loss: 1.097049  [ 1168/ 3200]\n",
            "loss: 1.349452  [ 1184/ 3200]\n",
            "loss: 1.212657  [ 1200/ 3200]\n",
            "loss: 1.150067  [ 1216/ 3200]\n",
            "loss: 1.117044  [ 1232/ 3200]\n",
            "loss: 1.068886  [ 1248/ 3200]\n",
            "loss: 1.025895  [ 1264/ 3200]\n",
            "loss: 1.050656  [ 1280/ 3200]\n",
            "loss: 1.030268  [ 1296/ 3200]\n",
            "loss: 1.054910  [ 1312/ 3200]\n",
            "loss: 1.134807  [ 1328/ 3200]\n",
            "loss: 1.074922  [ 1344/ 3200]\n",
            "loss: 1.327746  [ 1360/ 3200]\n",
            "loss: 1.110786  [ 1376/ 3200]\n",
            "loss: 1.184270  [ 1392/ 3200]\n",
            "loss: 0.985243  [ 1408/ 3200]\n",
            "loss: 1.258899  [ 1424/ 3200]\n",
            "loss: 1.132292  [ 1440/ 3200]\n",
            "loss: 1.024152  [ 1456/ 3200]\n",
            "loss: 0.958316  [ 1472/ 3200]\n",
            "loss: 1.237100  [ 1488/ 3200]\n",
            "loss: 1.024859  [ 1504/ 3200]\n",
            "loss: 1.287925  [ 1520/ 3200]\n",
            "loss: 1.110749  [ 1536/ 3200]\n",
            "loss: 1.067581  [ 1552/ 3200]\n",
            "loss: 1.168752  [ 1568/ 3200]\n",
            "loss: 1.213130  [ 1584/ 3200]\n",
            "loss: 1.073294  [ 1600/ 3200]\n",
            "loss: 1.084286  [ 1616/ 3200]\n",
            "loss: 1.096485  [ 1632/ 3200]\n",
            "loss: 1.069308  [ 1648/ 3200]\n",
            "loss: 1.093286  [ 1664/ 3200]\n",
            "loss: 1.040968  [ 1680/ 3200]\n",
            "loss: 1.067804  [ 1696/ 3200]\n",
            "loss: 1.117969  [ 1712/ 3200]\n",
            "loss: 1.086823  [ 1728/ 3200]\n",
            "loss: 0.999435  [ 1744/ 3200]\n",
            "loss: 1.209123  [ 1760/ 3200]\n",
            "loss: 1.145467  [ 1776/ 3200]\n",
            "loss: 1.210657  [ 1792/ 3200]\n",
            "loss: 1.270204  [ 1808/ 3200]\n",
            "loss: 1.168010  [ 1824/ 3200]\n",
            "loss: 1.182116  [ 1840/ 3200]\n",
            "loss: 1.142352  [ 1856/ 3200]\n",
            "loss: 1.119630  [ 1872/ 3200]\n",
            "loss: 1.153013  [ 1888/ 3200]\n",
            "loss: 1.358320  [ 1904/ 3200]\n",
            "loss: 1.229832  [ 1920/ 3200]\n",
            "loss: 1.204183  [ 1936/ 3200]\n",
            "loss: 1.224567  [ 1952/ 3200]\n",
            "loss: 1.105924  [ 1968/ 3200]\n",
            "loss: 1.339457  [ 1984/ 3200]\n",
            "loss: 1.052163  [ 2000/ 3200]\n",
            "loss: 1.218599  [ 2016/ 3200]\n",
            "loss: 1.107808  [ 2032/ 3200]\n",
            "loss: 1.066992  [ 2048/ 3200]\n",
            "loss: 0.982721  [ 2064/ 3200]\n",
            "loss: 1.141378  [ 2080/ 3200]\n",
            "loss: 1.054743  [ 2096/ 3200]\n",
            "loss: 1.164959  [ 2112/ 3200]\n",
            "loss: 1.207625  [ 2128/ 3200]\n",
            "loss: 1.122714  [ 2144/ 3200]\n",
            "loss: 1.174656  [ 2160/ 3200]\n",
            "loss: 1.053469  [ 2176/ 3200]\n",
            "loss: 0.914576  [ 2192/ 3200]\n",
            "loss: 1.117207  [ 2208/ 3200]\n",
            "loss: 1.035196  [ 2224/ 3200]\n",
            "loss: 1.193804  [ 2240/ 3200]\n",
            "loss: 1.162748  [ 2256/ 3200]\n",
            "loss: 1.040908  [ 2272/ 3200]\n",
            "loss: 0.953852  [ 2288/ 3200]\n",
            "loss: 1.231577  [ 2304/ 3200]\n",
            "loss: 1.181929  [ 2320/ 3200]\n",
            "loss: 1.118459  [ 2336/ 3200]\n",
            "loss: 1.214981  [ 2352/ 3200]\n",
            "loss: 1.330997  [ 2368/ 3200]\n",
            "loss: 1.135458  [ 2384/ 3200]\n",
            "loss: 1.127728  [ 2400/ 3200]\n",
            "loss: 1.171924  [ 2416/ 3200]\n",
            "loss: 1.190803  [ 2432/ 3200]\n",
            "loss: 1.260081  [ 2448/ 3200]\n",
            "loss: 1.087945  [ 2464/ 3200]\n",
            "loss: 1.103220  [ 2480/ 3200]\n",
            "loss: 1.239483  [ 2496/ 3200]\n",
            "loss: 1.211129  [ 2512/ 3200]\n",
            "loss: 1.026873  [ 2528/ 3200]\n",
            "loss: 1.061318  [ 2544/ 3200]\n",
            "loss: 1.065590  [ 2560/ 3200]\n",
            "loss: 1.041678  [ 2576/ 3200]\n",
            "loss: 1.318861  [ 2592/ 3200]\n",
            "loss: 1.373039  [ 2608/ 3200]\n",
            "loss: 1.114118  [ 2624/ 3200]\n",
            "loss: 1.055563  [ 2640/ 3200]\n",
            "loss: 1.197554  [ 2656/ 3200]\n",
            "loss: 1.083903  [ 2672/ 3200]\n",
            "loss: 1.109633  [ 2688/ 3200]\n",
            "loss: 1.133845  [ 2704/ 3200]\n",
            "loss: 1.174595  [ 2720/ 3200]\n",
            "loss: 1.242289  [ 2736/ 3200]\n",
            "loss: 1.178441  [ 2752/ 3200]\n",
            "loss: 1.014143  [ 2768/ 3200]\n",
            "loss: 1.082437  [ 2784/ 3200]\n",
            "loss: 1.252882  [ 2800/ 3200]\n",
            "loss: 1.108523  [ 2816/ 3200]\n",
            "loss: 1.178512  [ 2832/ 3200]\n",
            "loss: 1.075809  [ 2848/ 3200]\n",
            "loss: 1.101334  [ 2864/ 3200]\n",
            "loss: 1.096318  [ 2880/ 3200]\n",
            "loss: 1.104638  [ 2896/ 3200]\n",
            "loss: 1.137446  [ 2912/ 3200]\n",
            "loss: 1.027867  [ 2928/ 3200]\n",
            "loss: 1.046675  [ 2944/ 3200]\n",
            "loss: 1.127475  [ 2960/ 3200]\n",
            "loss: 1.166784  [ 2976/ 3200]\n",
            "loss: 1.259015  [ 2992/ 3200]\n",
            "loss: 1.016084  [ 3008/ 3200]\n",
            "loss: 1.186279  [ 3024/ 3200]\n",
            "loss: 1.190152  [ 3040/ 3200]\n",
            "loss: 1.113658  [ 3056/ 3200]\n",
            "loss: 1.064017  [ 3072/ 3200]\n",
            "loss: 1.202108  [ 3088/ 3200]\n",
            "loss: 1.344446  [ 3104/ 3200]\n",
            "loss: 1.191921  [ 3120/ 3200]\n",
            "loss: 1.157614  [ 3136/ 3200]\n",
            "loss: 1.040285  [ 3152/ 3200]\n",
            "loss: 1.165411  [ 3168/ 3200]\n",
            "loss: 1.189654  [ 3184/ 3200]\n",
            "current epoch: 21\n",
            "\n",
            "loss: 1.011046  [    0/ 3200]\n",
            "loss: 1.117218  [   16/ 3200]\n",
            "loss: 1.263675  [   32/ 3200]\n",
            "loss: 1.417097  [   48/ 3200]\n",
            "loss: 1.086078  [   64/ 3200]\n",
            "loss: 0.996720  [   80/ 3200]\n",
            "loss: 1.136767  [   96/ 3200]\n",
            "loss: 1.176452  [  112/ 3200]\n",
            "loss: 1.139716  [  128/ 3200]\n",
            "loss: 1.034048  [  144/ 3200]\n",
            "loss: 1.189939  [  160/ 3200]\n",
            "loss: 1.193298  [  176/ 3200]\n",
            "loss: 1.191222  [  192/ 3200]\n",
            "loss: 1.214993  [  208/ 3200]\n",
            "loss: 1.033492  [  224/ 3200]\n",
            "loss: 1.095037  [  240/ 3200]\n",
            "loss: 1.233436  [  256/ 3200]\n",
            "loss: 1.228064  [  272/ 3200]\n",
            "loss: 1.212791  [  288/ 3200]\n",
            "loss: 1.286208  [  304/ 3200]\n",
            "loss: 1.091465  [  320/ 3200]\n",
            "loss: 1.089868  [  336/ 3200]\n",
            "loss: 1.152127  [  352/ 3200]\n",
            "loss: 1.001038  [  368/ 3200]\n",
            "loss: 1.168670  [  384/ 3200]\n",
            "loss: 1.033792  [  400/ 3200]\n",
            "loss: 1.085639  [  416/ 3200]\n",
            "loss: 1.165923  [  432/ 3200]\n",
            "loss: 0.993285  [  448/ 3200]\n",
            "loss: 1.224217  [  464/ 3200]\n",
            "loss: 1.209731  [  480/ 3200]\n",
            "loss: 1.089403  [  496/ 3200]\n",
            "loss: 1.160942  [  512/ 3200]\n",
            "loss: 1.168400  [  528/ 3200]\n",
            "loss: 1.221335  [  544/ 3200]\n",
            "loss: 1.164766  [  560/ 3200]\n",
            "loss: 1.148677  [  576/ 3200]\n",
            "loss: 1.022324  [  592/ 3200]\n",
            "loss: 1.280843  [  608/ 3200]\n",
            "loss: 1.189942  [  624/ 3200]\n",
            "loss: 1.079180  [  640/ 3200]\n",
            "loss: 1.167390  [  656/ 3200]\n",
            "loss: 1.096791  [  672/ 3200]\n",
            "loss: 1.131778  [  688/ 3200]\n",
            "loss: 1.016869  [  704/ 3200]\n",
            "loss: 1.104720  [  720/ 3200]\n",
            "loss: 1.006525  [  736/ 3200]\n",
            "loss: 1.142876  [  752/ 3200]\n",
            "loss: 1.169850  [  768/ 3200]\n",
            "loss: 1.030947  [  784/ 3200]\n",
            "loss: 1.070156  [  800/ 3200]\n",
            "loss: 1.144110  [  816/ 3200]\n",
            "loss: 1.031865  [  832/ 3200]\n",
            "loss: 1.120268  [  848/ 3200]\n",
            "loss: 1.141009  [  864/ 3200]\n",
            "loss: 1.125889  [  880/ 3200]\n",
            "loss: 1.032566  [  896/ 3200]\n",
            "loss: 1.192252  [  912/ 3200]\n",
            "loss: 1.217437  [  928/ 3200]\n",
            "loss: 1.151484  [  944/ 3200]\n",
            "loss: 1.200816  [  960/ 3200]\n",
            "loss: 1.044672  [  976/ 3200]\n",
            "loss: 1.040606  [  992/ 3200]\n",
            "loss: 1.157551  [ 1008/ 3200]\n",
            "loss: 1.274874  [ 1024/ 3200]\n",
            "loss: 1.222198  [ 1040/ 3200]\n",
            "loss: 1.197028  [ 1056/ 3200]\n",
            "loss: 1.212287  [ 1072/ 3200]\n",
            "loss: 1.111455  [ 1088/ 3200]\n",
            "loss: 1.222295  [ 1104/ 3200]\n",
            "loss: 1.173417  [ 1120/ 3200]\n",
            "loss: 1.102984  [ 1136/ 3200]\n",
            "loss: 1.230519  [ 1152/ 3200]\n",
            "loss: 1.286755  [ 1168/ 3200]\n",
            "loss: 1.252908  [ 1184/ 3200]\n",
            "loss: 1.144992  [ 1200/ 3200]\n",
            "loss: 1.001412  [ 1216/ 3200]\n",
            "loss: 1.088696  [ 1232/ 3200]\n",
            "loss: 1.078096  [ 1248/ 3200]\n",
            "loss: 1.044527  [ 1264/ 3200]\n",
            "loss: 0.919768  [ 1280/ 3200]\n",
            "loss: 0.973813  [ 1296/ 3200]\n",
            "loss: 1.042030  [ 1312/ 3200]\n",
            "loss: 1.234531  [ 1328/ 3200]\n",
            "loss: 1.184075  [ 1344/ 3200]\n",
            "loss: 1.223134  [ 1360/ 3200]\n",
            "loss: 1.063775  [ 1376/ 3200]\n",
            "loss: 1.104849  [ 1392/ 3200]\n",
            "loss: 1.131356  [ 1408/ 3200]\n",
            "loss: 1.098375  [ 1424/ 3200]\n",
            "loss: 1.356685  [ 1440/ 3200]\n",
            "loss: 1.146111  [ 1456/ 3200]\n",
            "loss: 1.107523  [ 1472/ 3200]\n",
            "loss: 1.243753  [ 1488/ 3200]\n",
            "loss: 1.031267  [ 1504/ 3200]\n",
            "loss: 1.068273  [ 1520/ 3200]\n",
            "loss: 1.380032  [ 1536/ 3200]\n",
            "loss: 1.161621  [ 1552/ 3200]\n",
            "loss: 1.205356  [ 1568/ 3200]\n",
            "loss: 1.023551  [ 1584/ 3200]\n",
            "loss: 1.213514  [ 1600/ 3200]\n",
            "loss: 1.172884  [ 1616/ 3200]\n",
            "loss: 1.270939  [ 1632/ 3200]\n",
            "loss: 1.051513  [ 1648/ 3200]\n",
            "loss: 1.226448  [ 1664/ 3200]\n",
            "loss: 1.106972  [ 1680/ 3200]\n",
            "loss: 1.102172  [ 1696/ 3200]\n",
            "loss: 1.155481  [ 1712/ 3200]\n",
            "loss: 1.134523  [ 1728/ 3200]\n",
            "loss: 1.143353  [ 1744/ 3200]\n",
            "loss: 1.026163  [ 1760/ 3200]\n",
            "loss: 1.133445  [ 1776/ 3200]\n",
            "loss: 1.266942  [ 1792/ 3200]\n",
            "loss: 1.201044  [ 1808/ 3200]\n",
            "loss: 0.972464  [ 1824/ 3200]\n",
            "loss: 0.932305  [ 1840/ 3200]\n",
            "loss: 1.000832  [ 1856/ 3200]\n",
            "loss: 1.174811  [ 1872/ 3200]\n",
            "loss: 1.092235  [ 1888/ 3200]\n",
            "loss: 1.194571  [ 1904/ 3200]\n",
            "loss: 1.192998  [ 1920/ 3200]\n",
            "loss: 0.975430  [ 1936/ 3200]\n",
            "loss: 1.126386  [ 1952/ 3200]\n",
            "loss: 1.119437  [ 1968/ 3200]\n",
            "loss: 1.068117  [ 1984/ 3200]\n",
            "loss: 1.192612  [ 2000/ 3200]\n",
            "loss: 1.124468  [ 2016/ 3200]\n",
            "loss: 1.267017  [ 2032/ 3200]\n",
            "loss: 1.221348  [ 2048/ 3200]\n",
            "loss: 1.129520  [ 2064/ 3200]\n",
            "loss: 1.105741  [ 2080/ 3200]\n",
            "loss: 1.073284  [ 2096/ 3200]\n",
            "loss: 1.039574  [ 2112/ 3200]\n",
            "loss: 1.094907  [ 2128/ 3200]\n",
            "loss: 1.242579  [ 2144/ 3200]\n",
            "loss: 1.127258  [ 2160/ 3200]\n",
            "loss: 1.149950  [ 2176/ 3200]\n",
            "loss: 1.148711  [ 2192/ 3200]\n",
            "loss: 1.116445  [ 2208/ 3200]\n",
            "loss: 1.063309  [ 2224/ 3200]\n",
            "loss: 1.153729  [ 2240/ 3200]\n",
            "loss: 1.162915  [ 2256/ 3200]\n",
            "loss: 1.131372  [ 2272/ 3200]\n",
            "loss: 1.254920  [ 2288/ 3200]\n",
            "loss: 1.143608  [ 2304/ 3200]\n",
            "loss: 1.000119  [ 2320/ 3200]\n",
            "loss: 1.039407  [ 2336/ 3200]\n",
            "loss: 1.167237  [ 2352/ 3200]\n",
            "loss: 1.134414  [ 2368/ 3200]\n",
            "loss: 1.107308  [ 2384/ 3200]\n",
            "loss: 1.009547  [ 2400/ 3200]\n",
            "loss: 1.067243  [ 2416/ 3200]\n",
            "loss: 1.042714  [ 2432/ 3200]\n",
            "loss: 1.158665  [ 2448/ 3200]\n",
            "loss: 1.161485  [ 2464/ 3200]\n",
            "loss: 1.022317  [ 2480/ 3200]\n",
            "loss: 1.126138  [ 2496/ 3200]\n",
            "loss: 1.148176  [ 2512/ 3200]\n",
            "loss: 1.091771  [ 2528/ 3200]\n",
            "loss: 1.107671  [ 2544/ 3200]\n",
            "loss: 1.246880  [ 2560/ 3200]\n",
            "loss: 1.165236  [ 2576/ 3200]\n",
            "loss: 1.180190  [ 2592/ 3200]\n",
            "loss: 1.217461  [ 2608/ 3200]\n",
            "loss: 1.155788  [ 2624/ 3200]\n",
            "loss: 1.059328  [ 2640/ 3200]\n",
            "loss: 1.044148  [ 2656/ 3200]\n",
            "loss: 1.040726  [ 2672/ 3200]\n",
            "loss: 1.140006  [ 2688/ 3200]\n",
            "loss: 1.319405  [ 2704/ 3200]\n",
            "loss: 1.079421  [ 2720/ 3200]\n",
            "loss: 0.967223  [ 2736/ 3200]\n",
            "loss: 1.298028  [ 2752/ 3200]\n",
            "loss: 1.025624  [ 2768/ 3200]\n",
            "loss: 1.152447  [ 2784/ 3200]\n",
            "loss: 1.197963  [ 2800/ 3200]\n",
            "loss: 0.993977  [ 2816/ 3200]\n",
            "loss: 1.120241  [ 2832/ 3200]\n",
            "loss: 1.237896  [ 2848/ 3200]\n",
            "loss: 1.086182  [ 2864/ 3200]\n",
            "loss: 0.980699  [ 2880/ 3200]\n",
            "loss: 1.157032  [ 2896/ 3200]\n",
            "loss: 1.145851  [ 2912/ 3200]\n",
            "loss: 1.216578  [ 2928/ 3200]\n",
            "loss: 1.205649  [ 2944/ 3200]\n",
            "loss: 1.227192  [ 2960/ 3200]\n",
            "loss: 1.127560  [ 2976/ 3200]\n",
            "loss: 1.040877  [ 2992/ 3200]\n",
            "loss: 0.995437  [ 3008/ 3200]\n",
            "loss: 1.294943  [ 3024/ 3200]\n",
            "loss: 1.275700  [ 3040/ 3200]\n",
            "loss: 1.010899  [ 3056/ 3200]\n",
            "loss: 1.034477  [ 3072/ 3200]\n",
            "loss: 1.057250  [ 3088/ 3200]\n",
            "loss: 1.106236  [ 3104/ 3200]\n",
            "loss: 1.034041  [ 3120/ 3200]\n",
            "loss: 1.113477  [ 3136/ 3200]\n",
            "loss: 1.106187  [ 3152/ 3200]\n",
            "loss: 1.138406  [ 3168/ 3200]\n",
            "loss: 1.094208  [ 3184/ 3200]\n",
            "current epoch: 22\n",
            "\n",
            "loss: 0.995831  [    0/ 3200]\n",
            "loss: 1.193393  [   16/ 3200]\n",
            "loss: 1.091884  [   32/ 3200]\n",
            "loss: 1.043233  [   48/ 3200]\n",
            "loss: 1.081971  [   64/ 3200]\n",
            "loss: 1.039491  [   80/ 3200]\n",
            "loss: 1.174173  [   96/ 3200]\n",
            "loss: 1.186570  [  112/ 3200]\n",
            "loss: 1.164321  [  128/ 3200]\n",
            "loss: 1.007697  [  144/ 3200]\n",
            "loss: 1.275743  [  160/ 3200]\n",
            "loss: 1.056330  [  176/ 3200]\n",
            "loss: 1.064026  [  192/ 3200]\n",
            "loss: 1.039840  [  208/ 3200]\n",
            "loss: 1.208414  [  224/ 3200]\n",
            "loss: 1.123818  [  240/ 3200]\n",
            "loss: 1.182246  [  256/ 3200]\n",
            "loss: 1.154161  [  272/ 3200]\n",
            "loss: 1.045490  [  288/ 3200]\n",
            "loss: 1.022281  [  304/ 3200]\n",
            "loss: 1.229078  [  320/ 3200]\n",
            "loss: 1.138988  [  336/ 3200]\n",
            "loss: 1.173961  [  352/ 3200]\n",
            "loss: 1.034354  [  368/ 3200]\n",
            "loss: 0.921705  [  384/ 3200]\n",
            "loss: 1.055410  [  400/ 3200]\n",
            "loss: 1.018647  [  416/ 3200]\n",
            "loss: 1.139434  [  432/ 3200]\n",
            "loss: 1.109963  [  448/ 3200]\n",
            "loss: 1.057798  [  464/ 3200]\n",
            "loss: 1.141245  [  480/ 3200]\n",
            "loss: 1.110563  [  496/ 3200]\n",
            "loss: 1.189498  [  512/ 3200]\n",
            "loss: 1.283794  [  528/ 3200]\n",
            "loss: 1.203712  [  544/ 3200]\n",
            "loss: 1.289540  [  560/ 3200]\n",
            "loss: 1.007629  [  576/ 3200]\n",
            "loss: 1.027321  [  592/ 3200]\n",
            "loss: 1.170586  [  608/ 3200]\n",
            "loss: 1.050909  [  624/ 3200]\n",
            "loss: 1.239738  [  640/ 3200]\n",
            "loss: 1.180129  [  656/ 3200]\n",
            "loss: 1.072338  [  672/ 3200]\n",
            "loss: 1.169043  [  688/ 3200]\n",
            "loss: 1.319880  [  704/ 3200]\n",
            "loss: 1.277341  [  720/ 3200]\n",
            "loss: 1.176357  [  736/ 3200]\n",
            "loss: 1.073962  [  752/ 3200]\n",
            "loss: 1.162034  [  768/ 3200]\n",
            "loss: 1.098668  [  784/ 3200]\n",
            "loss: 1.174783  [  800/ 3200]\n",
            "loss: 1.035590  [  816/ 3200]\n",
            "loss: 1.061735  [  832/ 3200]\n",
            "loss: 1.085939  [  848/ 3200]\n",
            "loss: 0.978467  [  864/ 3200]\n",
            "loss: 1.043013  [  880/ 3200]\n",
            "loss: 0.998336  [  896/ 3200]\n",
            "loss: 1.274138  [  912/ 3200]\n",
            "loss: 1.091141  [  928/ 3200]\n",
            "loss: 1.150285  [  944/ 3200]\n",
            "loss: 0.974519  [  960/ 3200]\n",
            "loss: 1.087038  [  976/ 3200]\n",
            "loss: 1.129284  [  992/ 3200]\n",
            "loss: 0.951327  [ 1008/ 3200]\n",
            "loss: 0.982423  [ 1024/ 3200]\n",
            "loss: 0.983743  [ 1040/ 3200]\n",
            "loss: 1.150347  [ 1056/ 3200]\n",
            "loss: 1.227737  [ 1072/ 3200]\n",
            "loss: 1.137973  [ 1088/ 3200]\n",
            "loss: 1.202822  [ 1104/ 3200]\n",
            "loss: 1.007346  [ 1120/ 3200]\n",
            "loss: 1.131485  [ 1136/ 3200]\n",
            "loss: 1.283630  [ 1152/ 3200]\n",
            "loss: 1.030176  [ 1168/ 3200]\n",
            "loss: 1.203605  [ 1184/ 3200]\n",
            "loss: 1.134922  [ 1200/ 3200]\n",
            "loss: 0.941460  [ 1216/ 3200]\n",
            "loss: 0.999177  [ 1232/ 3200]\n",
            "loss: 1.090916  [ 1248/ 3200]\n",
            "loss: 1.319387  [ 1264/ 3200]\n",
            "loss: 0.982333  [ 1280/ 3200]\n",
            "loss: 1.184214  [ 1296/ 3200]\n",
            "loss: 1.125779  [ 1312/ 3200]\n",
            "loss: 1.154815  [ 1328/ 3200]\n",
            "loss: 0.968067  [ 1344/ 3200]\n",
            "loss: 1.143498  [ 1360/ 3200]\n",
            "loss: 1.120736  [ 1376/ 3200]\n",
            "loss: 1.073112  [ 1392/ 3200]\n",
            "loss: 1.098991  [ 1408/ 3200]\n",
            "loss: 1.132792  [ 1424/ 3200]\n",
            "loss: 0.948973  [ 1440/ 3200]\n",
            "loss: 1.138750  [ 1456/ 3200]\n",
            "loss: 1.004408  [ 1472/ 3200]\n",
            "loss: 1.001136  [ 1488/ 3200]\n",
            "loss: 1.115668  [ 1504/ 3200]\n",
            "loss: 1.147465  [ 1520/ 3200]\n",
            "loss: 1.107706  [ 1536/ 3200]\n",
            "loss: 0.913751  [ 1552/ 3200]\n",
            "loss: 1.251550  [ 1568/ 3200]\n",
            "loss: 1.067004  [ 1584/ 3200]\n",
            "loss: 1.072605  [ 1600/ 3200]\n",
            "loss: 1.184310  [ 1616/ 3200]\n",
            "loss: 1.132280  [ 1632/ 3200]\n",
            "loss: 1.136069  [ 1648/ 3200]\n",
            "loss: 1.231652  [ 1664/ 3200]\n",
            "loss: 0.966417  [ 1680/ 3200]\n",
            "loss: 1.058882  [ 1696/ 3200]\n",
            "loss: 0.999095  [ 1712/ 3200]\n",
            "loss: 1.257779  [ 1728/ 3200]\n",
            "loss: 1.104400  [ 1744/ 3200]\n",
            "loss: 1.111974  [ 1760/ 3200]\n",
            "loss: 1.127258  [ 1776/ 3200]\n",
            "loss: 0.952569  [ 1792/ 3200]\n",
            "loss: 0.990685  [ 1808/ 3200]\n",
            "loss: 1.177935  [ 1824/ 3200]\n",
            "loss: 1.174553  [ 1840/ 3200]\n",
            "loss: 1.085304  [ 1856/ 3200]\n",
            "loss: 1.055773  [ 1872/ 3200]\n",
            "loss: 1.078650  [ 1888/ 3200]\n",
            "loss: 1.112827  [ 1904/ 3200]\n",
            "loss: 1.014072  [ 1920/ 3200]\n",
            "loss: 1.086269  [ 1936/ 3200]\n",
            "loss: 1.025931  [ 1952/ 3200]\n",
            "loss: 1.106653  [ 1968/ 3200]\n",
            "loss: 1.183209  [ 1984/ 3200]\n",
            "loss: 1.254434  [ 2000/ 3200]\n",
            "loss: 1.164987  [ 2016/ 3200]\n",
            "loss: 1.276722  [ 2032/ 3200]\n",
            "loss: 0.999407  [ 2048/ 3200]\n",
            "loss: 0.995314  [ 2064/ 3200]\n",
            "loss: 1.172233  [ 2080/ 3200]\n",
            "loss: 1.149530  [ 2096/ 3200]\n",
            "loss: 1.063734  [ 2112/ 3200]\n",
            "loss: 1.195519  [ 2128/ 3200]\n",
            "loss: 1.078396  [ 2144/ 3200]\n",
            "loss: 1.112255  [ 2160/ 3200]\n",
            "loss: 1.256763  [ 2176/ 3200]\n",
            "loss: 1.060253  [ 2192/ 3200]\n",
            "loss: 0.974670  [ 2208/ 3200]\n",
            "loss: 1.209451  [ 2224/ 3200]\n",
            "loss: 1.107117  [ 2240/ 3200]\n",
            "loss: 1.134740  [ 2256/ 3200]\n",
            "loss: 1.120379  [ 2272/ 3200]\n",
            "loss: 1.049490  [ 2288/ 3200]\n",
            "loss: 1.117706  [ 2304/ 3200]\n",
            "loss: 1.063862  [ 2320/ 3200]\n",
            "loss: 1.125062  [ 2336/ 3200]\n",
            "loss: 1.061146  [ 2352/ 3200]\n",
            "loss: 1.197608  [ 2368/ 3200]\n",
            "loss: 1.182372  [ 2384/ 3200]\n",
            "loss: 1.188966  [ 2400/ 3200]\n",
            "loss: 0.931398  [ 2416/ 3200]\n",
            "loss: 0.980777  [ 2432/ 3200]\n",
            "loss: 1.262051  [ 2448/ 3200]\n",
            "loss: 1.229916  [ 2464/ 3200]\n",
            "loss: 0.915943  [ 2480/ 3200]\n",
            "loss: 0.965847  [ 2496/ 3200]\n",
            "loss: 1.415614  [ 2512/ 3200]\n",
            "loss: 1.171190  [ 2528/ 3200]\n",
            "loss: 1.185494  [ 2544/ 3200]\n",
            "loss: 1.147724  [ 2560/ 3200]\n",
            "loss: 1.108288  [ 2576/ 3200]\n",
            "loss: 1.079857  [ 2592/ 3200]\n",
            "loss: 1.033223  [ 2608/ 3200]\n",
            "loss: 1.143283  [ 2624/ 3200]\n",
            "loss: 1.455227  [ 2640/ 3200]\n",
            "loss: 1.128129  [ 2656/ 3200]\n",
            "loss: 1.269609  [ 2672/ 3200]\n",
            "loss: 1.137954  [ 2688/ 3200]\n",
            "loss: 1.130614  [ 2704/ 3200]\n",
            "loss: 1.177873  [ 2720/ 3200]\n",
            "loss: 0.872066  [ 2736/ 3200]\n",
            "loss: 1.170496  [ 2752/ 3200]\n",
            "loss: 1.128783  [ 2768/ 3200]\n",
            "loss: 1.179242  [ 2784/ 3200]\n",
            "loss: 1.136809  [ 2800/ 3200]\n",
            "loss: 1.372812  [ 2816/ 3200]\n",
            "loss: 1.129607  [ 2832/ 3200]\n",
            "loss: 1.072839  [ 2848/ 3200]\n",
            "loss: 1.055928  [ 2864/ 3200]\n",
            "loss: 1.264650  [ 2880/ 3200]\n",
            "loss: 1.065174  [ 2896/ 3200]\n",
            "loss: 0.964077  [ 2912/ 3200]\n",
            "loss: 1.042195  [ 2928/ 3200]\n",
            "loss: 1.264092  [ 2944/ 3200]\n",
            "loss: 1.192283  [ 2960/ 3200]\n",
            "loss: 1.056655  [ 2976/ 3200]\n",
            "loss: 1.141835  [ 2992/ 3200]\n",
            "loss: 1.149163  [ 3008/ 3200]\n",
            "loss: 1.124133  [ 3024/ 3200]\n",
            "loss: 1.221346  [ 3040/ 3200]\n",
            "loss: 1.148600  [ 3056/ 3200]\n",
            "loss: 1.252357  [ 3072/ 3200]\n",
            "loss: 1.071782  [ 3088/ 3200]\n",
            "loss: 0.923307  [ 3104/ 3200]\n",
            "loss: 1.311651  [ 3120/ 3200]\n",
            "loss: 1.211356  [ 3136/ 3200]\n",
            "loss: 1.136840  [ 3152/ 3200]\n",
            "loss: 1.154607  [ 3168/ 3200]\n",
            "loss: 0.997609  [ 3184/ 3200]\n",
            "current epoch: 23\n",
            "\n",
            "loss: 1.068310  [    0/ 3200]\n",
            "loss: 0.990216  [   16/ 3200]\n",
            "loss: 1.175822  [   32/ 3200]\n",
            "loss: 1.161401  [   48/ 3200]\n",
            "loss: 0.977664  [   64/ 3200]\n",
            "loss: 1.113742  [   80/ 3200]\n",
            "loss: 1.124437  [   96/ 3200]\n",
            "loss: 1.156295  [  112/ 3200]\n",
            "loss: 1.002296  [  128/ 3200]\n",
            "loss: 1.072095  [  144/ 3200]\n",
            "loss: 1.004024  [  160/ 3200]\n",
            "loss: 1.108519  [  176/ 3200]\n",
            "loss: 1.076290  [  192/ 3200]\n",
            "loss: 1.115517  [  208/ 3200]\n",
            "loss: 1.125769  [  224/ 3200]\n",
            "loss: 1.328218  [  240/ 3200]\n",
            "loss: 1.218693  [  256/ 3200]\n",
            "loss: 1.002840  [  272/ 3200]\n",
            "loss: 0.926858  [  288/ 3200]\n",
            "loss: 1.341713  [  304/ 3200]\n",
            "loss: 1.098732  [  320/ 3200]\n",
            "loss: 1.078518  [  336/ 3200]\n",
            "loss: 1.059172  [  352/ 3200]\n",
            "loss: 1.031844  [  368/ 3200]\n",
            "loss: 1.110255  [  384/ 3200]\n",
            "loss: 1.179875  [  400/ 3200]\n",
            "loss: 0.994477  [  416/ 3200]\n",
            "loss: 1.157296  [  432/ 3200]\n",
            "loss: 1.161628  [  448/ 3200]\n",
            "loss: 0.903419  [  464/ 3200]\n",
            "loss: 1.215257  [  480/ 3200]\n",
            "loss: 0.889348  [  496/ 3200]\n",
            "loss: 0.951701  [  512/ 3200]\n",
            "loss: 0.996356  [  528/ 3200]\n",
            "loss: 1.035612  [  544/ 3200]\n",
            "loss: 0.948907  [  560/ 3200]\n",
            "loss: 1.159657  [  576/ 3200]\n",
            "loss: 0.915072  [  592/ 3200]\n",
            "loss: 1.188877  [  608/ 3200]\n",
            "loss: 1.198275  [  624/ 3200]\n",
            "loss: 1.062359  [  640/ 3200]\n",
            "loss: 1.342603  [  656/ 3200]\n",
            "loss: 1.246320  [  672/ 3200]\n",
            "loss: 1.011237  [  688/ 3200]\n",
            "loss: 1.140348  [  704/ 3200]\n",
            "loss: 1.091344  [  720/ 3200]\n",
            "loss: 1.228194  [  736/ 3200]\n",
            "loss: 1.209959  [  752/ 3200]\n",
            "loss: 1.040288  [  768/ 3200]\n",
            "loss: 1.058879  [  784/ 3200]\n",
            "loss: 1.044567  [  800/ 3200]\n",
            "loss: 1.005547  [  816/ 3200]\n",
            "loss: 1.140013  [  832/ 3200]\n",
            "loss: 0.972990  [  848/ 3200]\n",
            "loss: 1.158154  [  864/ 3200]\n",
            "loss: 0.929983  [  880/ 3200]\n",
            "loss: 1.137928  [  896/ 3200]\n",
            "loss: 1.130241  [  912/ 3200]\n",
            "loss: 0.851467  [  928/ 3200]\n",
            "loss: 1.046642  [  944/ 3200]\n",
            "loss: 1.040402  [  960/ 3200]\n",
            "loss: 1.011367  [  976/ 3200]\n",
            "loss: 1.268087  [  992/ 3200]\n",
            "loss: 1.028666  [ 1008/ 3200]\n",
            "loss: 1.061411  [ 1024/ 3200]\n",
            "loss: 1.112113  [ 1040/ 3200]\n",
            "loss: 1.131100  [ 1056/ 3200]\n",
            "loss: 0.942832  [ 1072/ 3200]\n",
            "loss: 1.084291  [ 1088/ 3200]\n",
            "loss: 1.189389  [ 1104/ 3200]\n",
            "loss: 1.065175  [ 1120/ 3200]\n",
            "loss: 1.164513  [ 1136/ 3200]\n",
            "loss: 1.260488  [ 1152/ 3200]\n",
            "loss: 1.167295  [ 1168/ 3200]\n",
            "loss: 1.154514  [ 1184/ 3200]\n",
            "loss: 1.149819  [ 1200/ 3200]\n",
            "loss: 1.223059  [ 1216/ 3200]\n",
            "loss: 0.860487  [ 1232/ 3200]\n",
            "loss: 0.988115  [ 1248/ 3200]\n",
            "loss: 1.248746  [ 1264/ 3200]\n",
            "loss: 0.915179  [ 1280/ 3200]\n",
            "loss: 1.099287  [ 1296/ 3200]\n",
            "loss: 1.110367  [ 1312/ 3200]\n",
            "loss: 0.995258  [ 1328/ 3200]\n",
            "loss: 1.200841  [ 1344/ 3200]\n",
            "loss: 1.046215  [ 1360/ 3200]\n",
            "loss: 1.159743  [ 1376/ 3200]\n",
            "loss: 1.021010  [ 1392/ 3200]\n",
            "loss: 1.159148  [ 1408/ 3200]\n",
            "loss: 1.298462  [ 1424/ 3200]\n",
            "loss: 0.909805  [ 1440/ 3200]\n",
            "loss: 1.031769  [ 1456/ 3200]\n",
            "loss: 1.037710  [ 1472/ 3200]\n",
            "loss: 1.393983  [ 1488/ 3200]\n",
            "loss: 1.154133  [ 1504/ 3200]\n",
            "loss: 1.160520  [ 1520/ 3200]\n",
            "loss: 1.084752  [ 1536/ 3200]\n",
            "loss: 1.245669  [ 1552/ 3200]\n",
            "loss: 1.046448  [ 1568/ 3200]\n",
            "loss: 1.175594  [ 1584/ 3200]\n",
            "loss: 1.132622  [ 1600/ 3200]\n",
            "loss: 1.237336  [ 1616/ 3200]\n",
            "loss: 1.257389  [ 1632/ 3200]\n",
            "loss: 0.965163  [ 1648/ 3200]\n",
            "loss: 1.189386  [ 1664/ 3200]\n",
            "loss: 0.860663  [ 1680/ 3200]\n",
            "loss: 1.030036  [ 1696/ 3200]\n",
            "loss: 1.012450  [ 1712/ 3200]\n",
            "loss: 1.206781  [ 1728/ 3200]\n",
            "loss: 0.851846  [ 1744/ 3200]\n",
            "loss: 1.075907  [ 1760/ 3200]\n",
            "loss: 1.099712  [ 1776/ 3200]\n",
            "loss: 1.225822  [ 1792/ 3200]\n",
            "loss: 0.956768  [ 1808/ 3200]\n",
            "loss: 1.091795  [ 1824/ 3200]\n",
            "loss: 1.036432  [ 1840/ 3200]\n",
            "loss: 1.057172  [ 1856/ 3200]\n",
            "loss: 1.137522  [ 1872/ 3200]\n",
            "loss: 1.071480  [ 1888/ 3200]\n",
            "loss: 1.097909  [ 1904/ 3200]\n",
            "loss: 1.188039  [ 1920/ 3200]\n",
            "loss: 1.023573  [ 1936/ 3200]\n",
            "loss: 0.818423  [ 1952/ 3200]\n",
            "loss: 1.130477  [ 1968/ 3200]\n",
            "loss: 0.877378  [ 1984/ 3200]\n",
            "loss: 1.164068  [ 2000/ 3200]\n",
            "loss: 1.103116  [ 2016/ 3200]\n",
            "loss: 1.009197  [ 2032/ 3200]\n",
            "loss: 1.379832  [ 2048/ 3200]\n",
            "loss: 1.006902  [ 2064/ 3200]\n",
            "loss: 1.150056  [ 2080/ 3200]\n",
            "loss: 1.209499  [ 2096/ 3200]\n",
            "loss: 1.049134  [ 2112/ 3200]\n",
            "loss: 1.080175  [ 2128/ 3200]\n",
            "loss: 1.194586  [ 2144/ 3200]\n",
            "loss: 1.210912  [ 2160/ 3200]\n",
            "loss: 1.023994  [ 2176/ 3200]\n",
            "loss: 1.194332  [ 2192/ 3200]\n",
            "loss: 1.168412  [ 2208/ 3200]\n",
            "loss: 1.210821  [ 2224/ 3200]\n",
            "loss: 0.893494  [ 2240/ 3200]\n",
            "loss: 1.143640  [ 2256/ 3200]\n",
            "loss: 1.105151  [ 2272/ 3200]\n",
            "loss: 1.165677  [ 2288/ 3200]\n",
            "loss: 1.050976  [ 2304/ 3200]\n",
            "loss: 1.134639  [ 2320/ 3200]\n",
            "loss: 1.138443  [ 2336/ 3200]\n",
            "loss: 1.081770  [ 2352/ 3200]\n",
            "loss: 1.282817  [ 2368/ 3200]\n",
            "loss: 1.074996  [ 2384/ 3200]\n",
            "loss: 1.063604  [ 2400/ 3200]\n",
            "loss: 1.175023  [ 2416/ 3200]\n",
            "loss: 1.035008  [ 2432/ 3200]\n",
            "loss: 0.968797  [ 2448/ 3200]\n",
            "loss: 1.075326  [ 2464/ 3200]\n",
            "loss: 1.110215  [ 2480/ 3200]\n",
            "loss: 1.080300  [ 2496/ 3200]\n",
            "loss: 1.193407  [ 2512/ 3200]\n",
            "loss: 1.129975  [ 2528/ 3200]\n",
            "loss: 1.091596  [ 2544/ 3200]\n",
            "loss: 1.009907  [ 2560/ 3200]\n",
            "loss: 1.093444  [ 2576/ 3200]\n",
            "loss: 1.046096  [ 2592/ 3200]\n",
            "loss: 1.054013  [ 2608/ 3200]\n",
            "loss: 1.127271  [ 2624/ 3200]\n",
            "loss: 1.098764  [ 2640/ 3200]\n",
            "loss: 1.181555  [ 2656/ 3200]\n",
            "loss: 1.049204  [ 2672/ 3200]\n",
            "loss: 1.161839  [ 2688/ 3200]\n",
            "loss: 1.010744  [ 2704/ 3200]\n",
            "loss: 1.070935  [ 2720/ 3200]\n",
            "loss: 1.103919  [ 2736/ 3200]\n",
            "loss: 1.128974  [ 2752/ 3200]\n",
            "loss: 0.846341  [ 2768/ 3200]\n",
            "loss: 1.178010  [ 2784/ 3200]\n",
            "loss: 0.874966  [ 2800/ 3200]\n",
            "loss: 0.983578  [ 2816/ 3200]\n",
            "loss: 1.173529  [ 2832/ 3200]\n",
            "loss: 1.320992  [ 2848/ 3200]\n",
            "loss: 1.252148  [ 2864/ 3200]\n",
            "loss: 1.134640  [ 2880/ 3200]\n",
            "loss: 1.190675  [ 2896/ 3200]\n",
            "loss: 1.105205  [ 2912/ 3200]\n",
            "loss: 1.016030  [ 2928/ 3200]\n",
            "loss: 1.110274  [ 2944/ 3200]\n",
            "loss: 1.036974  [ 2960/ 3200]\n",
            "loss: 1.261152  [ 2976/ 3200]\n",
            "loss: 1.336601  [ 2992/ 3200]\n",
            "loss: 1.061389  [ 3008/ 3200]\n",
            "loss: 0.947400  [ 3024/ 3200]\n",
            "loss: 1.037870  [ 3040/ 3200]\n",
            "loss: 1.237367  [ 3056/ 3200]\n",
            "loss: 1.196671  [ 3072/ 3200]\n",
            "loss: 1.043505  [ 3088/ 3200]\n",
            "loss: 1.128128  [ 3104/ 3200]\n",
            "loss: 1.159887  [ 3120/ 3200]\n",
            "loss: 1.048713  [ 3136/ 3200]\n",
            "loss: 1.260514  [ 3152/ 3200]\n",
            "loss: 1.155037  [ 3168/ 3200]\n",
            "loss: 1.163331  [ 3184/ 3200]\n",
            "current epoch: 24\n",
            "\n",
            "loss: 1.116594  [    0/ 3200]\n",
            "loss: 1.134520  [   16/ 3200]\n",
            "loss: 1.003673  [   32/ 3200]\n",
            "loss: 1.064461  [   48/ 3200]\n",
            "loss: 1.108136  [   64/ 3200]\n",
            "loss: 0.859971  [   80/ 3200]\n",
            "loss: 1.090586  [   96/ 3200]\n",
            "loss: 1.069907  [  112/ 3200]\n",
            "loss: 1.173821  [  128/ 3200]\n",
            "loss: 1.097625  [  144/ 3200]\n",
            "loss: 1.128223  [  160/ 3200]\n",
            "loss: 1.189193  [  176/ 3200]\n",
            "loss: 1.336805  [  192/ 3200]\n",
            "loss: 1.144432  [  208/ 3200]\n",
            "loss: 1.162646  [  224/ 3200]\n",
            "loss: 1.248160  [  240/ 3200]\n",
            "loss: 1.191198  [  256/ 3200]\n",
            "loss: 1.277751  [  272/ 3200]\n",
            "loss: 1.233132  [  288/ 3200]\n",
            "loss: 1.038261  [  304/ 3200]\n",
            "loss: 1.059165  [  320/ 3200]\n",
            "loss: 1.116334  [  336/ 3200]\n",
            "loss: 1.095555  [  352/ 3200]\n",
            "loss: 1.155047  [  368/ 3200]\n",
            "loss: 1.191634  [  384/ 3200]\n",
            "loss: 1.110562  [  400/ 3200]\n",
            "loss: 0.998334  [  416/ 3200]\n",
            "loss: 1.121943  [  432/ 3200]\n",
            "loss: 1.197467  [  448/ 3200]\n",
            "loss: 1.120136  [  464/ 3200]\n",
            "loss: 1.058557  [  480/ 3200]\n",
            "loss: 1.062695  [  496/ 3200]\n",
            "loss: 1.234694  [  512/ 3200]\n",
            "loss: 1.191595  [  528/ 3200]\n",
            "loss: 1.306205  [  544/ 3200]\n",
            "loss: 1.019704  [  560/ 3200]\n",
            "loss: 1.147364  [  576/ 3200]\n",
            "loss: 1.074581  [  592/ 3200]\n",
            "loss: 1.038268  [  608/ 3200]\n",
            "loss: 1.051585  [  624/ 3200]\n",
            "loss: 1.186081  [  640/ 3200]\n",
            "loss: 1.115084  [  656/ 3200]\n",
            "loss: 1.142080  [  672/ 3200]\n",
            "loss: 1.207355  [  688/ 3200]\n",
            "loss: 1.160292  [  704/ 3200]\n",
            "loss: 0.824776  [  720/ 3200]\n",
            "loss: 0.940890  [  736/ 3200]\n",
            "loss: 1.045726  [  752/ 3200]\n",
            "loss: 1.027119  [  768/ 3200]\n",
            "loss: 0.932146  [  784/ 3200]\n",
            "loss: 1.124522  [  800/ 3200]\n",
            "loss: 1.144181  [  816/ 3200]\n",
            "loss: 1.425855  [  832/ 3200]\n",
            "loss: 1.041295  [  848/ 3200]\n",
            "loss: 0.978250  [  864/ 3200]\n",
            "loss: 1.157213  [  880/ 3200]\n",
            "loss: 1.344386  [  896/ 3200]\n",
            "loss: 1.314966  [  912/ 3200]\n",
            "loss: 1.206474  [  928/ 3200]\n",
            "loss: 1.209627  [  944/ 3200]\n",
            "loss: 1.039277  [  960/ 3200]\n",
            "loss: 1.031842  [  976/ 3200]\n",
            "loss: 1.259310  [  992/ 3200]\n",
            "loss: 1.000013  [ 1008/ 3200]\n",
            "loss: 1.120180  [ 1024/ 3200]\n",
            "loss: 1.126273  [ 1040/ 3200]\n",
            "loss: 1.018132  [ 1056/ 3200]\n",
            "loss: 1.358254  [ 1072/ 3200]\n",
            "loss: 1.095971  [ 1088/ 3200]\n",
            "loss: 1.091853  [ 1104/ 3200]\n",
            "loss: 1.146231  [ 1120/ 3200]\n",
            "loss: 1.098706  [ 1136/ 3200]\n",
            "loss: 1.105574  [ 1152/ 3200]\n",
            "loss: 1.256291  [ 1168/ 3200]\n",
            "loss: 1.172958  [ 1184/ 3200]\n",
            "loss: 1.110577  [ 1200/ 3200]\n",
            "loss: 1.202801  [ 1216/ 3200]\n",
            "loss: 1.173812  [ 1232/ 3200]\n",
            "loss: 1.317271  [ 1248/ 3200]\n",
            "loss: 0.987952  [ 1264/ 3200]\n",
            "loss: 1.223679  [ 1280/ 3200]\n",
            "loss: 1.003258  [ 1296/ 3200]\n",
            "loss: 1.044558  [ 1312/ 3200]\n",
            "loss: 0.919139  [ 1328/ 3200]\n",
            "loss: 1.069754  [ 1344/ 3200]\n",
            "loss: 1.061562  [ 1360/ 3200]\n",
            "loss: 1.106533  [ 1376/ 3200]\n",
            "loss: 1.139471  [ 1392/ 3200]\n",
            "loss: 0.910902  [ 1408/ 3200]\n",
            "loss: 0.985655  [ 1424/ 3200]\n",
            "loss: 0.894498  [ 1440/ 3200]\n",
            "loss: 1.101658  [ 1456/ 3200]\n",
            "loss: 0.908352  [ 1472/ 3200]\n",
            "loss: 1.009778  [ 1488/ 3200]\n",
            "loss: 1.093216  [ 1504/ 3200]\n",
            "loss: 1.085439  [ 1520/ 3200]\n",
            "loss: 0.983394  [ 1536/ 3200]\n",
            "loss: 0.901742  [ 1552/ 3200]\n",
            "loss: 1.091249  [ 1568/ 3200]\n",
            "loss: 1.064240  [ 1584/ 3200]\n",
            "loss: 0.834046  [ 1600/ 3200]\n",
            "loss: 1.250459  [ 1616/ 3200]\n",
            "loss: 1.168483  [ 1632/ 3200]\n",
            "loss: 1.095630  [ 1648/ 3200]\n",
            "loss: 1.087530  [ 1664/ 3200]\n",
            "loss: 0.983384  [ 1680/ 3200]\n",
            "loss: 1.003091  [ 1696/ 3200]\n",
            "loss: 1.173579  [ 1712/ 3200]\n",
            "loss: 1.110397  [ 1728/ 3200]\n",
            "loss: 0.831322  [ 1744/ 3200]\n",
            "loss: 1.425853  [ 1760/ 3200]\n",
            "loss: 1.302193  [ 1776/ 3200]\n",
            "loss: 1.062105  [ 1792/ 3200]\n",
            "loss: 1.147257  [ 1808/ 3200]\n",
            "loss: 1.380938  [ 1824/ 3200]\n",
            "loss: 1.027847  [ 1840/ 3200]\n",
            "loss: 1.001254  [ 1856/ 3200]\n",
            "loss: 1.074255  [ 1872/ 3200]\n",
            "loss: 1.124469  [ 1888/ 3200]\n",
            "loss: 1.084401  [ 1904/ 3200]\n",
            "loss: 0.999563  [ 1920/ 3200]\n",
            "loss: 1.176523  [ 1936/ 3200]\n",
            "loss: 1.173222  [ 1952/ 3200]\n",
            "loss: 0.985408  [ 1968/ 3200]\n",
            "loss: 1.022469  [ 1984/ 3200]\n",
            "loss: 0.956994  [ 2000/ 3200]\n",
            "loss: 0.859754  [ 2016/ 3200]\n",
            "loss: 0.930869  [ 2032/ 3200]\n",
            "loss: 1.124422  [ 2048/ 3200]\n",
            "loss: 1.064370  [ 2064/ 3200]\n",
            "loss: 1.197588  [ 2080/ 3200]\n",
            "loss: 1.186344  [ 2096/ 3200]\n",
            "loss: 1.172984  [ 2112/ 3200]\n",
            "loss: 1.161705  [ 2128/ 3200]\n",
            "loss: 1.229620  [ 2144/ 3200]\n",
            "loss: 1.060068  [ 2160/ 3200]\n",
            "loss: 1.083890  [ 2176/ 3200]\n",
            "loss: 1.126687  [ 2192/ 3200]\n",
            "loss: 1.023820  [ 2208/ 3200]\n",
            "loss: 0.945553  [ 2224/ 3200]\n",
            "loss: 1.080812  [ 2240/ 3200]\n",
            "loss: 0.998627  [ 2256/ 3200]\n",
            "loss: 1.104860  [ 2272/ 3200]\n",
            "loss: 1.089119  [ 2288/ 3200]\n",
            "loss: 1.091073  [ 2304/ 3200]\n",
            "loss: 1.028605  [ 2320/ 3200]\n",
            "loss: 0.994959  [ 2336/ 3200]\n",
            "loss: 1.039760  [ 2352/ 3200]\n",
            "loss: 1.090208  [ 2368/ 3200]\n",
            "loss: 0.965696  [ 2384/ 3200]\n",
            "loss: 0.958155  [ 2400/ 3200]\n",
            "loss: 1.087617  [ 2416/ 3200]\n",
            "loss: 1.049189  [ 2432/ 3200]\n",
            "loss: 1.120995  [ 2448/ 3200]\n",
            "loss: 1.155663  [ 2464/ 3200]\n",
            "loss: 1.009912  [ 2480/ 3200]\n",
            "loss: 1.146475  [ 2496/ 3200]\n",
            "loss: 1.016645  [ 2512/ 3200]\n",
            "loss: 1.199585  [ 2528/ 3200]\n",
            "loss: 1.019341  [ 2544/ 3200]\n",
            "loss: 1.146205  [ 2560/ 3200]\n",
            "loss: 1.263503  [ 2576/ 3200]\n",
            "loss: 1.063022  [ 2592/ 3200]\n",
            "loss: 0.967988  [ 2608/ 3200]\n",
            "loss: 1.057422  [ 2624/ 3200]\n",
            "loss: 1.046472  [ 2640/ 3200]\n",
            "loss: 0.932829  [ 2656/ 3200]\n",
            "loss: 0.868284  [ 2672/ 3200]\n",
            "loss: 1.060040  [ 2688/ 3200]\n",
            "loss: 1.057282  [ 2704/ 3200]\n",
            "loss: 1.041793  [ 2720/ 3200]\n",
            "loss: 1.013380  [ 2736/ 3200]\n",
            "loss: 1.176123  [ 2752/ 3200]\n",
            "loss: 0.854131  [ 2768/ 3200]\n",
            "loss: 1.009269  [ 2784/ 3200]\n",
            "loss: 1.075217  [ 2800/ 3200]\n",
            "loss: 1.243760  [ 2816/ 3200]\n",
            "loss: 0.958035  [ 2832/ 3200]\n",
            "loss: 1.130031  [ 2848/ 3200]\n",
            "loss: 1.187121  [ 2864/ 3200]\n",
            "loss: 1.169540  [ 2880/ 3200]\n",
            "loss: 0.954472  [ 2896/ 3200]\n",
            "loss: 0.999587  [ 2912/ 3200]\n",
            "loss: 1.106345  [ 2928/ 3200]\n",
            "loss: 0.981125  [ 2944/ 3200]\n",
            "loss: 0.929259  [ 2960/ 3200]\n",
            "loss: 1.084412  [ 2976/ 3200]\n",
            "loss: 1.249448  [ 2992/ 3200]\n",
            "loss: 0.981726  [ 3008/ 3200]\n",
            "loss: 1.076334  [ 3024/ 3200]\n",
            "loss: 1.057171  [ 3040/ 3200]\n",
            "loss: 1.096007  [ 3056/ 3200]\n",
            "loss: 0.938551  [ 3072/ 3200]\n",
            "loss: 1.138077  [ 3088/ 3200]\n",
            "loss: 1.043741  [ 3104/ 3200]\n",
            "loss: 1.176790  [ 3120/ 3200]\n",
            "loss: 1.070630  [ 3136/ 3200]\n",
            "loss: 1.127300  [ 3152/ 3200]\n",
            "loss: 1.047441  [ 3168/ 3200]\n",
            "loss: 0.976545  [ 3184/ 3200]\n",
            "current epoch: 25\n",
            "\n",
            "loss: 0.983590  [    0/ 3200]\n",
            "loss: 1.019160  [   16/ 3200]\n",
            "loss: 1.292055  [   32/ 3200]\n",
            "loss: 1.082871  [   48/ 3200]\n",
            "loss: 1.114465  [   64/ 3200]\n",
            "loss: 1.012542  [   80/ 3200]\n",
            "loss: 0.989046  [   96/ 3200]\n",
            "loss: 1.119148  [  112/ 3200]\n",
            "loss: 0.888055  [  128/ 3200]\n",
            "loss: 1.062289  [  144/ 3200]\n",
            "loss: 1.146083  [  160/ 3200]\n",
            "loss: 1.075001  [  176/ 3200]\n",
            "loss: 0.980027  [  192/ 3200]\n",
            "loss: 0.950076  [  208/ 3200]\n",
            "loss: 0.998027  [  224/ 3200]\n",
            "loss: 1.276710  [  240/ 3200]\n",
            "loss: 0.937715  [  256/ 3200]\n",
            "loss: 1.009462  [  272/ 3200]\n",
            "loss: 0.886180  [  288/ 3200]\n",
            "loss: 1.247536  [  304/ 3200]\n",
            "loss: 1.063373  [  320/ 3200]\n",
            "loss: 1.263270  [  336/ 3200]\n",
            "loss: 0.943528  [  352/ 3200]\n",
            "loss: 1.041656  [  368/ 3200]\n",
            "loss: 1.104852  [  384/ 3200]\n",
            "loss: 1.138696  [  400/ 3200]\n",
            "loss: 0.897326  [  416/ 3200]\n",
            "loss: 1.039177  [  432/ 3200]\n",
            "loss: 1.107586  [  448/ 3200]\n",
            "loss: 1.136724  [  464/ 3200]\n",
            "loss: 1.051518  [  480/ 3200]\n",
            "loss: 1.049971  [  496/ 3200]\n",
            "loss: 1.033582  [  512/ 3200]\n",
            "loss: 1.054905  [  528/ 3200]\n",
            "loss: 1.132480  [  544/ 3200]\n",
            "loss: 1.076333  [  560/ 3200]\n",
            "loss: 1.212280  [  576/ 3200]\n",
            "loss: 1.029525  [  592/ 3200]\n",
            "loss: 1.110204  [  608/ 3200]\n",
            "loss: 0.847652  [  624/ 3200]\n",
            "loss: 1.060910  [  640/ 3200]\n",
            "loss: 1.115780  [  656/ 3200]\n",
            "loss: 1.130114  [  672/ 3200]\n",
            "loss: 1.075293  [  688/ 3200]\n",
            "loss: 1.178996  [  704/ 3200]\n",
            "loss: 1.007791  [  720/ 3200]\n",
            "loss: 1.208899  [  736/ 3200]\n",
            "loss: 0.930359  [  752/ 3200]\n",
            "loss: 1.071418  [  768/ 3200]\n",
            "loss: 1.129723  [  784/ 3200]\n",
            "loss: 1.080817  [  800/ 3200]\n",
            "loss: 0.922741  [  816/ 3200]\n",
            "loss: 1.176018  [  832/ 3200]\n",
            "loss: 1.217918  [  848/ 3200]\n",
            "loss: 1.112991  [  864/ 3200]\n",
            "loss: 1.158110  [  880/ 3200]\n",
            "loss: 1.029270  [  896/ 3200]\n",
            "loss: 1.100253  [  912/ 3200]\n",
            "loss: 1.218848  [  928/ 3200]\n",
            "loss: 1.016837  [  944/ 3200]\n",
            "loss: 1.308325  [  960/ 3200]\n",
            "loss: 0.941563  [  976/ 3200]\n",
            "loss: 1.095088  [  992/ 3200]\n",
            "loss: 1.244975  [ 1008/ 3200]\n",
            "loss: 1.270787  [ 1024/ 3200]\n",
            "loss: 1.120550  [ 1040/ 3200]\n",
            "loss: 0.848172  [ 1056/ 3200]\n",
            "loss: 1.174934  [ 1072/ 3200]\n",
            "loss: 0.962300  [ 1088/ 3200]\n",
            "loss: 1.076466  [ 1104/ 3200]\n",
            "loss: 1.172061  [ 1120/ 3200]\n",
            "loss: 1.083652  [ 1136/ 3200]\n",
            "loss: 1.037218  [ 1152/ 3200]\n",
            "loss: 1.066268  [ 1168/ 3200]\n",
            "loss: 0.972752  [ 1184/ 3200]\n",
            "loss: 1.185276  [ 1200/ 3200]\n",
            "loss: 1.022442  [ 1216/ 3200]\n",
            "loss: 1.192707  [ 1232/ 3200]\n",
            "loss: 1.043404  [ 1248/ 3200]\n",
            "loss: 1.186765  [ 1264/ 3200]\n",
            "loss: 1.215271  [ 1280/ 3200]\n",
            "loss: 0.915121  [ 1296/ 3200]\n",
            "loss: 0.954826  [ 1312/ 3200]\n",
            "loss: 1.075246  [ 1328/ 3200]\n",
            "loss: 1.113344  [ 1344/ 3200]\n",
            "loss: 1.002473  [ 1360/ 3200]\n",
            "loss: 1.184081  [ 1376/ 3200]\n",
            "loss: 1.155914  [ 1392/ 3200]\n",
            "loss: 1.142857  [ 1408/ 3200]\n",
            "loss: 0.905520  [ 1424/ 3200]\n",
            "loss: 0.972915  [ 1440/ 3200]\n",
            "loss: 1.090138  [ 1456/ 3200]\n",
            "loss: 0.958957  [ 1472/ 3200]\n",
            "loss: 1.116710  [ 1488/ 3200]\n",
            "loss: 1.228301  [ 1504/ 3200]\n",
            "loss: 1.134923  [ 1520/ 3200]\n",
            "loss: 1.087035  [ 1536/ 3200]\n",
            "loss: 1.377985  [ 1552/ 3200]\n",
            "loss: 1.206341  [ 1568/ 3200]\n",
            "loss: 0.990646  [ 1584/ 3200]\n",
            "loss: 1.106702  [ 1600/ 3200]\n",
            "loss: 0.948968  [ 1616/ 3200]\n",
            "loss: 1.347506  [ 1632/ 3200]\n",
            "loss: 1.152515  [ 1648/ 3200]\n",
            "loss: 0.864348  [ 1664/ 3200]\n",
            "loss: 0.984855  [ 1680/ 3200]\n",
            "loss: 1.051112  [ 1696/ 3200]\n",
            "loss: 1.105909  [ 1712/ 3200]\n",
            "loss: 1.067863  [ 1728/ 3200]\n",
            "loss: 1.000909  [ 1744/ 3200]\n",
            "loss: 1.188908  [ 1760/ 3200]\n",
            "loss: 0.862451  [ 1776/ 3200]\n",
            "loss: 1.255666  [ 1792/ 3200]\n",
            "loss: 1.179802  [ 1808/ 3200]\n",
            "loss: 0.938767  [ 1824/ 3200]\n",
            "loss: 1.047926  [ 1840/ 3200]\n",
            "loss: 1.179346  [ 1856/ 3200]\n",
            "loss: 1.019429  [ 1872/ 3200]\n",
            "loss: 1.104102  [ 1888/ 3200]\n",
            "loss: 1.041583  [ 1904/ 3200]\n",
            "loss: 1.323007  [ 1920/ 3200]\n",
            "loss: 1.167680  [ 1936/ 3200]\n",
            "loss: 1.178242  [ 1952/ 3200]\n",
            "loss: 1.111084  [ 1968/ 3200]\n",
            "loss: 0.969619  [ 1984/ 3200]\n",
            "loss: 1.061568  [ 2000/ 3200]\n",
            "loss: 1.279998  [ 2016/ 3200]\n",
            "loss: 0.728394  [ 2032/ 3200]\n",
            "loss: 1.160926  [ 2048/ 3200]\n",
            "loss: 0.998456  [ 2064/ 3200]\n",
            "loss: 1.254465  [ 2080/ 3200]\n",
            "loss: 1.148277  [ 2096/ 3200]\n",
            "loss: 0.950313  [ 2112/ 3200]\n",
            "loss: 1.041172  [ 2128/ 3200]\n",
            "loss: 0.963241  [ 2144/ 3200]\n",
            "loss: 0.923726  [ 2160/ 3200]\n",
            "loss: 1.150870  [ 2176/ 3200]\n",
            "loss: 0.875400  [ 2192/ 3200]\n",
            "loss: 1.142613  [ 2208/ 3200]\n",
            "loss: 1.007164  [ 2224/ 3200]\n",
            "loss: 1.104777  [ 2240/ 3200]\n",
            "loss: 1.112740  [ 2256/ 3200]\n",
            "loss: 1.042772  [ 2272/ 3200]\n",
            "loss: 0.942699  [ 2288/ 3200]\n",
            "loss: 1.107111  [ 2304/ 3200]\n",
            "loss: 1.050346  [ 2320/ 3200]\n",
            "loss: 0.894363  [ 2336/ 3200]\n",
            "loss: 1.023726  [ 2352/ 3200]\n",
            "loss: 1.183687  [ 2368/ 3200]\n",
            "loss: 1.174407  [ 2384/ 3200]\n",
            "loss: 1.017410  [ 2400/ 3200]\n",
            "loss: 1.042830  [ 2416/ 3200]\n",
            "loss: 0.997172  [ 2432/ 3200]\n",
            "loss: 1.033639  [ 2448/ 3200]\n",
            "loss: 0.962759  [ 2464/ 3200]\n",
            "loss: 1.207053  [ 2480/ 3200]\n",
            "loss: 1.082782  [ 2496/ 3200]\n",
            "loss: 0.976399  [ 2512/ 3200]\n",
            "loss: 1.062880  [ 2528/ 3200]\n",
            "loss: 1.008602  [ 2544/ 3200]\n",
            "loss: 1.252769  [ 2560/ 3200]\n",
            "loss: 1.054473  [ 2576/ 3200]\n",
            "loss: 1.103495  [ 2592/ 3200]\n",
            "loss: 1.021306  [ 2608/ 3200]\n",
            "loss: 0.988350  [ 2624/ 3200]\n",
            "loss: 1.061596  [ 2640/ 3200]\n",
            "loss: 0.998832  [ 2656/ 3200]\n",
            "loss: 1.282415  [ 2672/ 3200]\n",
            "loss: 0.923959  [ 2688/ 3200]\n",
            "loss: 1.316624  [ 2704/ 3200]\n",
            "loss: 1.052289  [ 2720/ 3200]\n",
            "loss: 0.960586  [ 2736/ 3200]\n",
            "loss: 1.055351  [ 2752/ 3200]\n",
            "loss: 1.020749  [ 2768/ 3200]\n",
            "loss: 0.963419  [ 2784/ 3200]\n",
            "loss: 1.188057  [ 2800/ 3200]\n",
            "loss: 1.162588  [ 2816/ 3200]\n",
            "loss: 1.151409  [ 2832/ 3200]\n",
            "loss: 1.040734  [ 2848/ 3200]\n",
            "loss: 1.012988  [ 2864/ 3200]\n",
            "loss: 0.999673  [ 2880/ 3200]\n",
            "loss: 0.954211  [ 2896/ 3200]\n",
            "loss: 0.992007  [ 2912/ 3200]\n",
            "loss: 1.035832  [ 2928/ 3200]\n",
            "loss: 1.000187  [ 2944/ 3200]\n",
            "loss: 1.026150  [ 2960/ 3200]\n",
            "loss: 0.910161  [ 2976/ 3200]\n",
            "loss: 1.296554  [ 2992/ 3200]\n",
            "loss: 0.906483  [ 3008/ 3200]\n",
            "loss: 1.134562  [ 3024/ 3200]\n",
            "loss: 1.110451  [ 3040/ 3200]\n",
            "loss: 1.028210  [ 3056/ 3200]\n",
            "loss: 1.055985  [ 3072/ 3200]\n",
            "loss: 1.142683  [ 3088/ 3200]\n",
            "loss: 1.159296  [ 3104/ 3200]\n",
            "loss: 1.070946  [ 3120/ 3200]\n",
            "loss: 1.206280  [ 3136/ 3200]\n",
            "loss: 0.990557  [ 3152/ 3200]\n",
            "loss: 1.218081  [ 3168/ 3200]\n",
            "loss: 1.146399  [ 3184/ 3200]\n",
            "current epoch: 26\n",
            "\n",
            "loss: 1.075341  [    0/ 3200]\n",
            "loss: 1.033509  [   16/ 3200]\n",
            "loss: 0.951508  [   32/ 3200]\n",
            "loss: 1.198116  [   48/ 3200]\n",
            "loss: 1.162443  [   64/ 3200]\n",
            "loss: 1.110992  [   80/ 3200]\n",
            "loss: 0.925826  [   96/ 3200]\n",
            "loss: 1.063715  [  112/ 3200]\n",
            "loss: 1.059818  [  128/ 3200]\n",
            "loss: 0.952922  [  144/ 3200]\n",
            "loss: 1.137038  [  160/ 3200]\n",
            "loss: 1.144281  [  176/ 3200]\n",
            "loss: 0.951555  [  192/ 3200]\n",
            "loss: 0.956489  [  208/ 3200]\n",
            "loss: 0.950957  [  224/ 3200]\n",
            "loss: 1.225244  [  240/ 3200]\n",
            "loss: 0.962844  [  256/ 3200]\n",
            "loss: 1.002707  [  272/ 3200]\n",
            "loss: 1.072994  [  288/ 3200]\n",
            "loss: 1.092604  [  304/ 3200]\n",
            "loss: 1.001134  [  320/ 3200]\n",
            "loss: 1.017885  [  336/ 3200]\n",
            "loss: 1.264302  [  352/ 3200]\n",
            "loss: 1.116348  [  368/ 3200]\n",
            "loss: 0.994443  [  384/ 3200]\n",
            "loss: 1.094541  [  400/ 3200]\n",
            "loss: 1.046189  [  416/ 3200]\n",
            "loss: 0.850826  [  432/ 3200]\n",
            "loss: 1.086214  [  448/ 3200]\n",
            "loss: 1.060971  [  464/ 3200]\n",
            "loss: 1.172522  [  480/ 3200]\n",
            "loss: 1.323227  [  496/ 3200]\n",
            "loss: 1.284082  [  512/ 3200]\n",
            "loss: 1.192125  [  528/ 3200]\n",
            "loss: 0.932868  [  544/ 3200]\n",
            "loss: 0.982607  [  560/ 3200]\n",
            "loss: 0.940618  [  576/ 3200]\n",
            "loss: 1.210643  [  592/ 3200]\n",
            "loss: 1.017002  [  608/ 3200]\n",
            "loss: 1.171704  [  624/ 3200]\n",
            "loss: 1.080361  [  640/ 3200]\n",
            "loss: 1.230776  [  656/ 3200]\n",
            "loss: 1.125912  [  672/ 3200]\n",
            "loss: 1.152720  [  688/ 3200]\n",
            "loss: 1.071976  [  704/ 3200]\n",
            "loss: 0.993387  [  720/ 3200]\n",
            "loss: 1.139483  [  736/ 3200]\n",
            "loss: 1.034600  [  752/ 3200]\n",
            "loss: 1.031825  [  768/ 3200]\n",
            "loss: 1.209988  [  784/ 3200]\n",
            "loss: 0.996851  [  800/ 3200]\n",
            "loss: 0.918204  [  816/ 3200]\n",
            "loss: 1.248767  [  832/ 3200]\n",
            "loss: 0.978385  [  848/ 3200]\n",
            "loss: 1.070477  [  864/ 3200]\n",
            "loss: 1.032947  [  880/ 3200]\n",
            "loss: 1.027692  [  896/ 3200]\n",
            "loss: 0.979531  [  912/ 3200]\n",
            "loss: 1.167540  [  928/ 3200]\n",
            "loss: 0.857402  [  944/ 3200]\n",
            "loss: 0.974354  [  960/ 3200]\n",
            "loss: 1.140773  [  976/ 3200]\n",
            "loss: 1.005661  [  992/ 3200]\n",
            "loss: 0.874521  [ 1008/ 3200]\n",
            "loss: 1.084991  [ 1024/ 3200]\n",
            "loss: 1.293018  [ 1040/ 3200]\n",
            "loss: 1.021587  [ 1056/ 3200]\n",
            "loss: 1.001170  [ 1072/ 3200]\n",
            "loss: 0.991827  [ 1088/ 3200]\n",
            "loss: 1.045641  [ 1104/ 3200]\n",
            "loss: 1.053825  [ 1120/ 3200]\n",
            "loss: 1.045107  [ 1136/ 3200]\n",
            "loss: 1.023520  [ 1152/ 3200]\n",
            "loss: 1.120616  [ 1168/ 3200]\n",
            "loss: 1.343434  [ 1184/ 3200]\n",
            "loss: 1.227512  [ 1200/ 3200]\n",
            "loss: 1.032381  [ 1216/ 3200]\n",
            "loss: 1.120555  [ 1232/ 3200]\n",
            "loss: 0.751754  [ 1248/ 3200]\n",
            "loss: 1.048298  [ 1264/ 3200]\n",
            "loss: 0.984069  [ 1280/ 3200]\n",
            "loss: 1.191908  [ 1296/ 3200]\n",
            "loss: 1.044793  [ 1312/ 3200]\n",
            "loss: 1.168323  [ 1328/ 3200]\n",
            "loss: 1.052796  [ 1344/ 3200]\n",
            "loss: 1.023927  [ 1360/ 3200]\n",
            "loss: 1.190105  [ 1376/ 3200]\n",
            "loss: 1.056017  [ 1392/ 3200]\n",
            "loss: 0.959633  [ 1408/ 3200]\n",
            "loss: 1.000404  [ 1424/ 3200]\n",
            "loss: 1.097339  [ 1440/ 3200]\n",
            "loss: 1.040639  [ 1456/ 3200]\n",
            "loss: 0.973865  [ 1472/ 3200]\n",
            "loss: 0.996444  [ 1488/ 3200]\n",
            "loss: 0.968744  [ 1504/ 3200]\n",
            "loss: 1.080673  [ 1520/ 3200]\n",
            "loss: 1.141435  [ 1536/ 3200]\n",
            "loss: 0.968917  [ 1552/ 3200]\n",
            "loss: 1.101042  [ 1568/ 3200]\n",
            "loss: 0.987624  [ 1584/ 3200]\n",
            "loss: 1.285597  [ 1600/ 3200]\n",
            "loss: 1.109517  [ 1616/ 3200]\n",
            "loss: 0.995501  [ 1632/ 3200]\n",
            "loss: 1.093662  [ 1648/ 3200]\n",
            "loss: 1.132297  [ 1664/ 3200]\n",
            "loss: 0.942592  [ 1680/ 3200]\n",
            "loss: 1.021456  [ 1696/ 3200]\n",
            "loss: 1.110353  [ 1712/ 3200]\n",
            "loss: 1.255235  [ 1728/ 3200]\n",
            "loss: 1.065074  [ 1744/ 3200]\n",
            "loss: 1.013974  [ 1760/ 3200]\n",
            "loss: 0.895454  [ 1776/ 3200]\n",
            "loss: 1.131341  [ 1792/ 3200]\n",
            "loss: 1.189941  [ 1808/ 3200]\n",
            "loss: 1.076566  [ 1824/ 3200]\n",
            "loss: 1.089562  [ 1840/ 3200]\n",
            "loss: 0.967334  [ 1856/ 3200]\n",
            "loss: 0.966441  [ 1872/ 3200]\n",
            "loss: 1.070386  [ 1888/ 3200]\n",
            "loss: 0.987295  [ 1904/ 3200]\n",
            "loss: 1.033048  [ 1920/ 3200]\n",
            "loss: 0.899723  [ 1936/ 3200]\n",
            "loss: 1.005628  [ 1952/ 3200]\n",
            "loss: 1.160492  [ 1968/ 3200]\n",
            "loss: 1.136439  [ 1984/ 3200]\n",
            "loss: 1.188423  [ 2000/ 3200]\n",
            "loss: 1.129835  [ 2016/ 3200]\n",
            "loss: 1.103386  [ 2032/ 3200]\n",
            "loss: 1.052331  [ 2048/ 3200]\n",
            "loss: 1.140288  [ 2064/ 3200]\n",
            "loss: 1.062490  [ 2080/ 3200]\n",
            "loss: 1.082837  [ 2096/ 3200]\n",
            "loss: 1.079983  [ 2112/ 3200]\n",
            "loss: 0.954823  [ 2128/ 3200]\n",
            "loss: 1.390765  [ 2144/ 3200]\n",
            "loss: 1.074232  [ 2160/ 3200]\n",
            "loss: 0.998076  [ 2176/ 3200]\n",
            "loss: 0.966709  [ 2192/ 3200]\n",
            "loss: 0.876145  [ 2208/ 3200]\n",
            "loss: 1.196775  [ 2224/ 3200]\n",
            "loss: 0.894114  [ 2240/ 3200]\n",
            "loss: 1.198306  [ 2256/ 3200]\n",
            "loss: 0.903485  [ 2272/ 3200]\n",
            "loss: 1.120369  [ 2288/ 3200]\n",
            "loss: 1.168770  [ 2304/ 3200]\n",
            "loss: 0.974907  [ 2320/ 3200]\n",
            "loss: 1.014725  [ 2336/ 3200]\n",
            "loss: 1.184143  [ 2352/ 3200]\n",
            "loss: 0.939380  [ 2368/ 3200]\n",
            "loss: 1.073481  [ 2384/ 3200]\n",
            "loss: 1.219691  [ 2400/ 3200]\n",
            "loss: 1.180738  [ 2416/ 3200]\n",
            "loss: 1.223189  [ 2432/ 3200]\n",
            "loss: 1.072083  [ 2448/ 3200]\n",
            "loss: 1.083727  [ 2464/ 3200]\n",
            "loss: 1.190214  [ 2480/ 3200]\n",
            "loss: 1.173058  [ 2496/ 3200]\n",
            "loss: 0.892747  [ 2512/ 3200]\n",
            "loss: 1.030906  [ 2528/ 3200]\n",
            "loss: 1.056598  [ 2544/ 3200]\n",
            "loss: 1.194949  [ 2560/ 3200]\n",
            "loss: 1.068423  [ 2576/ 3200]\n",
            "loss: 1.038486  [ 2592/ 3200]\n",
            "loss: 0.952789  [ 2608/ 3200]\n",
            "loss: 1.178292  [ 2624/ 3200]\n",
            "loss: 1.077882  [ 2640/ 3200]\n",
            "loss: 0.916283  [ 2656/ 3200]\n",
            "loss: 0.991695  [ 2672/ 3200]\n",
            "loss: 1.129893  [ 2688/ 3200]\n",
            "loss: 0.863090  [ 2704/ 3200]\n",
            "loss: 1.134091  [ 2720/ 3200]\n",
            "loss: 0.924402  [ 2736/ 3200]\n",
            "loss: 0.941107  [ 2752/ 3200]\n",
            "loss: 1.235372  [ 2768/ 3200]\n",
            "loss: 1.110296  [ 2784/ 3200]\n",
            "loss: 1.099173  [ 2800/ 3200]\n",
            "loss: 1.136374  [ 2816/ 3200]\n",
            "loss: 1.027627  [ 2832/ 3200]\n",
            "loss: 1.094941  [ 2848/ 3200]\n",
            "loss: 1.012189  [ 2864/ 3200]\n",
            "loss: 1.038270  [ 2880/ 3200]\n",
            "loss: 1.025006  [ 2896/ 3200]\n",
            "loss: 0.986532  [ 2912/ 3200]\n",
            "loss: 1.033756  [ 2928/ 3200]\n",
            "loss: 1.096052  [ 2944/ 3200]\n",
            "loss: 1.148126  [ 2960/ 3200]\n",
            "loss: 1.199074  [ 2976/ 3200]\n",
            "loss: 0.936865  [ 2992/ 3200]\n",
            "loss: 0.849697  [ 3008/ 3200]\n",
            "loss: 1.062550  [ 3024/ 3200]\n",
            "loss: 0.949273  [ 3040/ 3200]\n",
            "loss: 0.952558  [ 3056/ 3200]\n",
            "loss: 1.186544  [ 3072/ 3200]\n",
            "loss: 1.177011  [ 3088/ 3200]\n",
            "loss: 1.060164  [ 3104/ 3200]\n",
            "loss: 0.957366  [ 3120/ 3200]\n",
            "loss: 1.163468  [ 3136/ 3200]\n",
            "loss: 0.946096  [ 3152/ 3200]\n",
            "loss: 1.012466  [ 3168/ 3200]\n",
            "loss: 1.100373  [ 3184/ 3200]\n",
            "current epoch: 27\n",
            "\n",
            "loss: 1.025213  [    0/ 3200]\n",
            "loss: 0.902886  [   16/ 3200]\n",
            "loss: 0.974425  [   32/ 3200]\n",
            "loss: 1.179269  [   48/ 3200]\n",
            "loss: 1.051165  [   64/ 3200]\n",
            "loss: 0.888608  [   80/ 3200]\n",
            "loss: 0.933270  [   96/ 3200]\n",
            "loss: 1.055716  [  112/ 3200]\n",
            "loss: 0.962558  [  128/ 3200]\n",
            "loss: 1.241389  [  144/ 3200]\n",
            "loss: 0.989645  [  160/ 3200]\n",
            "loss: 1.006992  [  176/ 3200]\n",
            "loss: 1.243089  [  192/ 3200]\n",
            "loss: 1.092010  [  208/ 3200]\n",
            "loss: 1.165927  [  224/ 3200]\n",
            "loss: 1.116205  [  240/ 3200]\n",
            "loss: 0.992895  [  256/ 3200]\n",
            "loss: 1.077938  [  272/ 3200]\n",
            "loss: 1.045505  [  288/ 3200]\n",
            "loss: 0.877062  [  304/ 3200]\n",
            "loss: 0.978562  [  320/ 3200]\n",
            "loss: 0.999298  [  336/ 3200]\n",
            "loss: 1.104192  [  352/ 3200]\n",
            "loss: 0.997175  [  368/ 3200]\n",
            "loss: 1.148387  [  384/ 3200]\n",
            "loss: 1.091557  [  400/ 3200]\n",
            "loss: 1.005918  [  416/ 3200]\n",
            "loss: 0.902443  [  432/ 3200]\n",
            "loss: 0.959949  [  448/ 3200]\n",
            "loss: 1.137372  [  464/ 3200]\n",
            "loss: 1.061410  [  480/ 3200]\n",
            "loss: 1.123681  [  496/ 3200]\n",
            "loss: 0.961568  [  512/ 3200]\n",
            "loss: 1.021733  [  528/ 3200]\n",
            "loss: 1.316747  [  544/ 3200]\n",
            "loss: 1.080900  [  560/ 3200]\n",
            "loss: 1.062625  [  576/ 3200]\n",
            "loss: 1.213900  [  592/ 3200]\n",
            "loss: 1.081858  [  608/ 3200]\n",
            "loss: 1.093730  [  624/ 3200]\n",
            "loss: 1.009610  [  640/ 3200]\n",
            "loss: 1.110561  [  656/ 3200]\n",
            "loss: 1.052318  [  672/ 3200]\n",
            "loss: 1.025776  [  688/ 3200]\n",
            "loss: 1.001065  [  704/ 3200]\n",
            "loss: 1.015062  [  720/ 3200]\n",
            "loss: 1.013265  [  736/ 3200]\n",
            "loss: 1.295615  [  752/ 3200]\n",
            "loss: 1.013636  [  768/ 3200]\n",
            "loss: 1.083544  [  784/ 3200]\n",
            "loss: 1.208244  [  800/ 3200]\n",
            "loss: 1.245980  [  816/ 3200]\n",
            "loss: 1.126178  [  832/ 3200]\n",
            "loss: 1.041574  [  848/ 3200]\n",
            "loss: 1.157311  [  864/ 3200]\n",
            "loss: 1.057493  [  880/ 3200]\n",
            "loss: 0.942401  [  896/ 3200]\n",
            "loss: 1.153546  [  912/ 3200]\n",
            "loss: 1.083951  [  928/ 3200]\n",
            "loss: 1.074946  [  944/ 3200]\n",
            "loss: 1.254005  [  960/ 3200]\n",
            "loss: 0.976935  [  976/ 3200]\n",
            "loss: 0.889484  [  992/ 3200]\n",
            "loss: 1.196674  [ 1008/ 3200]\n",
            "loss: 1.124526  [ 1024/ 3200]\n",
            "loss: 1.038140  [ 1040/ 3200]\n",
            "loss: 1.076444  [ 1056/ 3200]\n",
            "loss: 0.980633  [ 1072/ 3200]\n",
            "loss: 1.189672  [ 1088/ 3200]\n",
            "loss: 0.999018  [ 1104/ 3200]\n",
            "loss: 1.034558  [ 1120/ 3200]\n",
            "loss: 0.934448  [ 1136/ 3200]\n",
            "loss: 1.238635  [ 1152/ 3200]\n",
            "loss: 0.933862  [ 1168/ 3200]\n",
            "loss: 0.823424  [ 1184/ 3200]\n",
            "loss: 1.107341  [ 1200/ 3200]\n",
            "loss: 1.087394  [ 1216/ 3200]\n",
            "loss: 1.095662  [ 1232/ 3200]\n",
            "loss: 1.169417  [ 1248/ 3200]\n",
            "loss: 0.885567  [ 1264/ 3200]\n",
            "loss: 1.087653  [ 1280/ 3200]\n",
            "loss: 1.078095  [ 1296/ 3200]\n",
            "loss: 1.082605  [ 1312/ 3200]\n",
            "loss: 0.906685  [ 1328/ 3200]\n",
            "loss: 1.020514  [ 1344/ 3200]\n",
            "loss: 1.027412  [ 1360/ 3200]\n",
            "loss: 0.924659  [ 1376/ 3200]\n",
            "loss: 1.231842  [ 1392/ 3200]\n",
            "loss: 1.299962  [ 1408/ 3200]\n",
            "loss: 0.933133  [ 1424/ 3200]\n",
            "loss: 1.001312  [ 1440/ 3200]\n",
            "loss: 0.965963  [ 1456/ 3200]\n",
            "loss: 1.082023  [ 1472/ 3200]\n",
            "loss: 0.790239  [ 1488/ 3200]\n",
            "loss: 1.248258  [ 1504/ 3200]\n",
            "loss: 0.967216  [ 1520/ 3200]\n",
            "loss: 1.107842  [ 1536/ 3200]\n",
            "loss: 1.185493  [ 1552/ 3200]\n",
            "loss: 1.142135  [ 1568/ 3200]\n",
            "loss: 0.959654  [ 1584/ 3200]\n",
            "loss: 0.986329  [ 1600/ 3200]\n",
            "loss: 1.103707  [ 1616/ 3200]\n",
            "loss: 0.899618  [ 1632/ 3200]\n",
            "loss: 1.042099  [ 1648/ 3200]\n",
            "loss: 0.933613  [ 1664/ 3200]\n",
            "loss: 1.173570  [ 1680/ 3200]\n",
            "loss: 1.111926  [ 1696/ 3200]\n",
            "loss: 1.041377  [ 1712/ 3200]\n",
            "loss: 1.072306  [ 1728/ 3200]\n",
            "loss: 0.901315  [ 1744/ 3200]\n",
            "loss: 1.010208  [ 1760/ 3200]\n",
            "loss: 1.135053  [ 1776/ 3200]\n",
            "loss: 0.937068  [ 1792/ 3200]\n",
            "loss: 1.125787  [ 1808/ 3200]\n",
            "loss: 1.033813  [ 1824/ 3200]\n",
            "loss: 1.177239  [ 1840/ 3200]\n",
            "loss: 0.899267  [ 1856/ 3200]\n",
            "loss: 0.785502  [ 1872/ 3200]\n",
            "loss: 1.283225  [ 1888/ 3200]\n",
            "loss: 0.991517  [ 1904/ 3200]\n",
            "loss: 1.178105  [ 1920/ 3200]\n",
            "loss: 0.989671  [ 1936/ 3200]\n",
            "loss: 1.184265  [ 1952/ 3200]\n",
            "loss: 1.237358  [ 1968/ 3200]\n",
            "loss: 1.009722  [ 1984/ 3200]\n",
            "loss: 1.008765  [ 2000/ 3200]\n",
            "loss: 1.038433  [ 2016/ 3200]\n",
            "loss: 1.168365  [ 2032/ 3200]\n",
            "loss: 1.139379  [ 2048/ 3200]\n",
            "loss: 1.121492  [ 2064/ 3200]\n",
            "loss: 1.113106  [ 2080/ 3200]\n",
            "loss: 0.982634  [ 2096/ 3200]\n",
            "loss: 1.161743  [ 2112/ 3200]\n",
            "loss: 1.089327  [ 2128/ 3200]\n",
            "loss: 0.969377  [ 2144/ 3200]\n",
            "loss: 0.790527  [ 2160/ 3200]\n",
            "loss: 1.080641  [ 2176/ 3200]\n",
            "loss: 1.032380  [ 2192/ 3200]\n",
            "loss: 1.002058  [ 2208/ 3200]\n",
            "loss: 1.081760  [ 2224/ 3200]\n",
            "loss: 0.949370  [ 2240/ 3200]\n",
            "loss: 0.866674  [ 2256/ 3200]\n",
            "loss: 1.278827  [ 2272/ 3200]\n",
            "loss: 1.146302  [ 2288/ 3200]\n",
            "loss: 1.235762  [ 2304/ 3200]\n",
            "loss: 1.033688  [ 2320/ 3200]\n",
            "loss: 0.918865  [ 2336/ 3200]\n",
            "loss: 0.967138  [ 2352/ 3200]\n",
            "loss: 0.925501  [ 2368/ 3200]\n",
            "loss: 1.088911  [ 2384/ 3200]\n",
            "loss: 1.203403  [ 2400/ 3200]\n",
            "loss: 1.084957  [ 2416/ 3200]\n",
            "loss: 1.035953  [ 2432/ 3200]\n",
            "loss: 1.054095  [ 2448/ 3200]\n",
            "loss: 1.030483  [ 2464/ 3200]\n",
            "loss: 0.992559  [ 2480/ 3200]\n",
            "loss: 0.862314  [ 2496/ 3200]\n",
            "loss: 0.921554  [ 2512/ 3200]\n",
            "loss: 0.712810  [ 2528/ 3200]\n",
            "loss: 0.796283  [ 2544/ 3200]\n",
            "loss: 1.081470  [ 2560/ 3200]\n",
            "loss: 0.945303  [ 2576/ 3200]\n",
            "loss: 1.075056  [ 2592/ 3200]\n",
            "loss: 1.037436  [ 2608/ 3200]\n",
            "loss: 0.992779  [ 2624/ 3200]\n",
            "loss: 0.865874  [ 2640/ 3200]\n",
            "loss: 1.118152  [ 2656/ 3200]\n",
            "loss: 1.186674  [ 2672/ 3200]\n",
            "loss: 1.116320  [ 2688/ 3200]\n",
            "loss: 1.185033  [ 2704/ 3200]\n",
            "loss: 0.903774  [ 2720/ 3200]\n",
            "loss: 0.997139  [ 2736/ 3200]\n",
            "loss: 1.132847  [ 2752/ 3200]\n",
            "loss: 1.207952  [ 2768/ 3200]\n",
            "loss: 1.148247  [ 2784/ 3200]\n",
            "loss: 1.020528  [ 2800/ 3200]\n",
            "loss: 1.025427  [ 2816/ 3200]\n",
            "loss: 0.940895  [ 2832/ 3200]\n",
            "loss: 1.170238  [ 2848/ 3200]\n",
            "loss: 0.897789  [ 2864/ 3200]\n",
            "loss: 1.036750  [ 2880/ 3200]\n",
            "loss: 1.033385  [ 2896/ 3200]\n",
            "loss: 0.992889  [ 2912/ 3200]\n",
            "loss: 1.027508  [ 2928/ 3200]\n",
            "loss: 1.132365  [ 2944/ 3200]\n",
            "loss: 1.121173  [ 2960/ 3200]\n",
            "loss: 1.223613  [ 2976/ 3200]\n",
            "loss: 1.124585  [ 2992/ 3200]\n",
            "loss: 1.307948  [ 3008/ 3200]\n",
            "loss: 1.113256  [ 3024/ 3200]\n",
            "loss: 0.832976  [ 3040/ 3200]\n",
            "loss: 1.044260  [ 3056/ 3200]\n",
            "loss: 1.151049  [ 3072/ 3200]\n",
            "loss: 1.020180  [ 3088/ 3200]\n",
            "loss: 0.892302  [ 3104/ 3200]\n",
            "loss: 0.880395  [ 3120/ 3200]\n",
            "loss: 1.208087  [ 3136/ 3200]\n",
            "loss: 1.176320  [ 3152/ 3200]\n",
            "loss: 0.894187  [ 3168/ 3200]\n",
            "loss: 1.210671  [ 3184/ 3200]\n",
            "current epoch: 28\n",
            "\n",
            "loss: 1.059812  [    0/ 3200]\n",
            "loss: 1.097717  [   16/ 3200]\n",
            "loss: 0.899134  [   32/ 3200]\n",
            "loss: 0.950484  [   48/ 3200]\n",
            "loss: 1.093080  [   64/ 3200]\n",
            "loss: 1.044930  [   80/ 3200]\n",
            "loss: 0.891928  [   96/ 3200]\n",
            "loss: 0.950893  [  112/ 3200]\n",
            "loss: 1.123313  [  128/ 3200]\n",
            "loss: 1.027211  [  144/ 3200]\n",
            "loss: 0.852641  [  160/ 3200]\n",
            "loss: 0.928296  [  176/ 3200]\n",
            "loss: 0.963176  [  192/ 3200]\n",
            "loss: 1.014290  [  208/ 3200]\n",
            "loss: 1.019505  [  224/ 3200]\n",
            "loss: 0.988552  [  240/ 3200]\n",
            "loss: 1.168237  [  256/ 3200]\n",
            "loss: 1.061471  [  272/ 3200]\n",
            "loss: 1.086025  [  288/ 3200]\n",
            "loss: 1.078150  [  304/ 3200]\n",
            "loss: 1.311180  [  320/ 3200]\n",
            "loss: 1.026827  [  336/ 3200]\n",
            "loss: 0.773935  [  352/ 3200]\n",
            "loss: 1.044013  [  368/ 3200]\n",
            "loss: 1.206282  [  384/ 3200]\n",
            "loss: 1.098756  [  400/ 3200]\n",
            "loss: 1.284034  [  416/ 3200]\n",
            "loss: 1.024990  [  432/ 3200]\n",
            "loss: 1.060757  [  448/ 3200]\n",
            "loss: 0.900118  [  464/ 3200]\n",
            "loss: 0.709768  [  480/ 3200]\n",
            "loss: 0.996555  [  496/ 3200]\n",
            "loss: 0.924173  [  512/ 3200]\n",
            "loss: 1.052992  [  528/ 3200]\n",
            "loss: 1.233755  [  544/ 3200]\n",
            "loss: 1.016999  [  560/ 3200]\n",
            "loss: 1.179901  [  576/ 3200]\n",
            "loss: 0.994507  [  592/ 3200]\n",
            "loss: 1.111968  [  608/ 3200]\n",
            "loss: 1.195129  [  624/ 3200]\n",
            "loss: 1.148939  [  640/ 3200]\n",
            "loss: 1.263837  [  656/ 3200]\n",
            "loss: 1.092257  [  672/ 3200]\n",
            "loss: 1.084882  [  688/ 3200]\n",
            "loss: 0.918719  [  704/ 3200]\n",
            "loss: 1.082464  [  720/ 3200]\n",
            "loss: 1.183851  [  736/ 3200]\n",
            "loss: 1.084748  [  752/ 3200]\n",
            "loss: 1.173533  [  768/ 3200]\n",
            "loss: 1.008747  [  784/ 3200]\n",
            "loss: 1.090179  [  800/ 3200]\n",
            "loss: 1.418097  [  816/ 3200]\n",
            "loss: 1.370907  [  832/ 3200]\n",
            "loss: 1.060822  [  848/ 3200]\n",
            "loss: 1.146251  [  864/ 3200]\n",
            "loss: 1.088701  [  880/ 3200]\n",
            "loss: 1.091901  [  896/ 3200]\n",
            "loss: 0.983577  [  912/ 3200]\n",
            "loss: 1.105016  [  928/ 3200]\n",
            "loss: 0.841758  [  944/ 3200]\n",
            "loss: 1.078206  [  960/ 3200]\n",
            "loss: 0.950806  [  976/ 3200]\n",
            "loss: 0.900907  [  992/ 3200]\n",
            "loss: 1.125364  [ 1008/ 3200]\n",
            "loss: 1.019406  [ 1024/ 3200]\n",
            "loss: 0.935012  [ 1040/ 3200]\n",
            "loss: 1.127203  [ 1056/ 3200]\n",
            "loss: 0.963016  [ 1072/ 3200]\n",
            "loss: 0.914389  [ 1088/ 3200]\n",
            "loss: 1.065939  [ 1104/ 3200]\n",
            "loss: 0.928676  [ 1120/ 3200]\n",
            "loss: 0.801484  [ 1136/ 3200]\n",
            "loss: 1.116520  [ 1152/ 3200]\n",
            "loss: 1.108896  [ 1168/ 3200]\n",
            "loss: 0.917752  [ 1184/ 3200]\n",
            "loss: 1.044249  [ 1200/ 3200]\n",
            "loss: 0.925592  [ 1216/ 3200]\n",
            "loss: 0.986992  [ 1232/ 3200]\n",
            "loss: 1.268077  [ 1248/ 3200]\n",
            "loss: 0.878775  [ 1264/ 3200]\n",
            "loss: 0.904776  [ 1280/ 3200]\n",
            "loss: 1.142658  [ 1296/ 3200]\n",
            "loss: 1.155078  [ 1312/ 3200]\n",
            "loss: 1.144113  [ 1328/ 3200]\n",
            "loss: 1.122883  [ 1344/ 3200]\n",
            "loss: 0.990391  [ 1360/ 3200]\n",
            "loss: 0.864785  [ 1376/ 3200]\n",
            "loss: 0.871885  [ 1392/ 3200]\n",
            "loss: 0.988931  [ 1408/ 3200]\n",
            "loss: 0.939619  [ 1424/ 3200]\n",
            "loss: 1.034787  [ 1440/ 3200]\n",
            "loss: 1.007239  [ 1456/ 3200]\n",
            "loss: 0.988215  [ 1472/ 3200]\n",
            "loss: 0.886187  [ 1488/ 3200]\n",
            "loss: 1.166486  [ 1504/ 3200]\n",
            "loss: 1.100092  [ 1520/ 3200]\n",
            "loss: 0.987053  [ 1536/ 3200]\n",
            "loss: 1.067894  [ 1552/ 3200]\n",
            "loss: 0.979184  [ 1568/ 3200]\n",
            "loss: 1.072115  [ 1584/ 3200]\n",
            "loss: 0.964508  [ 1600/ 3200]\n",
            "loss: 1.107996  [ 1616/ 3200]\n",
            "loss: 0.937469  [ 1632/ 3200]\n",
            "loss: 0.954704  [ 1648/ 3200]\n",
            "loss: 0.888170  [ 1664/ 3200]\n",
            "loss: 1.113312  [ 1680/ 3200]\n",
            "loss: 1.434567  [ 1696/ 3200]\n",
            "loss: 1.230507  [ 1712/ 3200]\n",
            "loss: 1.135826  [ 1728/ 3200]\n",
            "loss: 0.769208  [ 1744/ 3200]\n",
            "loss: 1.023060  [ 1760/ 3200]\n",
            "loss: 1.131232  [ 1776/ 3200]\n",
            "loss: 1.071721  [ 1792/ 3200]\n",
            "loss: 1.122423  [ 1808/ 3200]\n",
            "loss: 0.953272  [ 1824/ 3200]\n",
            "loss: 1.077200  [ 1840/ 3200]\n",
            "loss: 1.276731  [ 1856/ 3200]\n",
            "loss: 1.056723  [ 1872/ 3200]\n",
            "loss: 1.138171  [ 1888/ 3200]\n",
            "loss: 0.731928  [ 1904/ 3200]\n",
            "loss: 1.044448  [ 1920/ 3200]\n",
            "loss: 1.125568  [ 1936/ 3200]\n",
            "loss: 0.939795  [ 1952/ 3200]\n",
            "loss: 0.977112  [ 1968/ 3200]\n",
            "loss: 1.105887  [ 1984/ 3200]\n",
            "loss: 1.129020  [ 2000/ 3200]\n",
            "loss: 1.016164  [ 2016/ 3200]\n",
            "loss: 0.997888  [ 2032/ 3200]\n",
            "loss: 0.989548  [ 2048/ 3200]\n",
            "loss: 1.046969  [ 2064/ 3200]\n",
            "loss: 1.123512  [ 2080/ 3200]\n",
            "loss: 0.821822  [ 2096/ 3200]\n",
            "loss: 1.095768  [ 2112/ 3200]\n",
            "loss: 1.100190  [ 2128/ 3200]\n",
            "loss: 0.869304  [ 2144/ 3200]\n",
            "loss: 0.735939  [ 2160/ 3200]\n",
            "loss: 0.850746  [ 2176/ 3200]\n",
            "loss: 0.975317  [ 2192/ 3200]\n",
            "loss: 0.879570  [ 2208/ 3200]\n",
            "loss: 0.864305  [ 2224/ 3200]\n",
            "loss: 0.843591  [ 2240/ 3200]\n",
            "loss: 1.383245  [ 2256/ 3200]\n",
            "loss: 0.983644  [ 2272/ 3200]\n",
            "loss: 1.178894  [ 2288/ 3200]\n",
            "loss: 0.843261  [ 2304/ 3200]\n",
            "loss: 1.172668  [ 2320/ 3200]\n",
            "loss: 1.168997  [ 2336/ 3200]\n",
            "loss: 1.085041  [ 2352/ 3200]\n",
            "loss: 0.985603  [ 2368/ 3200]\n",
            "loss: 0.956765  [ 2384/ 3200]\n",
            "loss: 0.971616  [ 2400/ 3200]\n",
            "loss: 0.976410  [ 2416/ 3200]\n",
            "loss: 1.060322  [ 2432/ 3200]\n",
            "loss: 1.114536  [ 2448/ 3200]\n",
            "loss: 1.386763  [ 2464/ 3200]\n",
            "loss: 1.274042  [ 2480/ 3200]\n",
            "loss: 0.894876  [ 2496/ 3200]\n",
            "loss: 0.782116  [ 2512/ 3200]\n",
            "loss: 1.058676  [ 2528/ 3200]\n",
            "loss: 1.199774  [ 2544/ 3200]\n",
            "loss: 0.889216  [ 2560/ 3200]\n",
            "loss: 1.037114  [ 2576/ 3200]\n",
            "loss: 0.892892  [ 2592/ 3200]\n",
            "loss: 0.851087  [ 2608/ 3200]\n",
            "loss: 1.045032  [ 2624/ 3200]\n",
            "loss: 1.299335  [ 2640/ 3200]\n",
            "loss: 1.004373  [ 2656/ 3200]\n",
            "loss: 0.971698  [ 2672/ 3200]\n",
            "loss: 1.016972  [ 2688/ 3200]\n",
            "loss: 0.948721  [ 2704/ 3200]\n",
            "loss: 1.351629  [ 2720/ 3200]\n",
            "loss: 1.173361  [ 2736/ 3200]\n",
            "loss: 1.223759  [ 2752/ 3200]\n",
            "loss: 0.923682  [ 2768/ 3200]\n",
            "loss: 1.212332  [ 2784/ 3200]\n",
            "loss: 0.975109  [ 2800/ 3200]\n",
            "loss: 0.955376  [ 2816/ 3200]\n",
            "loss: 1.019292  [ 2832/ 3200]\n",
            "loss: 1.146474  [ 2848/ 3200]\n",
            "loss: 1.360225  [ 2864/ 3200]\n",
            "loss: 0.930083  [ 2880/ 3200]\n",
            "loss: 1.083164  [ 2896/ 3200]\n",
            "loss: 1.154114  [ 2912/ 3200]\n",
            "loss: 0.867668  [ 2928/ 3200]\n",
            "loss: 1.044619  [ 2944/ 3200]\n",
            "loss: 1.098276  [ 2960/ 3200]\n",
            "loss: 0.928584  [ 2976/ 3200]\n",
            "loss: 1.058373  [ 2992/ 3200]\n",
            "loss: 1.027518  [ 3008/ 3200]\n",
            "loss: 1.174448  [ 3024/ 3200]\n",
            "loss: 0.938551  [ 3040/ 3200]\n",
            "loss: 1.178147  [ 3056/ 3200]\n",
            "loss: 1.114806  [ 3072/ 3200]\n",
            "loss: 1.052550  [ 3088/ 3200]\n",
            "loss: 1.336219  [ 3104/ 3200]\n",
            "loss: 1.170198  [ 3120/ 3200]\n",
            "loss: 0.845794  [ 3136/ 3200]\n",
            "loss: 0.862062  [ 3152/ 3200]\n",
            "loss: 1.074823  [ 3168/ 3200]\n",
            "loss: 0.995873  [ 3184/ 3200]\n",
            "current epoch: 29\n",
            "\n",
            "loss: 1.101496  [    0/ 3200]\n",
            "loss: 0.895750  [   16/ 3200]\n",
            "loss: 1.123814  [   32/ 3200]\n",
            "loss: 0.976756  [   48/ 3200]\n",
            "loss: 0.961124  [   64/ 3200]\n",
            "loss: 1.081333  [   80/ 3200]\n",
            "loss: 1.216676  [   96/ 3200]\n",
            "loss: 1.191709  [  112/ 3200]\n",
            "loss: 1.040244  [  128/ 3200]\n",
            "loss: 0.968953  [  144/ 3200]\n",
            "loss: 1.239633  [  160/ 3200]\n",
            "loss: 1.027019  [  176/ 3200]\n",
            "loss: 0.819050  [  192/ 3200]\n",
            "loss: 0.908780  [  208/ 3200]\n",
            "loss: 1.276799  [  224/ 3200]\n",
            "loss: 1.033356  [  240/ 3200]\n",
            "loss: 0.939208  [  256/ 3200]\n",
            "loss: 1.028167  [  272/ 3200]\n",
            "loss: 1.223684  [  288/ 3200]\n",
            "loss: 1.052540  [  304/ 3200]\n",
            "loss: 0.934318  [  320/ 3200]\n",
            "loss: 1.208974  [  336/ 3200]\n",
            "loss: 0.905325  [  352/ 3200]\n",
            "loss: 1.203398  [  368/ 3200]\n",
            "loss: 0.917155  [  384/ 3200]\n",
            "loss: 1.153747  [  400/ 3200]\n",
            "loss: 0.949253  [  416/ 3200]\n",
            "loss: 1.158330  [  432/ 3200]\n",
            "loss: 0.887937  [  448/ 3200]\n",
            "loss: 1.014986  [  464/ 3200]\n",
            "loss: 1.102683  [  480/ 3200]\n",
            "loss: 1.063092  [  496/ 3200]\n",
            "loss: 1.156177  [  512/ 3200]\n",
            "loss: 1.002214  [  528/ 3200]\n",
            "loss: 1.003962  [  544/ 3200]\n",
            "loss: 0.971161  [  560/ 3200]\n",
            "loss: 0.864585  [  576/ 3200]\n",
            "loss: 0.967333  [  592/ 3200]\n",
            "loss: 0.959849  [  608/ 3200]\n",
            "loss: 1.053371  [  624/ 3200]\n",
            "loss: 1.023315  [  640/ 3200]\n",
            "loss: 0.861508  [  656/ 3200]\n",
            "loss: 0.828165  [  672/ 3200]\n",
            "loss: 1.324902  [  688/ 3200]\n",
            "loss: 0.941006  [  704/ 3200]\n",
            "loss: 1.136773  [  720/ 3200]\n",
            "loss: 0.972647  [  736/ 3200]\n",
            "loss: 1.129378  [  752/ 3200]\n",
            "loss: 0.837074  [  768/ 3200]\n",
            "loss: 1.097511  [  784/ 3200]\n",
            "loss: 1.013866  [  800/ 3200]\n",
            "loss: 1.038601  [  816/ 3200]\n",
            "loss: 1.151999  [  832/ 3200]\n",
            "loss: 1.043018  [  848/ 3200]\n",
            "loss: 1.276019  [  864/ 3200]\n",
            "loss: 0.952206  [  880/ 3200]\n",
            "loss: 1.158514  [  896/ 3200]\n",
            "loss: 0.940630  [  912/ 3200]\n",
            "loss: 0.904610  [  928/ 3200]\n",
            "loss: 1.133906  [  944/ 3200]\n",
            "loss: 1.006228  [  960/ 3200]\n",
            "loss: 1.072509  [  976/ 3200]\n",
            "loss: 1.071349  [  992/ 3200]\n",
            "loss: 1.058060  [ 1008/ 3200]\n",
            "loss: 1.212891  [ 1024/ 3200]\n",
            "loss: 0.897512  [ 1040/ 3200]\n",
            "loss: 1.108813  [ 1056/ 3200]\n",
            "loss: 0.820601  [ 1072/ 3200]\n",
            "loss: 0.954742  [ 1088/ 3200]\n",
            "loss: 1.004827  [ 1104/ 3200]\n",
            "loss: 0.851287  [ 1120/ 3200]\n",
            "loss: 0.941494  [ 1136/ 3200]\n",
            "loss: 0.832892  [ 1152/ 3200]\n",
            "loss: 1.222340  [ 1168/ 3200]\n",
            "loss: 0.852275  [ 1184/ 3200]\n",
            "loss: 0.929034  [ 1200/ 3200]\n",
            "loss: 1.150037  [ 1216/ 3200]\n",
            "loss: 1.188760  [ 1232/ 3200]\n",
            "loss: 1.140033  [ 1248/ 3200]\n",
            "loss: 1.176309  [ 1264/ 3200]\n",
            "loss: 0.893260  [ 1280/ 3200]\n",
            "loss: 1.140901  [ 1296/ 3200]\n",
            "loss: 1.313312  [ 1312/ 3200]\n",
            "loss: 1.178953  [ 1328/ 3200]\n",
            "loss: 0.871350  [ 1344/ 3200]\n",
            "loss: 1.049396  [ 1360/ 3200]\n",
            "loss: 1.136412  [ 1376/ 3200]\n",
            "loss: 0.970234  [ 1392/ 3200]\n",
            "loss: 1.328123  [ 1408/ 3200]\n",
            "loss: 0.980542  [ 1424/ 3200]\n",
            "loss: 0.994043  [ 1440/ 3200]\n",
            "loss: 1.140830  [ 1456/ 3200]\n",
            "loss: 0.875173  [ 1472/ 3200]\n",
            "loss: 1.076064  [ 1488/ 3200]\n",
            "loss: 1.171335  [ 1504/ 3200]\n",
            "loss: 1.009740  [ 1520/ 3200]\n",
            "loss: 0.950349  [ 1536/ 3200]\n",
            "loss: 0.932102  [ 1552/ 3200]\n",
            "loss: 0.845794  [ 1568/ 3200]\n",
            "loss: 1.078749  [ 1584/ 3200]\n",
            "loss: 1.145978  [ 1600/ 3200]\n",
            "loss: 1.120862  [ 1616/ 3200]\n",
            "loss: 1.234535  [ 1632/ 3200]\n",
            "loss: 0.971706  [ 1648/ 3200]\n",
            "loss: 0.891912  [ 1664/ 3200]\n",
            "loss: 0.941013  [ 1680/ 3200]\n",
            "loss: 1.034178  [ 1696/ 3200]\n",
            "loss: 0.897281  [ 1712/ 3200]\n",
            "loss: 0.929469  [ 1728/ 3200]\n",
            "loss: 0.862859  [ 1744/ 3200]\n",
            "loss: 1.078052  [ 1760/ 3200]\n",
            "loss: 1.047034  [ 1776/ 3200]\n",
            "loss: 1.008204  [ 1792/ 3200]\n",
            "loss: 1.003399  [ 1808/ 3200]\n",
            "loss: 1.041064  [ 1824/ 3200]\n",
            "loss: 1.284578  [ 1840/ 3200]\n",
            "loss: 1.108828  [ 1856/ 3200]\n",
            "loss: 0.751574  [ 1872/ 3200]\n",
            "loss: 0.967341  [ 1888/ 3200]\n",
            "loss: 1.092630  [ 1904/ 3200]\n",
            "loss: 0.967271  [ 1920/ 3200]\n",
            "loss: 0.757716  [ 1936/ 3200]\n",
            "loss: 0.932114  [ 1952/ 3200]\n",
            "loss: 1.016212  [ 1968/ 3200]\n",
            "loss: 0.936680  [ 1984/ 3200]\n",
            "loss: 0.986869  [ 2000/ 3200]\n",
            "loss: 1.019935  [ 2016/ 3200]\n",
            "loss: 1.131286  [ 2032/ 3200]\n",
            "loss: 0.892370  [ 2048/ 3200]\n",
            "loss: 1.035566  [ 2064/ 3200]\n",
            "loss: 1.171725  [ 2080/ 3200]\n",
            "loss: 0.939634  [ 2096/ 3200]\n",
            "loss: 1.024273  [ 2112/ 3200]\n",
            "loss: 0.920298  [ 2128/ 3200]\n",
            "loss: 0.840656  [ 2144/ 3200]\n",
            "loss: 0.884579  [ 2160/ 3200]\n",
            "loss: 0.980235  [ 2176/ 3200]\n",
            "loss: 0.742394  [ 2192/ 3200]\n",
            "loss: 1.081238  [ 2208/ 3200]\n",
            "loss: 0.877353  [ 2224/ 3200]\n",
            "loss: 1.273980  [ 2240/ 3200]\n",
            "loss: 1.082511  [ 2256/ 3200]\n",
            "loss: 1.074737  [ 2272/ 3200]\n",
            "loss: 1.091645  [ 2288/ 3200]\n",
            "loss: 0.970101  [ 2304/ 3200]\n",
            "loss: 1.231714  [ 2320/ 3200]\n",
            "loss: 0.795456  [ 2336/ 3200]\n",
            "loss: 1.012341  [ 2352/ 3200]\n",
            "loss: 1.319809  [ 2368/ 3200]\n",
            "loss: 1.051964  [ 2384/ 3200]\n",
            "loss: 1.063257  [ 2400/ 3200]\n",
            "loss: 0.769348  [ 2416/ 3200]\n",
            "loss: 1.038254  [ 2432/ 3200]\n",
            "loss: 1.099560  [ 2448/ 3200]\n",
            "loss: 0.993514  [ 2464/ 3200]\n",
            "loss: 1.258859  [ 2480/ 3200]\n",
            "loss: 1.133784  [ 2496/ 3200]\n",
            "loss: 0.995245  [ 2512/ 3200]\n",
            "loss: 1.108568  [ 2528/ 3200]\n",
            "loss: 1.302973  [ 2544/ 3200]\n",
            "loss: 0.927860  [ 2560/ 3200]\n",
            "loss: 0.826341  [ 2576/ 3200]\n",
            "loss: 0.977528  [ 2592/ 3200]\n",
            "loss: 1.232024  [ 2608/ 3200]\n",
            "loss: 1.059411  [ 2624/ 3200]\n",
            "loss: 0.835680  [ 2640/ 3200]\n",
            "loss: 0.895163  [ 2656/ 3200]\n",
            "loss: 1.102547  [ 2672/ 3200]\n",
            "loss: 1.268096  [ 2688/ 3200]\n",
            "loss: 0.940505  [ 2704/ 3200]\n",
            "loss: 1.150607  [ 2720/ 3200]\n",
            "loss: 1.016633  [ 2736/ 3200]\n",
            "loss: 1.155711  [ 2752/ 3200]\n",
            "loss: 0.859684  [ 2768/ 3200]\n",
            "loss: 0.996787  [ 2784/ 3200]\n",
            "loss: 1.289313  [ 2800/ 3200]\n",
            "loss: 0.769114  [ 2816/ 3200]\n",
            "loss: 1.139809  [ 2832/ 3200]\n",
            "loss: 1.157651  [ 2848/ 3200]\n",
            "loss: 0.869458  [ 2864/ 3200]\n",
            "loss: 0.944121  [ 2880/ 3200]\n",
            "loss: 1.245024  [ 2896/ 3200]\n",
            "loss: 0.929996  [ 2912/ 3200]\n",
            "loss: 0.999680  [ 2928/ 3200]\n",
            "loss: 1.119846  [ 2944/ 3200]\n",
            "loss: 1.034344  [ 2960/ 3200]\n",
            "loss: 1.124552  [ 2976/ 3200]\n",
            "loss: 0.839903  [ 2992/ 3200]\n",
            "loss: 1.019556  [ 3008/ 3200]\n",
            "loss: 1.046948  [ 3024/ 3200]\n",
            "loss: 1.032190  [ 3040/ 3200]\n",
            "loss: 1.014674  [ 3056/ 3200]\n",
            "loss: 1.028792  [ 3072/ 3200]\n",
            "loss: 1.197517  [ 3088/ 3200]\n",
            "loss: 1.048878  [ 3104/ 3200]\n",
            "loss: 1.003643  [ 3120/ 3200]\n",
            "loss: 0.884073  [ 3136/ 3200]\n",
            "loss: 1.096880  [ 3152/ 3200]\n",
            "loss: 0.881272  [ 3168/ 3200]\n",
            "loss: 0.943330  [ 3184/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 57.0%, Avg loss: 0.000754 \n",
            "\n",
            "f1 score is: 0.5634911060333252\n",
            " the confusion matrix is:\n",
            "tensor([[254,  29,   9,   5],\n",
            "        [ 41, 116, 130,  37],\n",
            "        [ 40,  64, 270,  25],\n",
            "        [ 29, 113,  69, 145]])\n",
            "\n",
            " time needed is: 16.769384384155273\n"
          ]
        }
      ],
      "source": [
        "# step 5 + 6 + 7\n",
        "\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "if step == 5 or step == 6:\n",
        "  start_time = time.time()\n",
        "  training(num_epochs, optimizer, train_dataloader, costfun, model)\n",
        "  testing(test_dataloader, costfun, model)\n",
        "  print(f\"\\n time needed is: {time.time() - start_time}\")  # https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution\n",
        "else:\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  testing(test_dataloader, costfun, bestmodel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTSnuJUgvbS-"
      },
      "source": [
        "With the CPU (step 5) the time needed for a run was 5.566997289657593 seconds. \\\\\n",
        "With the GPU (step 6) the time needed for a run was 5.525189638137817 seconds.\n",
        " \\\\\n",
        "In order to count the times, printing was commented out as to lessen the time and lean towards the actual time needed. We see that the GPU (for this workload) is slightly faster but not by a big margin.\n",
        "\n",
        "\n",
        "For step 6 we got on a random run: Accuracy: 61.4%, Avg loss: 0.000730 \n",
        "f1 score: 0.5894482135772705\n",
        " \\\\\n",
        "For step 7 we got on a random run: Accuracy: 60.5%, Avg loss: 0.000740 \n",
        "f1 score: 0.5706803798675537\n",
        "\n",
        " \\\\\n",
        "The best epoch is not the last one, for example in the results printed here, the best epoch was the 22nd. This can be explained due to the overfitting that occurs in the latest epochs. As we go through the epochs, the model becomes overfitted to the training data and has trouble generalizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWlTTEb_1jI"
      },
      "source": [
        "**QUESTION 2: CONVOLUTIONAL NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "zJ3KnmTLACFm",
        "outputId": "4a93383f-c6e1-44f9-f472-ae553e404c56"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# step 1\n",
        "\n",
        "\n",
        "melgramtraindata = np.load('/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/X.npy')\n",
        "melgramtrainlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/labels.npy\")\n",
        "\n",
        "melgramtestdata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/X.npy\")\n",
        "melgramtestlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/labels.npy\")\n",
        "\n",
        "melgramvaldata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/X.npy\")\n",
        "melgramvallabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/labels.npy\")\n",
        "\n",
        "\n",
        "class melgramdataset(Dataset): #use opposite labels\n",
        "  def __init__(self,data, labels,labels_map,transform=None, target_transform=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.melgramlabels = []\n",
        "    for l in labels:\n",
        "      self.melgramlabels.append(labels_map[l])\n",
        "    self.melgramlabels = np.array(self.melgramlabels)\n",
        "    self.melgramdata = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.melgramlabels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sound = self.melgramdata[index]\n",
        "    label = self.melgramlabels[index]\n",
        "    if self.transform:\n",
        "      sound = self.transform(sound)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "\n",
        "    return sound, label\n",
        "\n",
        "labels_map = {\n",
        "    0: \"classical\",\n",
        "    1: \"blues\",\n",
        "    2: \"rock_metal_hardrock\",\n",
        "    3: \"hiphop\",\n",
        "}\n",
        "\n",
        "opposite_labels_map = {\n",
        "    \"classical\":0,\n",
        "    \"blues\":1,\n",
        "    \"rock_metal_hardrock\":2,\n",
        "    \"hiphop\":3,\n",
        "}\n",
        "\n",
        "training_data = melgramdataset(melgramtraindata, melgramtrainlabels, opposite_labels_map, torch.tensor)\n",
        "val_data = melgramdataset(melgramvaldata, melgramvallabels, opposite_labels_map, torch.tensor)\n",
        "test_data = melgramdataset(melgramtestdata, melgramtestlabels, opposite_labels_map, torch.tensor)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# visualization\n",
        "\n",
        "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "img, label = training_data[sample_idx]\n",
        "plt.subplot(3,1,1)\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img.squeeze())\n",
        "\n",
        "\n",
        "sample_idx = torch.randint(len(test_data), size=(1,)).item()\n",
        "img, label = test_data[sample_idx]\n",
        "plt.subplot(3,1,2)\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img.squeeze())\n",
        "\n",
        "\n",
        "sample_idx = torch.randint(len(val_data), size=(1,)).item()\n",
        "img, label = val_data[sample_idx]\n",
        "plt.subplot(3,1,3)\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img.squeeze())\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=800 ,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=1376 ,shuffle=False )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EqsLnDwKLYi",
        "outputId": "6dc8eabc-af65-40bb-ddc0-c2e931afb16b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (convolution_layer): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# step 2\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(LeNet, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256,32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozssu7R5VLci",
        "outputId": "44eb160a-5147-4f1e-91e4-530cf8a707de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.9%, Avg loss: 0.001680 \n",
            "\n",
            "f1 score is: 0.2872035503387451\n",
            " the confusion matrix is:\n",
            "tensor([[142,   0,   0,  58],\n",
            "        [ 52,   0,   0, 148],\n",
            "        [ 15,   1,   0, 184],\n",
            "        [ 14,   1,   0, 185]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 41.1%, Avg loss: 0.001542 \n",
            "\n",
            "f1 score is: 0.28692811727523804\n",
            " the confusion matrix is:\n",
            "tensor([[145,   0,   0,  55],\n",
            "        [ 55,   0,   0, 145],\n",
            "        [ 19,   0,   0, 181],\n",
            "        [ 16,   0,   0, 184]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.9%, Avg loss: 0.001526 \n",
            "\n",
            "f1 score is: 0.4033478796482086\n",
            " the confusion matrix is:\n",
            "tensor([[ 79,  86,   0,  35],\n",
            "        [ 11,  89,  11,  89],\n",
            "        [  3,  35,  22, 140],\n",
            "        [  1,  37,   9, 153]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.5%, Avg loss: 0.001431 \n",
            "\n",
            "f1 score is: 0.3033977150917053\n",
            " the confusion matrix is:\n",
            "tensor([[150,   4,   0,  46],\n",
            "        [ 55,   3,   0, 142],\n",
            "        [ 19,   1,   0, 180],\n",
            "        [ 11,   2,   0, 187]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.2%, Avg loss: 0.001517 \n",
            "\n",
            "f1 score is: 0.3650168180465698\n",
            " the confusion matrix is:\n",
            "tensor([[196,   3,   1,   0],\n",
            "        [149,  15,  27,   9],\n",
            "        [ 70,  21,  74,  35],\n",
            "        [ 80,  19,  48,  53]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 0.001339 \n",
            "\n",
            "f1 score is: 0.5119436979293823\n",
            " the confusion matrix is:\n",
            "tensor([[140,  46,   2,  12],\n",
            "        [ 46,  79,  30,  45],\n",
            "        [  8,  40,  79,  73],\n",
            "        [  7,  37,  43, 113]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 43.5%, Avg loss: 0.001346 \n",
            "\n",
            "f1 score is: 0.3450252413749695\n",
            " the confusion matrix is:\n",
            "tensor([[147,  19,   1,  33],\n",
            "        [ 55,   8,  11, 126],\n",
            "        [ 11,   6,  12, 171],\n",
            "        [  7,   4,   8, 181]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.001348 \n",
            "\n",
            "f1 score is: 0.4916137456893921\n",
            " the confusion matrix is:\n",
            "tensor([[126,  58,   8,   8],\n",
            "        [ 27,  81,  62,  30],\n",
            "        [  1,  43, 143,  13],\n",
            "        [  2,  41, 108,  49]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.001325 \n",
            "\n",
            "f1 score is: 0.48988014459609985\n",
            " the confusion matrix is:\n",
            "tensor([[133,  40,   7,  20],\n",
            "        [ 33,  29,  68,  70],\n",
            "        [  2,  13, 110,  75],\n",
            "        [  3,  11,  51, 135]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.0%, Avg loss: 0.001359 \n",
            "\n",
            "f1 score is: 0.42568933963775635\n",
            " the confusion matrix is:\n",
            "tensor([[128,  23,   2,  47],\n",
            "        [ 26,  15,  39, 120],\n",
            "        [  2,   4,  52, 142],\n",
            "        [  0,   3,  16, 181]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.001285 \n",
            "\n",
            "f1 score is: 0.49172455072402954\n",
            " the confusion matrix is:\n",
            "tensor([[129,  68,   0,   3],\n",
            "        [ 30, 129,   6,  35],\n",
            "        [  2, 106,  33,  59],\n",
            "        [  1,  81,   7, 111]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.1%, Avg loss: 0.001223 \n",
            "\n",
            "f1 score is: 0.555802047252655\n",
            " the confusion matrix is:\n",
            "tensor([[145,  42,   2,  11],\n",
            "        [ 41,  66,  38,  55],\n",
            "        [  3,  41,  97,  59],\n",
            "        [  3,  21,  35, 141]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.4%, Avg loss: 0.001200 \n",
            "\n",
            "f1 score is: 0.5492663979530334\n",
            " the confusion matrix is:\n",
            "tensor([[157,  33,   3,   7],\n",
            "        [ 44,  46,  67,  43],\n",
            "        [  4,  30, 127,  39],\n",
            "        [  5,  15,  59, 121]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.5%, Avg loss: 0.001301 \n",
            "\n",
            "f1 score is: 0.5341590642929077\n",
            " the confusion matrix is:\n",
            "tensor([[173,  26,   1,   0],\n",
            "        [ 62, 106,  20,  12],\n",
            "        [ 12,  83,  98,   7],\n",
            "        [ 11,  75,  55,  59]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.2%, Avg loss: 0.001140 \n",
            "\n",
            "f1 score is: 0.5946645736694336\n",
            " the confusion matrix is:\n",
            "tensor([[151,  36,   2,  11],\n",
            "        [ 34,  67,  40,  59],\n",
            "        [  2,  31, 109,  58],\n",
            "        [  1,  17,  27, 155]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 0.001414 \n",
            "\n",
            "f1 score is: 0.4708747863769531\n",
            " the confusion matrix is:\n",
            "tensor([[ 94,  45,   6,  55],\n",
            "        [ 12,  17,  48, 123],\n",
            "        [  0,   4, 108,  88],\n",
            "        [  0,   0,  15, 185]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 0.001137 \n",
            "\n",
            "f1 score is: 0.5662384033203125\n",
            " the confusion matrix is:\n",
            "tensor([[160,  22,  13,   5],\n",
            "        [ 39,  37,  85,  39],\n",
            "        [  3,  10, 152,  35],\n",
            "        [  1,  10,  65, 124]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.001099 \n",
            "\n",
            "f1 score is: 0.610343873500824\n",
            " the confusion matrix is:\n",
            "tensor([[152,  42,   0,   6],\n",
            "        [ 34,  75,  35,  56],\n",
            "        [  2,  35, 107,  56],\n",
            "        [  0,  17,  24, 159]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.4%, Avg loss: 0.001086 \n",
            "\n",
            "f1 score is: 0.5712725520133972\n",
            " the confusion matrix is:\n",
            "tensor([[173,  15,   9,   3],\n",
            "        [ 49,  33,  77,  41],\n",
            "        [  6,  15, 144,  35],\n",
            "        [  5,  13,  49, 133]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.001127 \n",
            "\n",
            "f1 score is: 0.626472532749176\n",
            " the confusion matrix is:\n",
            "tensor([[173,  25,   0,   2],\n",
            "        [ 47, 118,  17,  18],\n",
            "        [  8,  67,  99,  26],\n",
            "        [  4,  51,  33, 112]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 0.001063 \n",
            "\n",
            "f1 score is: 0.6388795375823975\n",
            " the confusion matrix is:\n",
            "tensor([[154,  43,   1,   2],\n",
            "        [ 30, 111,  29,  30],\n",
            "        [  2,  55, 110,  33],\n",
            "        [  0,  30,  37, 133]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.001075 \n",
            "\n",
            "f1 score is: 0.6036275625228882\n",
            " the confusion matrix is:\n",
            "tensor([[157,  33,   5,   5],\n",
            "        [ 31,  46,  56,  67],\n",
            "        [  2,  16, 130,  52],\n",
            "        [  0,   9,  24, 167]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.001046 \n",
            "\n",
            "f1 score is: 0.655522346496582\n",
            " the confusion matrix is:\n",
            "tensor([[162,  34,   0,   4],\n",
            "        [ 31, 118,  17,  34],\n",
            "        [  2,  64, 100,  34],\n",
            "        [  0,  34,  23, 143]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 0.001083 \n",
            "\n",
            "f1 score is: 0.6232152581214905\n",
            " the confusion matrix is:\n",
            "tensor([[160,  35,   0,   5],\n",
            "        [ 42,  97,   5,  56],\n",
            "        [  3,  65,  82,  50],\n",
            "        [  2,  22,  11, 165]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 0.001065 \n",
            "\n",
            "f1 score is: 0.6358256936073303\n",
            " the confusion matrix is:\n",
            "tensor([[159,  32,   5,   4],\n",
            "        [ 32,  85,  61,  22],\n",
            "        [  2,  32, 143,  23],\n",
            "        [  2,  18,  57, 123]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.1%, Avg loss: 0.001018 \n",
            "\n",
            "f1 score is: 0.652000904083252\n",
            " the confusion matrix is:\n",
            "tensor([[165,  32,   0,   3],\n",
            "        [ 37, 113,  23,  27],\n",
            "        [  7,  56, 110,  27],\n",
            "        [  2,  32,  33, 133]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 0.001006 \n",
            "\n",
            "f1 score is: 0.6652631759643555\n",
            " the confusion matrix is:\n",
            "tensor([[168,  29,   0,   3],\n",
            "        [ 37, 118,  24,  21],\n",
            "        [  7,  52, 114,  27],\n",
            "        [  3,  32,  33, 132]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.001032 \n",
            "\n",
            "f1 score is: 0.65650874376297\n",
            " the confusion matrix is:\n",
            "tensor([[158,  36,   1,   5],\n",
            "        [ 25,  97,  27,  51],\n",
            "        [  2,  46, 109,  43],\n",
            "        [  0,  15,  22, 163]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.001083 \n",
            "\n",
            "f1 score is: 0.6351503729820251\n",
            " the confusion matrix is:\n",
            "tensor([[139,  48,   8,   5],\n",
            "        [ 17,  73,  74,  36],\n",
            "        [  2,  14, 154,  30],\n",
            "        [  1,   8,  45, 146]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.000991 \n",
            "\n",
            "f1 score is: 0.6599544286727905\n",
            " the confusion matrix is:\n",
            "tensor([[159,  37,   0,   4],\n",
            "        [ 26, 110,  27,  37],\n",
            "        [  4,  56, 108,  32],\n",
            "        [  0,  23,  27, 150]])\n",
            "best epoch is: 26\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.000587 \n",
            "\n",
            "f1 score is: 0.6789075136184692\n",
            " the confusion matrix is:\n",
            "tensor([[239,  48,   1,   9],\n",
            "        [ 15, 207,  55,  47],\n",
            "        [ 10, 118, 196,  75],\n",
            "        [  2,  40,  33, 281]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0005872668048670126, tensor(0.6789), 67.07848837209302, ConfusionMatrix())"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# step 3 + 4\n",
        "\n",
        "\n",
        "def testing(dataloader, costfunct, model):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      test_loss += costfunct(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  acc = 100*correct\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  f1 = f1_score(pred, y, num_classes=4, average=\"macro\")\n",
        "  confmatrix = ConfusionMatrix(num_classes=4).to(device)\n",
        "  print(f\"f1 score is: {f1.item()}\\n the confusion matrix is:\")\n",
        "  print(confmatrix(pred.argmax(1), y))\n",
        "  model.train()\n",
        "  return test_loss, f1, acc, confmatrix\n",
        "\n",
        "\n",
        "def training_with_validation(epochs, optimizer, dataloader, validationdata,costfunct, model):\n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    print(f\"current epoch: {ep}\\n\")\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print()\n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "\n",
        "  print(f\"best epoch is: {bestepoch}\")\n",
        "  return bestmodel\n",
        "\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "testing(test_dataloader, costfun, bestmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fCgZb94f1O_"
      },
      "source": [
        "Without pooling and padding we have way too many weights to calculate making the process increadebly slow and decreasing accuracy.\n",
        " \n",
        "With those 2 additions the weights to be calculated are decreased significantly. We managed to achieve upwards of 71.2% accuracy on the training set, and using the best epoch found with the validation set, we managed to reach Accuracy: 60.5%, Avg loss: 0.000675 f1 score: 0.6041808724403381"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCUKsMYlsc-A",
        "outputId": "021cdbd4-3e68-4e25-c18c-8fbae1cff8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 43.8%, Avg loss: 0.001727 \n",
            "\n",
            "f1 score is: 0.3563973307609558\n",
            " the confusion matrix is:\n",
            "tensor([[185,   2,   0,  13],\n",
            "        [138,  34,   0,  28],\n",
            "        [111,  75,   0,  14],\n",
            "        [ 51,  18,   0, 131]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.2%, Avg loss: 0.001721 \n",
            "\n",
            "f1 score is: 0.2822229266166687\n",
            " the confusion matrix is:\n",
            "tensor([[161,   0,   0,  39],\n",
            "        [118,   0,   0,  82],\n",
            "        [114,   0,   0,  86],\n",
            "        [ 23,   0,   0, 177]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.8%, Avg loss: 0.001716 \n",
            "\n",
            "f1 score is: 0.28803834319114685\n",
            " the confusion matrix is:\n",
            "tensor([[131,   0,   0,  69],\n",
            "        [ 32,   0,   0, 168],\n",
            "        [ 23,   0,   0, 177],\n",
            "        [  5,   0,   0, 195]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.1%, Avg loss: 0.001710 \n",
            "\n",
            "f1 score is: 0.28794679045677185\n",
            " the confusion matrix is:\n",
            "tensor([[125,   0,   0,  75],\n",
            "        [ 25,   0,   0, 175],\n",
            "        [ 13,   0,   0, 187],\n",
            "        [  4,   0,   0, 196]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 0.001702 \n",
            "\n",
            "f1 score is: 0.24431467056274414\n",
            " the confusion matrix is:\n",
            "tensor([[ 80,   0,   0, 120],\n",
            "        [ 11,   0,   0, 189],\n",
            "        [  4,   0,   0, 196],\n",
            "        [  2,   0,   0, 198]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.2%, Avg loss: 0.001691 \n",
            "\n",
            "f1 score is: 0.2733900845050812\n",
            " the confusion matrix is:\n",
            "tensor([[110,   0,   0,  90],\n",
            "        [ 23,   0,   0, 177],\n",
            "        [ 10,   0,   0, 190],\n",
            "        [  4,   0,   0, 196]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.8%, Avg loss: 0.001675 \n",
            "\n",
            "f1 score is: 0.2846217453479767\n",
            " the confusion matrix is:\n",
            "tensor([[123,   0,   0,  77],\n",
            "        [ 26,   0,   0, 174],\n",
            "        [ 13,   0,   0, 187],\n",
            "        [  5,   0,   0, 195]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.6%, Avg loss: 0.001656 \n",
            "\n",
            "f1 score is: 0.26346975564956665\n",
            " the confusion matrix is:\n",
            "tensor([[175,   0,   0,  25],\n",
            "        [125,   0,   0,  75],\n",
            "        [ 68,   0,   0, 132],\n",
            "        [ 58,   0,   0, 142]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.0%, Avg loss: 0.001617 \n",
            "\n",
            "f1 score is: 0.28689315915107727\n",
            " the confusion matrix is:\n",
            "tensor([[161,   0,   0,  39],\n",
            "        [ 79,   0,   0, 121],\n",
            "        [ 29,   0,   0, 171],\n",
            "        [ 25,   0,   0, 175]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 43.4%, Avg loss: 0.001581 \n",
            "\n",
            "f1 score is: 0.3413273096084595\n",
            " the confusion matrix is:\n",
            "tensor([[138,   4,   0,  58],\n",
            "        [ 40,  23,   0, 137],\n",
            "        [ 17,   7,   0, 176],\n",
            "        [ 11,   3,   0, 186]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 0.001541 \n",
            "\n",
            "f1 score is: 0.37401485443115234\n",
            " the confusion matrix is:\n",
            "tensor([[152,   8,   0,  40],\n",
            "        [ 52,  43,   0, 105],\n",
            "        [ 20,  21,   0, 159],\n",
            "        [ 16,  14,   0, 170]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 44.8%, Avg loss: 0.001511 \n",
            "\n",
            "f1 score is: 0.3640831410884857\n",
            " the confusion matrix is:\n",
            "tensor([[145,   9,   0,  46],\n",
            "        [ 44,  36,   0, 120],\n",
            "        [ 18,  11,   0, 171],\n",
            "        [ 14,   9,   0, 177]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.8%, Avg loss: 0.001493 \n",
            "\n",
            "f1 score is: 0.37153851985931396\n",
            " the confusion matrix is:\n",
            "tensor([[144,  45,   0,  11],\n",
            "        [ 49, 129,   0,  22],\n",
            "        [ 17, 134,   0,  49],\n",
            "        [ 13, 118,   0,  69]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 41.4%, Avg loss: 0.001477 \n",
            "\n",
            "f1 score is: 0.3592827320098877\n",
            " the confusion matrix is:\n",
            "tensor([[131,  61,   0,   8],\n",
            "        [ 41, 141,   0,  18],\n",
            "        [ 15, 139,   0,  46],\n",
            "        [  9, 132,   0,  59]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 43.0%, Avg loss: 0.001470 \n",
            "\n",
            "f1 score is: 0.3772421181201935\n",
            " the confusion matrix is:\n",
            "tensor([[108,  64,   0,  28],\n",
            "        [ 24,  91,   1,  84],\n",
            "        [ 12,  52,   0, 136],\n",
            "        [  2,  53,   0, 145]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 45.2%, Avg loss: 0.001416 \n",
            "\n",
            "f1 score is: 0.3849402666091919\n",
            " the confusion matrix is:\n",
            "tensor([[146,  29,   0,  25],\n",
            "        [ 52,  67,   0,  81],\n",
            "        [ 17,  44,   0, 139],\n",
            "        [ 11,  40,   0, 149]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 44.0%, Avg loss: 0.001427 \n",
            "\n",
            "f1 score is: 0.3895086348056793\n",
            " the confusion matrix is:\n",
            "tensor([[119,  55,   1,  25],\n",
            "        [ 29,  90,   0,  81],\n",
            "        [ 12,  53,   2, 133],\n",
            "        [  3,  56,   0, 141]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.4%, Avg loss: 0.001389 \n",
            "\n",
            "f1 score is: 0.3939361870288849\n",
            " the confusion matrix is:\n",
            "tensor([[162,  17,   1,  20],\n",
            "        [ 61,  57,   0,  82],\n",
            "        [ 21,  32,   0, 147],\n",
            "        [ 15,  25,   0, 160]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.1%, Avg loss: 0.001385 \n",
            "\n",
            "f1 score is: 0.43724262714385986\n",
            " the confusion matrix is:\n",
            "tensor([[170,  10,   2,  18],\n",
            "        [ 68,  46,  20,  66],\n",
            "        [ 24,  18,  25, 133],\n",
            "        [ 20,  19,   9, 152]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 0.001366 \n",
            "\n",
            "f1 score is: 0.42356646060943604\n",
            " the confusion matrix is:\n",
            "tensor([[148,  43,   1,   8],\n",
            "        [ 46,  87,  14,  53],\n",
            "        [ 15,  61,  13, 111],\n",
            "        [  7,  66,   6, 121]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 0.001380 \n",
            "\n",
            "f1 score is: 0.40237507224082947\n",
            " the confusion matrix is:\n",
            "tensor([[130,  48,   1,  21],\n",
            "        [ 34,  86,   0,  80],\n",
            "        [ 12,  45,   0, 143],\n",
            "        [  2,  44,   0, 154]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.6%, Avg loss: 0.001352 \n",
            "\n",
            "f1 score is: 0.4886593818664551\n",
            " the confusion matrix is:\n",
            "tensor([[156,  18,  15,  11],\n",
            "        [ 54,  44,  39,  63],\n",
            "        [ 19,  10,  66, 105],\n",
            "        [ 12,  17,  24, 147]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 0.001338 \n",
            "\n",
            "f1 score is: 0.49434205889701843\n",
            " the confusion matrix is:\n",
            "tensor([[151,  42,   2,   5],\n",
            "        [ 53,  81,  21,  45],\n",
            "        [ 18,  48,  56,  78],\n",
            "        [ 11,  55,  18, 116]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 0.001346 \n",
            "\n",
            "f1 score is: 0.5186599493026733\n",
            " the confusion matrix is:\n",
            "tensor([[147,  49,   3,   1],\n",
            "        [ 47, 102,  29,  22],\n",
            "        [ 15,  70,  87,  28],\n",
            "        [  9,  72,  41,  78]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.6%, Avg loss: 0.001396 \n",
            "\n",
            "f1 score is: 0.4639095664024353\n",
            " the confusion matrix is:\n",
            "tensor([[109,  59,   1,  31],\n",
            "        [ 22,  62,  27,  89],\n",
            "        [  4,  21,  49, 126],\n",
            "        [  0,  24,  15, 161]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.6%, Avg loss: 0.001331 \n",
            "\n",
            "f1 score is: 0.5210433006286621\n",
            " the confusion matrix is:\n",
            "tensor([[135,  57,   7,   1],\n",
            "        [ 41,  97,  38,  24],\n",
            "        [  7,  60, 105,  28],\n",
            "        [  3,  66,  55,  76]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.5%, Avg loss: 0.001314 \n",
            "\n",
            "f1 score is: 0.4231455326080322\n",
            " the confusion matrix is:\n",
            "tensor([[159,  18,   1,  22],\n",
            "        [ 58,  47,   7,  88],\n",
            "        [ 18,  13,  15, 154],\n",
            "        [ 10,  19,   4, 167]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.2%, Avg loss: 0.001295 \n",
            "\n",
            "f1 score is: 0.5181173086166382\n",
            " the confusion matrix is:\n",
            "tensor([[151,  35,   3,  11],\n",
            "        [ 48,  68,  28,  56],\n",
            "        [ 14,  31,  65,  90],\n",
            "        [  7,  32,  19, 142]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.1%, Avg loss: 0.001305 \n",
            "\n",
            "f1 score is: 0.5243026614189148\n",
            " the confusion matrix is:\n",
            "tensor([[153,  23,  21,   3],\n",
            "        [ 49,  50,  62,  39],\n",
            "        [ 14,  15, 139,  32],\n",
            "        [  9,  22,  78,  91]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.8%, Avg loss: 0.001291 \n",
            "\n",
            "f1 score is: 0.46714669466018677\n",
            " the confusion matrix is:\n",
            "tensor([[139,  48,   1,  12],\n",
            "        [ 40,  76,  15,  69],\n",
            "        [  6,  40,  27, 127],\n",
            "        [  2,  34,   8, 156]])\n",
            "best epoch is: 28\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 0.000768 \n",
            "\n",
            "f1 score is: 0.4658815860748291\n",
            " the confusion matrix is:\n",
            "tensor([[201,  64,   0,  32],\n",
            "        [ 28, 107,  59, 130],\n",
            "        [ 20,  91,  65, 223],\n",
            "        [ 13,  49,   5, 289]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.8%, Avg loss: 0.001357 \n",
            "\n",
            "f1 score is: 0.4274972677230835\n",
            " the confusion matrix is:\n",
            "tensor([[179,  21,   0,   0],\n",
            "        [ 89,  84,  24,   3],\n",
            "        [ 14,  74, 105,   7],\n",
            "        [ 21,  80,  85,  14]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 46.4%, Avg loss: 0.001293 \n",
            "\n",
            "f1 score is: 0.4072526693344116\n",
            " the confusion matrix is:\n",
            "tensor([[100,  91,   0,   9],\n",
            "        [  8,  74,   1, 117],\n",
            "        [  0,  22,   5, 173],\n",
            "        [  0,   7,   1, 192]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.000931 \n",
            "\n",
            "f1 score is: 0.700563907623291\n",
            " the confusion matrix is:\n",
            "tensor([[175,  16,   8,   1],\n",
            "        [ 29,  85,  47,  39],\n",
            "        [  7,  18, 140,  35],\n",
            "        [  1,  11,  19, 169]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.000903 \n",
            "\n",
            "f1 score is: 0.7235007286071777\n",
            " the confusion matrix is:\n",
            "tensor([[154,  39,   7,   0],\n",
            "        [ 14, 140,  34,  12],\n",
            "        [  2,  31, 141,  26],\n",
            "        [  0,  30,  30, 140]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.000999 \n",
            "\n",
            "f1 score is: 0.6954971551895142\n",
            " the confusion matrix is:\n",
            "tensor([[195,   2,   3,   0],\n",
            "        [ 49,  90,  53,   8],\n",
            "        [ 11,   7, 171,  11],\n",
            "        [  2,  22,  65, 111]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.000931 \n",
            "\n",
            "f1 score is: 0.6944181323051453\n",
            " the confusion matrix is:\n",
            "tensor([[157,  21,  22,   0],\n",
            "        [ 13,  81,  66,  40],\n",
            "        [  2,   9, 162,  27],\n",
            "        [  0,   9,  30, 161]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.000916 \n",
            "\n",
            "f1 score is: 0.7210734486579895\n",
            " the confusion matrix is:\n",
            "tensor([[168,  20,  12,   0],\n",
            "        [ 11, 116,  61,  12],\n",
            "        [  3,  14, 172,  11],\n",
            "        [  0,  15,  66, 119]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.000809 \n",
            "\n",
            "f1 score is: 0.7644339203834534\n",
            " the confusion matrix is:\n",
            "tensor([[186,   7,   7,   0],\n",
            "        [ 18, 130,  37,  15],\n",
            "        [  5,  25, 155,  15],\n",
            "        [  0,  22,  37, 141]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.000835 \n",
            "\n",
            "f1 score is: 0.7481844425201416\n",
            " the confusion matrix is:\n",
            "tensor([[151,  44,   5,   0],\n",
            "        [  7, 148,  20,  25],\n",
            "        [  2,  40, 137,  21],\n",
            "        [  0,  27,  15, 158]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.000854 \n",
            "\n",
            "f1 score is: 0.7395389676094055\n",
            " the confusion matrix is:\n",
            "tensor([[158,  39,   3,   0],\n",
            "        [ 10, 127,  16,  47],\n",
            "        [  3,  35, 128,  34],\n",
            "        [  0,  18,   5, 177]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.000851 \n",
            "\n",
            "f1 score is: 0.7235722541809082\n",
            " the confusion matrix is:\n",
            "tensor([[179,   7,  14,   0],\n",
            "        [ 17,  93,  59,  31],\n",
            "        [  5,  15, 165,  15],\n",
            "        [  0,  11,  43, 146]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.000845 \n",
            "\n",
            "f1 score is: 0.7488556504249573\n",
            " the confusion matrix is:\n",
            "tensor([[174,  15,  11,   0],\n",
            "        [ 14, 126,  41,  19],\n",
            "        [  3,  24, 159,  14],\n",
            "        [  0,  20,  41, 139]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.000876 \n",
            "\n",
            "f1 score is: 0.7356740832328796\n",
            " the confusion matrix is:\n",
            "tensor([[168,  20,  12,   0],\n",
            "        [  9, 101,  43,  47],\n",
            "        [  2,  21, 156,  21],\n",
            "        [  0,  12,  22, 166]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.000866 \n",
            "\n",
            "f1 score is: 0.7512339353561401\n",
            " the confusion matrix is:\n",
            "tensor([[180,  11,   9,   0],\n",
            "        [ 18, 116,  34,  32],\n",
            "        [  7,  21, 153,  19],\n",
            "        [  0,  20,  26, 154]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.000831 \n",
            "\n",
            "f1 score is: 0.7708014845848083\n",
            " the confusion matrix is:\n",
            "tensor([[184,  12,   4,   0],\n",
            "        [ 16, 132,  27,  25],\n",
            "        [  6,  27, 150,  17],\n",
            "        [  0,  30,  19, 151]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.000924 \n",
            "\n",
            "f1 score is: 0.7368788719177246\n",
            " the confusion matrix is:\n",
            "tensor([[172,  17,  11,   0],\n",
            "        [ 12, 123,  45,  20],\n",
            "        [  5,  26, 156,  13],\n",
            "        [  0,  23,  40, 137]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.000886 \n",
            "\n",
            "f1 score is: 0.7477670907974243\n",
            " the confusion matrix is:\n",
            "tensor([[183,  12,   5,   0],\n",
            "        [ 14, 121,  36,  29],\n",
            "        [  7,  29, 149,  15],\n",
            "        [  0,  23,  31, 146]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.000954 \n",
            "\n",
            "f1 score is: 0.7416106462478638\n",
            " the confusion matrix is:\n",
            "tensor([[175,  12,  13,   0],\n",
            "        [ 14, 105,  44,  37],\n",
            "        [  5,  19, 157,  19],\n",
            "        [  0,  16,  25, 159]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.000951 \n",
            "\n",
            "f1 score is: 0.7427237629890442\n",
            " the confusion matrix is:\n",
            "tensor([[186,   9,   5,   0],\n",
            "        [ 20, 125,  31,  24],\n",
            "        [  7,  35, 143,  15],\n",
            "        [  0,  33,  26, 141]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.000955 \n",
            "\n",
            "f1 score is: 0.7489619851112366\n",
            " the confusion matrix is:\n",
            "tensor([[185,   9,   6,   0],\n",
            "        [ 17, 124,  35,  24],\n",
            "        [  6,  30, 147,  17],\n",
            "        [  0,  27,  29, 144]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.001057 \n",
            "\n",
            "f1 score is: 0.7330250144004822\n",
            " the confusion matrix is:\n",
            "tensor([[157,  23,  18,   2],\n",
            "        [  9, 114,  35,  42],\n",
            "        [  2,  28, 151,  19],\n",
            "        [  0,  16,  20, 164]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 0.001131 \n",
            "\n",
            "f1 score is: 0.7325491905212402\n",
            " the confusion matrix is:\n",
            "tensor([[171,  18,  11,   0],\n",
            "        [ 11, 124,  43,  22],\n",
            "        [  3,  30, 155,  12],\n",
            "        [  0,  24,  42, 134]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.001080 \n",
            "\n",
            "f1 score is: 0.7552208304405212\n",
            " the confusion matrix is:\n",
            "tensor([[173,  21,   6,   0],\n",
            "        [ 12, 136,  26,  26],\n",
            "        [  3,  40, 141,  16],\n",
            "        [  0,  28,  20, 152]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.001162 \n",
            "\n",
            "f1 score is: 0.742251992225647\n",
            " the confusion matrix is:\n",
            "tensor([[178,  20,   2,   0],\n",
            "        [ 14, 130,  30,  26],\n",
            "        [  3,  43, 138,  16],\n",
            "        [  0,  30,  24, 146]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.001162 \n",
            "\n",
            "f1 score is: 0.7496723532676697\n",
            " the confusion matrix is:\n",
            "tensor([[176,  19,   5,   0],\n",
            "        [ 15, 137,  23,  25],\n",
            "        [  5,  44, 135,  16],\n",
            "        [  0,  31,  19, 150]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.001229 \n",
            "\n",
            "f1 score is: 0.7356548309326172\n",
            " the confusion matrix is:\n",
            "tensor([[182,  11,   7,   0],\n",
            "        [ 18, 113,  39,  30],\n",
            "        [  5,  30, 149,  16],\n",
            "        [  0,  22,  32, 146]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.001300 \n",
            "\n",
            "f1 score is: 0.7414533495903015\n",
            " the confusion matrix is:\n",
            "tensor([[172,  21,   5,   2],\n",
            "        [ 12, 115,  29,  44],\n",
            "        [  3,  34, 138,  25],\n",
            "        [  0,  15,  16, 169]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.001287 \n",
            "\n",
            "f1 score is: 0.7447947263717651\n",
            " the confusion matrix is:\n",
            "tensor([[173,  22,   5,   0],\n",
            "        [ 13, 130,  30,  27],\n",
            "        [  4,  40, 139,  17],\n",
            "        [  0,  27,  21, 152]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.001303 \n",
            "\n",
            "f1 score is: 0.7416121363639832\n",
            " the confusion matrix is:\n",
            "tensor([[176,  17,   6,   1],\n",
            "        [ 15, 124,  31,  30],\n",
            "        [  5,  36, 140,  19],\n",
            "        [  0,  23,  24, 153]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.001371 \n",
            "\n",
            "f1 score is: 0.7406837940216064\n",
            " the confusion matrix is:\n",
            "tensor([[171,  19,   9,   1],\n",
            "        [ 13, 118,  38,  31],\n",
            "        [  3,  32, 149,  16],\n",
            "        [  0,  20,  26, 154]])\n",
            "best epoch is: 14\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.000877 \n",
            "\n",
            "f1 score is: 0.75447678565979\n",
            " the confusion matrix is:\n",
            "tensor([[268,  19,   6,   4],\n",
            "        [  9, 196,  94,  25],\n",
            "        [ 10,  71, 274,  44],\n",
            "        [  2,  30,  32, 292]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 47.6%, Avg loss: 0.001399 \n",
            "\n",
            "f1 score is: 0.4199572205543518\n",
            " the confusion matrix is:\n",
            "tensor([[136,  17,  32,  15],\n",
            "        [ 22,  17,  17, 144],\n",
            "        [  6,   4,  36, 154],\n",
            "        [  1,   2,   5, 192]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.1%, Avg loss: 0.001160 \n",
            "\n",
            "f1 score is: 0.5660053491592407\n",
            " the confusion matrix is:\n",
            "tensor([[155,   8,  35,   2],\n",
            "        [ 40,  23,  68,  69],\n",
            "        [  9,   4, 153,  34],\n",
            "        [  5,  14,  23, 158]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.001033 \n",
            "\n",
            "f1 score is: 0.684368908405304\n",
            " the confusion matrix is:\n",
            "tensor([[150,  23,  25,   2],\n",
            "        [ 18,  90,  40,  52],\n",
            "        [  7,  33, 141,  19],\n",
            "        [  0,  21,   9, 170]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.000999 \n",
            "\n",
            "f1 score is: 0.680956244468689\n",
            " the confusion matrix is:\n",
            "tensor([[189,   7,   3,   1],\n",
            "        [ 50, 104,  19,  27],\n",
            "        [ 15,  65,  99,  21],\n",
            "        [  4,  35,   2, 159]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.000928 \n",
            "\n",
            "f1 score is: 0.72745281457901\n",
            " the confusion matrix is:\n",
            "tensor([[178,  16,   5,   1],\n",
            "        [ 29, 109,  17,  45],\n",
            "        [ 12,  46, 117,  25],\n",
            "        [  1,  12,   3, 184]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.000858 \n",
            "\n",
            "f1 score is: 0.7525166869163513\n",
            " the confusion matrix is:\n",
            "tensor([[165,  18,  17,   0],\n",
            "        [ 19,  99,  42,  40],\n",
            "        [  5,  17, 162,  16],\n",
            "        [  0,   8,  10, 182]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.000856 \n",
            "\n",
            "f1 score is: 0.7677156925201416\n",
            " the confusion matrix is:\n",
            "tensor([[156,  35,   8,   1],\n",
            "        [ 15, 126,  27,  32],\n",
            "        [  4,  33, 150,  13],\n",
            "        [  0,  13,   5, 182]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.000931 \n",
            "\n",
            "f1 score is: 0.7514235973358154\n",
            " the confusion matrix is:\n",
            "tensor([[165,  26,   9,   0],\n",
            "        [ 22, 129,  32,  17],\n",
            "        [  5,  33, 153,   9],\n",
            "        [  0,  29,  19, 152]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.001035 \n",
            "\n",
            "f1 score is: 0.7095730900764465\n",
            " the confusion matrix is:\n",
            "tensor([[148,  43,   8,   1],\n",
            "        [ 19, 145,  12,  24],\n",
            "        [  7,  73, 111,   9],\n",
            "        [  0,  34,   8, 158]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 0.001139 \n",
            "\n",
            "f1 score is: 0.6893608570098877\n",
            " the confusion matrix is:\n",
            "tensor([[106,  70,  23,   1],\n",
            "        [ 10, 110,  62,  18],\n",
            "        [  4,  18, 164,  14],\n",
            "        [  0,   8,  19, 173]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.5%, Avg loss: 0.001068 \n",
            "\n",
            "f1 score is: 0.7501394152641296\n",
            " the confusion matrix is:\n",
            "tensor([[179,  13,   7,   1],\n",
            "        [ 32, 105,  42,  21],\n",
            "        [  9,  29, 153,   9],\n",
            "        [  0,  17,  16, 167]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.001002 \n",
            "\n",
            "f1 score is: 0.7326290607452393\n",
            " the confusion matrix is:\n",
            "tensor([[176,  17,   7,   0],\n",
            "        [ 36, 114,  26,  24],\n",
            "        [  9,  52, 128,  11],\n",
            "        [  0,  18,  12, 170]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.001454 \n",
            "\n",
            "f1 score is: 0.6666445732116699\n",
            " the confusion matrix is:\n",
            "tensor([[187,   4,   9,   0],\n",
            "        [ 75,  68,  48,   9],\n",
            "        [ 14,  27, 152,   7],\n",
            "        [  5,  26,  31, 138]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.001167 \n",
            "\n",
            "f1 score is: 0.7342096567153931\n",
            " the confusion matrix is:\n",
            "tensor([[161,  18,  21,   0],\n",
            "        [ 19,  89,  66,  26],\n",
            "        [  4,  21, 166,   9],\n",
            "        [  0,  10,  13, 177]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.001201 \n",
            "\n",
            "f1 score is: 0.738584041595459\n",
            " the confusion matrix is:\n",
            "tensor([[166,  17,  17,   0],\n",
            "        [ 29, 110,  37,  24],\n",
            "        [  9,  33, 145,  13],\n",
            "        [  0,  20,   8, 172]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 0.001459 \n",
            "\n",
            "f1 score is: 0.7055486440658569\n",
            " the confusion matrix is:\n",
            "tensor([[133,  30,  36,   1],\n",
            "        [ 20, 106,  51,  23],\n",
            "        [  6,  22, 160,  12],\n",
            "        [  0,  11,  23, 166]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.001248 \n",
            "\n",
            "f1 score is: 0.7199856042861938\n",
            " the confusion matrix is:\n",
            "tensor([[157,  32,  10,   1],\n",
            "        [ 26, 120,  31,  23],\n",
            "        [  7,  47, 128,  18],\n",
            "        [  0,  23,   7, 170]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.001649 \n",
            "\n",
            "f1 score is: 0.6899381875991821\n",
            " the confusion matrix is:\n",
            "tensor([[132,  55,  12,   1],\n",
            "        [ 11, 106,  47,  36],\n",
            "        [  4,  37, 138,  21],\n",
            "        [  0,  13,  12, 175]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.001478 \n",
            "\n",
            "f1 score is: 0.7290026545524597\n",
            " the confusion matrix is:\n",
            "tensor([[161,  28,  10,   1],\n",
            "        [ 20, 104,  40,  36],\n",
            "        [  5,  29, 147,  19],\n",
            "        [  0,  10,  16, 174]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.001247 \n",
            "\n",
            "f1 score is: 0.7413142919540405\n",
            " the confusion matrix is:\n",
            "tensor([[167,  22,  10,   1],\n",
            "        [ 21, 109,  51,  19],\n",
            "        [  6,  20, 164,  10],\n",
            "        [  0,  27,  19, 154]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.001425 \n",
            "\n",
            "f1 score is: 0.7283110618591309\n",
            " the confusion matrix is:\n",
            "tensor([[163,  10,  27,   0],\n",
            "        [ 28, 103,  46,  23],\n",
            "        [  7,  28, 153,  12],\n",
            "        [  0,  24,  10, 166]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.001483 \n",
            "\n",
            "f1 score is: 0.7205355763435364\n",
            " the confusion matrix is:\n",
            "tensor([[155,  28,  16,   1],\n",
            "        [ 15,  91,  51,  43],\n",
            "        [  6,  16, 163,  15],\n",
            "        [  0,   9,  18, 173]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.001641 \n",
            "\n",
            "f1 score is: 0.7005650997161865\n",
            " the confusion matrix is:\n",
            "tensor([[175,   9,  16,   0],\n",
            "        [ 38,  97,  47,  18],\n",
            "        [  6,  48, 132,  14],\n",
            "        [  0,  22,  19, 159]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.001497 \n",
            "\n",
            "f1 score is: 0.734330415725708\n",
            " the confusion matrix is:\n",
            "tensor([[163,  16,  20,   1],\n",
            "        [ 20, 130,  33,  17],\n",
            "        [  7,  48, 134,  11],\n",
            "        [  0,  30,  12, 158]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.002089 \n",
            "\n",
            "f1 score is: 0.7014476656913757\n",
            " the confusion matrix is:\n",
            "tensor([[161,  24,  12,   3],\n",
            "        [ 22, 112,  37,  29],\n",
            "        [  8,  59, 116,  17],\n",
            "        [  0,  17,  10, 173]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.002169 \n",
            "\n",
            "f1 score is: 0.7055190801620483\n",
            " the confusion matrix is:\n",
            "tensor([[154,   8,  38,   0],\n",
            "        [ 34,  85,  56,  25],\n",
            "        [  7,  18, 161,  14],\n",
            "        [  0,  14,  15, 171]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.002092 \n",
            "\n",
            "f1 score is: 0.6901931166648865\n",
            " the confusion matrix is:\n",
            "tensor([[143,  44,  12,   1],\n",
            "        [ 18, 105,  27,  50],\n",
            "        [  5,  41, 120,  34],\n",
            "        [  0,   8,   5, 187]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.002209 \n",
            "\n",
            "f1 score is: 0.6944713592529297\n",
            " the confusion matrix is:\n",
            "tensor([[123,  56,  20,   1],\n",
            "        [ 15, 119,  36,  30],\n",
            "        [  2,  42, 137,  19],\n",
            "        [  0,  15,  10, 175]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.002107 \n",
            "\n",
            "f1 score is: 0.7018350958824158\n",
            " the confusion matrix is:\n",
            "tensor([[172,  19,   9,   0],\n",
            "        [ 29, 109,  44,  18],\n",
            "        [  9,  45, 135,  11],\n",
            "        [  0,  19,  36, 145]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.001772 \n",
            "\n",
            "f1 score is: 0.7072975039482117\n",
            " the confusion matrix is:\n",
            "tensor([[169,  15,  15,   1],\n",
            "        [ 28,  91,  51,  30],\n",
            "        [  7,  34, 142,  17],\n",
            "        [  0,  10,  21, 169]])\n",
            "best epoch is: 6\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.000999 \n",
            "\n",
            "f1 score is: 0.6798825263977051\n",
            " the confusion matrix is:\n",
            "tensor([[271,  14,  10,   2],\n",
            "        [ 25, 138, 128,  33],\n",
            "        [ 19,  83, 233,  64],\n",
            "        [  4,  36,  21, 295]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.001407 \n",
            "\n",
            "f1 score is: 0.4108162820339203\n",
            " the confusion matrix is:\n",
            "tensor([[172,   0,  22,   6],\n",
            "        [ 87,   0,  46,  67],\n",
            "        [ 41,   0,  59, 100],\n",
            "        [ 13,   0,  16, 171]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 43.4%, Avg loss: 0.001466 \n",
            "\n",
            "f1 score is: 0.3854244351387024\n",
            " the confusion matrix is:\n",
            "tensor([[ 79,  27,  84,  10],\n",
            "        [ 21,   9,  94,  76],\n",
            "        [  2,   1,  77, 120],\n",
            "        [  2,   0,  16, 182]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.001445 \n",
            "\n",
            "f1 score is: 0.4573867917060852\n",
            " the confusion matrix is:\n",
            "tensor([[137,  52,   3,   8],\n",
            "        [ 47,  77,  10,  66],\n",
            "        [  7,  70,  15, 108],\n",
            "        [  6,  13,   3, 178]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 0.001328 \n",
            "\n",
            "f1 score is: 0.4975430965423584\n",
            " the confusion matrix is:\n",
            "tensor([[106,  81,   7,   6],\n",
            "        [ 12, 134,  11,  43],\n",
            "        [  2, 113,  18,  67],\n",
            "        [  1,  30,   0, 169]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 0.001276 \n",
            "\n",
            "f1 score is: 0.5175544619560242\n",
            " the confusion matrix is:\n",
            "tensor([[120,  35,  42,   3],\n",
            "        [ 14,  19, 135,  32],\n",
            "        [  4,   5, 143,  48],\n",
            "        [  1,   2,  42, 155]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.0%, Avg loss: 0.001364 \n",
            "\n",
            "f1 score is: 0.5374643206596375\n",
            " the confusion matrix is:\n",
            "tensor([[ 68,  97,  30,   5],\n",
            "        [  5,  89,  70,  36],\n",
            "        [  1,  48, 118,  33],\n",
            "        [  1,  15,  27, 157]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.5%, Avg loss: 0.001306 \n",
            "\n",
            "f1 score is: 0.5641940236091614\n",
            " the confusion matrix is:\n",
            "tensor([[116,  74,   0,  10],\n",
            "        [ 18, 106,  23,  53],\n",
            "        [  3,  77,  59,  61],\n",
            "        [  3,  14,   4, 179]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 0.001256 \n",
            "\n",
            "f1 score is: 0.5816231966018677\n",
            " the confusion matrix is:\n",
            "tensor([[117,  60,  18,   5],\n",
            "        [  8,  49, 112,  31],\n",
            "        [  1,  14, 146,  39],\n",
            "        [  2,   9,  28, 161]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 0.001276 \n",
            "\n",
            "f1 score is: 0.543133020401001\n",
            " the confusion matrix is:\n",
            "tensor([[163,  37,   0,   0],\n",
            "        [ 56, 114,  18,  12],\n",
            "        [ 12, 138,  33,  17],\n",
            "        [  7,  49,   7, 137]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.001185 \n",
            "\n",
            "f1 score is: 0.6149153709411621\n",
            " the confusion matrix is:\n",
            "tensor([[134,  56,   9,   1],\n",
            "        [ 33,  89,  50,  28],\n",
            "        [  9,  48, 105,  38],\n",
            "        [  2,  17,  16, 165]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.2%, Avg loss: 0.001168 \n",
            "\n",
            "f1 score is: 0.6013937592506409\n",
            " the confusion matrix is:\n",
            "tensor([[151,  45,   4,   0],\n",
            "        [ 50,  80,  47,  23],\n",
            "        [ 10,  55, 104,  31],\n",
            "        [  9,  29,  15, 147]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.001220 \n",
            "\n",
            "f1 score is: 0.5734783411026001\n",
            " the confusion matrix is:\n",
            "tensor([[123,  61,  11,   5],\n",
            "        [ 21,  96,  16,  67],\n",
            "        [  2,  68,  68,  62],\n",
            "        [  3,  12,   6, 179]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.001336 \n",
            "\n",
            "f1 score is: 0.6252326369285583\n",
            " the confusion matrix is:\n",
            "tensor([[131,  63,   6,   0],\n",
            "        [ 10, 128,  34,  28],\n",
            "        [  5,  77,  80,  38],\n",
            "        [  1,  27,  13, 159]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 0.001336 \n",
            "\n",
            "f1 score is: 0.6180513501167297\n",
            " the confusion matrix is:\n",
            "tensor([[155,  29,  16,   0],\n",
            "        [ 34,  86,  45,  35],\n",
            "        [  7,  53,  97,  43],\n",
            "        [  3,  19,  17, 161]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.001519 \n",
            "\n",
            "f1 score is: 0.6231528520584106\n",
            " the confusion matrix is:\n",
            "tensor([[137,  56,   4,   3],\n",
            "        [ 22, 104,  38,  36],\n",
            "        [  7,  52,  92,  49],\n",
            "        [  2,  18,  13, 167]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.001446 \n",
            "\n",
            "f1 score is: 0.63282310962677\n",
            " the confusion matrix is:\n",
            "tensor([[132,  35,  33,   0],\n",
            "        [ 18,  83,  75,  24],\n",
            "        [  4,  32, 138,  26],\n",
            "        [  2,  13,  33, 152]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 0.001650 \n",
            "\n",
            "f1 score is: 0.6245877742767334\n",
            " the confusion matrix is:\n",
            "tensor([[112,  71,  16,   1],\n",
            "        [ 11,  96,  60,  33],\n",
            "        [  0,  44, 125,  31],\n",
            "        [  0,  19,  18, 163]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 0.001426 \n",
            "\n",
            "f1 score is: 0.6418008804321289\n",
            " the confusion matrix is:\n",
            "tensor([[129,  49,  21,   1],\n",
            "        [ 21,  99,  43,  37],\n",
            "        [  5,  41, 121,  33],\n",
            "        [  0,  16,  20, 164]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.001545 \n",
            "\n",
            "f1 score is: 0.6085337996482849\n",
            " the confusion matrix is:\n",
            "tensor([[166,   6,  28,   0],\n",
            "        [ 51,  51,  77,  21],\n",
            "        [ 17,  26, 133,  24],\n",
            "        [  7,  15,  26, 152]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.002047 \n",
            "\n",
            "f1 score is: 0.6328932642936707\n",
            " the confusion matrix is:\n",
            "tensor([[135,  35,  30,   0],\n",
            "        [ 20, 102,  57,  21],\n",
            "        [  3,  47, 129,  21],\n",
            "        [  4,  24,  37, 135]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.001823 \n",
            "\n",
            "f1 score is: 0.629388689994812\n",
            " the confusion matrix is:\n",
            "tensor([[130,  46,  24,   0],\n",
            "        [ 18, 116,  31,  35],\n",
            "        [  3,  64,  95,  38],\n",
            "        [  1,  22,  16, 161]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.001545 \n",
            "\n",
            "f1 score is: 0.6622554063796997\n",
            " the confusion matrix is:\n",
            "tensor([[164,  29,   7,   0],\n",
            "        [ 27, 108,  48,  17],\n",
            "        [  7,  56, 111,  26],\n",
            "        [  2,  31,  22, 145]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.001516 \n",
            "\n",
            "f1 score is: 0.6462624073028564\n",
            " the confusion matrix is:\n",
            "tensor([[169,  19,  11,   1],\n",
            "        [ 48,  83,  47,  22],\n",
            "        [ 16,  33, 125,  26],\n",
            "        [  5,  18,  31, 146]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.6%, Avg loss: 0.001716 \n",
            "\n",
            "f1 score is: 0.6350798606872559\n",
            " the confusion matrix is:\n",
            "tensor([[137,  27,  35,   1],\n",
            "        [ 25,  89,  54,  32],\n",
            "        [  6,  38, 123,  33],\n",
            "        [  1,  16,  23, 160]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.0%, Avg loss: 0.001681 \n",
            "\n",
            "f1 score is: 0.6141113042831421\n",
            " the confusion matrix is:\n",
            "tensor([[134,  55,   9,   2],\n",
            "        [ 23, 118,  35,  24],\n",
            "        [  3,  83,  83,  31],\n",
            "        [  2,  27,  18, 153]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 0.001742 \n",
            "\n",
            "f1 score is: 0.6292552351951599\n",
            " the confusion matrix is:\n",
            "tensor([[148,  31,  21,   0],\n",
            "        [ 30,  85,  58,  27],\n",
            "        [ 12,  42, 121,  25],\n",
            "        [  1,  19,  30, 150]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 0.002617 \n",
            "\n",
            "f1 score is: 0.6089493036270142\n",
            " the confusion matrix is:\n",
            "tensor([[186,   8,   6,   0],\n",
            "        [ 71,  67,  51,  11],\n",
            "        [ 24,  45, 115,  16],\n",
            "        [  7,  28,  36, 129]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 0.002448 \n",
            "\n",
            "f1 score is: 0.6078266501426697\n",
            " the confusion matrix is:\n",
            "tensor([[130,  51,  16,   3],\n",
            "        [ 20, 112,  39,  29],\n",
            "        [  6,  75,  86,  33],\n",
            "        [  2,  24,  18, 156]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 0.002463 \n",
            "\n",
            "f1 score is: 0.6231231093406677\n",
            " the confusion matrix is:\n",
            "tensor([[141,  29,  30,   0],\n",
            "        [ 25,  72,  85,  18],\n",
            "        [  4,  29, 143,  24],\n",
            "        [  2,  18,  37, 143]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.002155 \n",
            "\n",
            "f1 score is: 0.6260892152786255\n",
            " the confusion matrix is:\n",
            "tensor([[155,  36,   8,   1],\n",
            "        [ 26,  97,  54,  23],\n",
            "        [  9,  61, 107,  23],\n",
            "        [  3,  25,  33, 139]])\n",
            "best epoch is: 21\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 0.001197 \n",
            "\n",
            "f1 score is: 0.64720618724823\n",
            " the confusion matrix is:\n",
            "tensor([[255,  29,  10,   3],\n",
            "        [ 23, 178,  94,  29],\n",
            "        [ 24, 119, 209,  47],\n",
            "        [  9,  57,  55, 235]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.001257 \n",
            "\n",
            "f1 score is: 0.5804572701454163\n",
            " the confusion matrix is:\n",
            "tensor([[142,  42,  13,   3],\n",
            "        [ 22,  76,  72,  30],\n",
            "        [  1,  21, 154,  24],\n",
            "        [  3,  10,  93,  94]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.2%, Avg loss: 0.001270 \n",
            "\n",
            "f1 score is: 0.5570400357246399\n",
            " the confusion matrix is:\n",
            "tensor([[107,  65,  21,   7],\n",
            "        [  9,  42, 108,  41],\n",
            "        [  1,   4, 163,  32],\n",
            "        [  0,   2,  52, 146]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.001096 \n",
            "\n",
            "f1 score is: 0.650394082069397\n",
            " the confusion matrix is:\n",
            "tensor([[142,  53,   3,   2],\n",
            "        [ 29, 113,  46,  12],\n",
            "        [  1,  49, 129,  21],\n",
            "        [  2,  19,  48, 131]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.000985 \n",
            "\n",
            "f1 score is: 0.6712428331375122\n",
            " the confusion matrix is:\n",
            "tensor([[198,   2,   0,   0],\n",
            "        [ 57, 106,  12,  25],\n",
            "        [ 15,  58,  91,  36],\n",
            "        [  4,  28,  15, 153]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.8%, Avg loss: 0.001106 \n",
            "\n",
            "f1 score is: 0.6580959558486938\n",
            " the confusion matrix is:\n",
            "tensor([[127,  70,   0,   3],\n",
            "        [  3, 129,   5,  63],\n",
            "        [  1,  54,  87,  58],\n",
            "        [  0,  15,   2, 183]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.000899 \n",
            "\n",
            "f1 score is: 0.7082738876342773\n",
            " the confusion matrix is:\n",
            "tensor([[191,   6,   3,   0],\n",
            "        [ 32,  77,  53,  38],\n",
            "        [ 10,  12, 144,  34],\n",
            "        [  1,   6,  25, 168]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.000830 \n",
            "\n",
            "f1 score is: 0.7410514950752258\n",
            " the confusion matrix is:\n",
            "tensor([[190,   4,   6,   0],\n",
            "        [ 31, 113,  26,  30],\n",
            "        [ 14,  21, 134,  31],\n",
            "        [  0,  23,  16, 161]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.000841 \n",
            "\n",
            "f1 score is: 0.7441651225090027\n",
            " the confusion matrix is:\n",
            "tensor([[170,  19,  11,   0],\n",
            "        [  6, 112,  37,  45],\n",
            "        [  4,  26, 148,  22],\n",
            "        [  0,  12,  22, 166]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.1%, Avg loss: 0.000838 \n",
            "\n",
            "f1 score is: 0.7549022436141968\n",
            " the confusion matrix is:\n",
            "tensor([[167,  31,   2,   0],\n",
            "        [  8, 140,  28,  24],\n",
            "        [  3,  33, 143,  21],\n",
            "        [  0,  26,  23, 151]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 0.001062 \n",
            "\n",
            "f1 score is: 0.7003644704818726\n",
            " the confusion matrix is:\n",
            "tensor([[188,   5,   7,   0],\n",
            "        [ 28, 102,  61,   9],\n",
            "        [ 12,  16, 164,   8],\n",
            "        [  1,  27,  61, 111]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.001219 \n",
            "\n",
            "f1 score is: 0.6771794557571411\n",
            " the confusion matrix is:\n",
            "tensor([[199,   0,   1,   0],\n",
            "        [ 59,  95,  36,  10],\n",
            "        [ 26,  25, 137,  12],\n",
            "        [  4,  44,  33, 119]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.001198 \n",
            "\n",
            "f1 score is: 0.6882892847061157\n",
            " the confusion matrix is:\n",
            "tensor([[193,   2,   5,   0],\n",
            "        [ 44,  74,  59,  23],\n",
            "        [ 15,  10, 158,  17],\n",
            "        [  1,  21,  39, 139]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.001015 \n",
            "\n",
            "f1 score is: 0.7475394606590271\n",
            " the confusion matrix is:\n",
            "tensor([[192,   6,   2,   0],\n",
            "        [ 17, 113,  33,  37],\n",
            "        [ 12,  22, 142,  24],\n",
            "        [  0,  25,  20, 155]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.001232 \n",
            "\n",
            "f1 score is: 0.7408460974693298\n",
            " the confusion matrix is:\n",
            "tensor([[190,   5,   5,   0],\n",
            "        [ 23, 127,  33,  17],\n",
            "        [ 10,  35, 142,  13],\n",
            "        [  0,  40,  25, 135]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.001316 \n",
            "\n",
            "f1 score is: 0.7410531044006348\n",
            " the confusion matrix is:\n",
            "tensor([[185,  14,   1,   0],\n",
            "        [ 15, 129,  35,  21],\n",
            "        [  8,  32, 148,  12],\n",
            "        [  0,  36,  33, 131]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.001615 \n",
            "\n",
            "f1 score is: 0.7184121608734131\n",
            " the confusion matrix is:\n",
            "tensor([[167,  12,  21,   0],\n",
            "        [ 20, 106,  44,  30],\n",
            "        [  9,  18, 145,  28],\n",
            "        [  1,  22,  18, 159]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.001511 \n",
            "\n",
            "f1 score is: 0.7416457533836365\n",
            " the confusion matrix is:\n",
            "tensor([[183,  10,   7,   0],\n",
            "        [ 15, 137,  30,  18],\n",
            "        [  8,  45, 133,  14],\n",
            "        [  0,  44,  17, 139]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.001635 \n",
            "\n",
            "f1 score is: 0.7352687120437622\n",
            " the confusion matrix is:\n",
            "tensor([[183,  10,   7,   0],\n",
            "        [ 13, 124,  30,  33],\n",
            "        [ 10,  38, 131,  21],\n",
            "        [  0,  30,  19, 151]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.001618 \n",
            "\n",
            "f1 score is: 0.7482606768608093\n",
            " the confusion matrix is:\n",
            "tensor([[183,   9,   8,   0],\n",
            "        [ 12, 115,  42,  31],\n",
            "        [  6,  22, 157,  15],\n",
            "        [  0,  22,  33, 145]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.001929 \n",
            "\n",
            "f1 score is: 0.7459474802017212\n",
            " the confusion matrix is:\n",
            "tensor([[170,  21,   9,   0],\n",
            "        [  7, 128,  42,  23],\n",
            "        [  6,  22, 154,  18],\n",
            "        [  0,  25,  32, 143]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.001764 \n",
            "\n",
            "f1 score is: 0.7560441493988037\n",
            " the confusion matrix is:\n",
            "tensor([[186,  11,   3,   0],\n",
            "        [ 16, 138,  24,  22],\n",
            "        [ 10,  36, 138,  16],\n",
            "        [  0,  36,  21, 143]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.002131 \n",
            "\n",
            "f1 score is: 0.733066976070404\n",
            " the confusion matrix is:\n",
            "tensor([[179,  13,   8,   0],\n",
            "        [ 13, 103,  36,  48],\n",
            "        [  8,  22, 145,  25],\n",
            "        [  0,  19,  18, 163]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.002094 \n",
            "\n",
            "f1 score is: 0.7317529916763306\n",
            " the confusion matrix is:\n",
            "tensor([[181,  11,   8,   0],\n",
            "        [ 14, 113,  44,  29],\n",
            "        [ 10,  22, 152,  16],\n",
            "        [  1,  25,  33, 141]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.002380 \n",
            "\n",
            "f1 score is: 0.708197832107544\n",
            " the confusion matrix is:\n",
            "tensor([[172,  26,   2,   0],\n",
            "        [  7, 146,  14,  33],\n",
            "        [  7,  76,  87,  30],\n",
            "        [  0,  29,   7, 164]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.002046 \n",
            "\n",
            "f1 score is: 0.7340524196624756\n",
            " the confusion matrix is:\n",
            "tensor([[162,  26,  12,   0],\n",
            "        [  8, 121,  32,  39],\n",
            "        [  5,  27, 143,  25],\n",
            "        [  0,  23,  17, 160]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.002124 \n",
            "\n",
            "f1 score is: 0.748020350933075\n",
            " the confusion matrix is:\n",
            "tensor([[183,  11,   6,   0],\n",
            "        [ 12, 118,  35,  35],\n",
            "        [ 10,  22, 147,  21],\n",
            "        [  0,  28,  20, 152]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.002372 \n",
            "\n",
            "f1 score is: 0.7486437559127808\n",
            " the confusion matrix is:\n",
            "tensor([[166,  23,  11,   0],\n",
            "        [  8, 128,  31,  33],\n",
            "        [  8,  22, 147,  23],\n",
            "        [  0,  22,  21, 157]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.002366 \n",
            "\n",
            "f1 score is: 0.753055214881897\n",
            " the confusion matrix is:\n",
            "tensor([[178,  14,   8,   0],\n",
            "        [ 11, 125,  32,  32],\n",
            "        [ 10,  23, 145,  22],\n",
            "        [  0,  25,  20, 155]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.002453 \n",
            "\n",
            "f1 score is: 0.7554934024810791\n",
            " the confusion matrix is:\n",
            "tensor([[178,  14,   8,   0],\n",
            "        [ 11, 125,  32,  32],\n",
            "        [ 10,  22, 147,  21],\n",
            "        [  0,  24,  21, 155]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.002534 \n",
            "\n",
            "f1 score is: 0.752901554107666\n",
            " the confusion matrix is:\n",
            "tensor([[178,  14,   8,   0],\n",
            "        [ 11, 124,  32,  33],\n",
            "        [ 10,  22, 146,  22],\n",
            "        [  0,  24,  21, 155]])\n",
            "best epoch is: 20\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.001751 \n",
            "\n",
            "f1 score is: 0.7297154664993286\n",
            " the confusion matrix is:\n",
            "tensor([[266,  19,  11,   1],\n",
            "        [ 23, 190,  90,  21],\n",
            "        [ 14,  77, 263,  45],\n",
            "        [  4,  36,  38, 278]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 41.6%, Avg loss: 0.001690 \n",
            "\n",
            "f1 score is: 0.2783787250518799\n",
            " the confusion matrix is:\n",
            "tensor([[173,   0,   0,  27],\n",
            "        [106,   0,   0,  94],\n",
            "        [ 57,   0,   0, 143],\n",
            "        [ 40,   0,   0, 160]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.4%, Avg loss: 0.001573 \n",
            "\n",
            "f1 score is: 0.26205334067344666\n",
            " the confusion matrix is:\n",
            "tensor([[175,   0,   0,  25],\n",
            "        [117,   0,   0,  83],\n",
            "        [ 59,   0,   0, 141],\n",
            "        [ 60,   0,   0, 140]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 0.001457 \n",
            "\n",
            "f1 score is: 0.38587966561317444\n",
            " the confusion matrix is:\n",
            "tensor([[147,  28,   0,  25],\n",
            "        [ 53,  69,   0,  78],\n",
            "        [ 18,  46,   0, 136],\n",
            "        [ 14,  39,   0, 147]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 42.0%, Avg loss: 0.001444 \n",
            "\n",
            "f1 score is: 0.3029190003871918\n",
            " the confusion matrix is:\n",
            "tensor([[185,   2,   0,  13],\n",
            "        [112,  13,   0,  75],\n",
            "        [ 48,   9,   0, 143],\n",
            "        [ 60,   2,   0, 138]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.001345 \n",
            "\n",
            "f1 score is: 0.4698740839958191\n",
            " the confusion matrix is:\n",
            "tensor([[153,  24,   2,  21],\n",
            "        [ 60,  29,  28,  83],\n",
            "        [ 15,  16,  81,  88],\n",
            "        [ 16,  15,  33, 136]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 0.001442 \n",
            "\n",
            "f1 score is: 0.46449413895606995\n",
            " the confusion matrix is:\n",
            "tensor([[102,  51,  10,  37],\n",
            "        [ 23,  30,  52,  95],\n",
            "        [  1,  10,  99,  90],\n",
            "        [  0,   9,  37, 154]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.001318 \n",
            "\n",
            "f1 score is: 0.5021881461143494\n",
            " the confusion matrix is:\n",
            "tensor([[138,  56,   6,   0],\n",
            "        [ 43,  77,  62,  18],\n",
            "        [  8,  41, 140,  11],\n",
            "        [  9,  42,  94,  55]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.4%, Avg loss: 0.001362 \n",
            "\n",
            "f1 score is: 0.4437776803970337\n",
            " the confusion matrix is:\n",
            "tensor([[141,  23,  28,   8],\n",
            "        [ 48,  12,  98,  42],\n",
            "        [ 11,   2, 168,  19],\n",
            "        [  9,   6, 111,  74]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.4%, Avg loss: 0.001288 \n",
            "\n",
            "f1 score is: 0.482147753238678\n",
            " the confusion matrix is:\n",
            "tensor([[151,  23,  16,  10],\n",
            "        [ 54,  17,  78,  51],\n",
            "        [ 12,   8, 147,  33],\n",
            "        [ 10,   7,  79, 104]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.1%, Avg loss: 0.001278 \n",
            "\n",
            "f1 score is: 0.5188045501708984\n",
            " the confusion matrix is:\n",
            "tensor([[136,  44,   4,  16],\n",
            "        [ 41,  48,  43,  68],\n",
            "        [  5,  20,  96,  79],\n",
            "        [  6,  16,  33, 145]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.001350 \n",
            "\n",
            "f1 score is: 0.49512195587158203\n",
            " the confusion matrix is:\n",
            "tensor([[109,  61,  18,  12],\n",
            "        [ 27,  38,  74,  61],\n",
            "        [  1,  16, 143,  40],\n",
            "        [  0,  14,  71, 115]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.2%, Avg loss: 0.001263 \n",
            "\n",
            "f1 score is: 0.5479691028594971\n",
            " the confusion matrix is:\n",
            "tensor([[161,  37,   2,   0],\n",
            "        [ 57, 107,  23,  13],\n",
            "        [ 11,  73, 108,   8],\n",
            "        [ 11,  72,  51,  66]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.2%, Avg loss: 0.001232 \n",
            "\n",
            "f1 score is: 0.5655704736709595\n",
            " the confusion matrix is:\n",
            "tensor([[146,  42,   9,   3],\n",
            "        [ 44,  64,  64,  28],\n",
            "        [  6,  26, 145,  23],\n",
            "        [  4,  25,  68, 103]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.8%, Avg loss: 0.001220 \n",
            "\n",
            "f1 score is: 0.5329960584640503\n",
            " the confusion matrix is:\n",
            "tensor([[154,  16,  14,  16],\n",
            "        [ 40,  26,  73,  61],\n",
            "        [  7,   5, 139,  49],\n",
            "        [  3,  10,  52, 135]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.001178 \n",
            "\n",
            "f1 score is: 0.5803999304771423\n",
            " the confusion matrix is:\n",
            "tensor([[153,  39,   3,   5],\n",
            "        [ 41,  79,  36,  44],\n",
            "        [  6,  45, 105,  44],\n",
            "        [  4,  29,  38, 129]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.1%, Avg loss: 0.001230 \n",
            "\n",
            "f1 score is: 0.5624035596847534\n",
            " the confusion matrix is:\n",
            "tensor([[140,  41,   1,  18],\n",
            "        [ 33,  74,  15,  78],\n",
            "        [  3,  42,  75,  80],\n",
            "        [  1,  20,  11, 168]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.8%, Avg loss: 0.001188 \n",
            "\n",
            "f1 score is: 0.585275411605835\n",
            " the confusion matrix is:\n",
            "tensor([[163,  33,   4,   0],\n",
            "        [ 43,  89,  53,  15],\n",
            "        [  7,  43, 130,  20],\n",
            "        [  1,  48,  63,  88]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.6%, Avg loss: 0.001211 \n",
            "\n",
            "f1 score is: 0.5297183990478516\n",
            " the confusion matrix is:\n",
            "tensor([[165,  10,  20,   5],\n",
            "        [ 52,  29,  98,  21],\n",
            "        [  5,   6, 168,  21],\n",
            "        [  4,  10,  95,  91]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.0%, Avg loss: 0.001192 \n",
            "\n",
            "f1 score is: 0.5775368809700012\n",
            " the confusion matrix is:\n",
            "tensor([[126,  53,  10,  11],\n",
            "        [ 26,  69,  58,  47],\n",
            "        [  2,  26, 128,  44],\n",
            "        [  0,  14,  45, 141]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.001093 \n",
            "\n",
            "f1 score is: 0.6285419464111328\n",
            " the confusion matrix is:\n",
            "tensor([[167,  26,   4,   3],\n",
            "        [ 40,  94,  35,  31],\n",
            "        [  6,  43, 115,  36],\n",
            "        [  1,  24,  46, 129]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.001106 \n",
            "\n",
            "f1 score is: 0.6256139874458313\n",
            " the confusion matrix is:\n",
            "tensor([[169,  25,   4,   2],\n",
            "        [ 48,  99,  38,  15],\n",
            "        [  8,  45, 121,  26],\n",
            "        [  2,  35,  50, 113]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 0.001157 \n",
            "\n",
            "f1 score is: 0.6069035530090332\n",
            " the confusion matrix is:\n",
            "tensor([[129,  54,   3,  14],\n",
            "        [ 20,  80,  26,  74],\n",
            "        [  2,  28, 111,  59],\n",
            "        [  0,  12,  21, 167]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.001096 \n",
            "\n",
            "f1 score is: 0.6413324475288391\n",
            " the confusion matrix is:\n",
            "tensor([[163,  31,   4,   2],\n",
            "        [ 34, 117,  35,  14],\n",
            "        [  5,  46, 127,  22],\n",
            "        [  0,  38,  57, 105]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.1%, Avg loss: 0.001166 \n",
            "\n",
            "f1 score is: 0.6167851686477661\n",
            " the confusion matrix is:\n",
            "tensor([[106,  86,   3,   5],\n",
            "        [ 15, 127,  25,  33],\n",
            "        [  1,  47, 111,  41],\n",
            "        [  0,  24,  31, 145]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.6%, Avg loss: 0.001152 \n",
            "\n",
            "f1 score is: 0.6282569169998169\n",
            " the confusion matrix is:\n",
            "tensor([[179,  15,   5,   1],\n",
            "        [ 51,  93,  52,   4],\n",
            "        [  9,  31, 144,  16],\n",
            "        [  2,  33,  72,  93]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.6%, Avg loss: 0.001051 \n",
            "\n",
            "f1 score is: 0.6505830883979797\n",
            " the confusion matrix is:\n",
            "tensor([[178,  16,   5,   1],\n",
            "        [ 47,  93,  41,  19],\n",
            "        [  7,  33, 133,  27],\n",
            "        [  1,  25,  53, 121]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.001147 \n",
            "\n",
            "f1 score is: 0.6322551369667053\n",
            " the confusion matrix is:\n",
            "tensor([[133,  59,   3,   5],\n",
            "        [ 18,  97,  26,  59],\n",
            "        [  1,  31, 111,  57],\n",
            "        [  0,  14,  22, 164]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 0.001064 \n",
            "\n",
            "f1 score is: 0.6615171432495117\n",
            " the confusion matrix is:\n",
            "tensor([[175,  22,   3,   0],\n",
            "        [ 39, 131,  25,   5],\n",
            "        [  7,  59, 113,  21],\n",
            "        [  0,  49,  41, 110]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.001115 \n",
            "\n",
            "f1 score is: 0.6159444451332092\n",
            " the confusion matrix is:\n",
            "tensor([[140,  39,  12,   9],\n",
            "        [ 17,  66,  29,  88],\n",
            "        [  2,  15, 115,  68],\n",
            "        [  0,   8,  13, 179]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.000993 \n",
            "\n",
            "f1 score is: 0.666645348072052\n",
            " the confusion matrix is:\n",
            "tensor([[179,  18,   1,   2],\n",
            "        [ 35, 109,  10,  46],\n",
            "        [  9,  50,  91,  50],\n",
            "        [  0,  26,  13, 161]])\n",
            "best epoch is: 29\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.4%, Avg loss: 0.000620 \n",
            "\n",
            "f1 score is: 0.6590175032615662\n",
            " the confusion matrix is:\n",
            "tensor([[259,  23,   0,  15],\n",
            "        [ 33, 199,  28,  64],\n",
            "        [ 23, 117, 150, 109],\n",
            "        [ 10,  35,   6, 305]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 26.5%, Avg loss: 0.001723 \n",
            "\n",
            "f1 score is: 0.13220372796058655\n",
            " the confusion matrix is:\n",
            "tensor([[ 10, 190,   0,   0],\n",
            "        [  1, 199,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 195,   2,   3]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001734 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0],\n",
            "        [  0, 200,   0,   0]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0],\n",
            "        [  0,   0, 200,   0]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[200,   0,   0,   0],\n",
            "        [200,   0,   0,   0],\n",
            "        [200,   0,   0,   0],\n",
            "        [200,   0,   0,   0]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.001733 \n",
            "\n",
            "f1 score is: 0.10000000149011612\n",
            " the confusion matrix is:\n",
            "tensor([[200,   0,   0,   0],\n",
            "        [200,   0,   0,   0],\n",
            "        [200,   0,   0,   0],\n",
            "        [200,   0,   0,   0]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.4%, Avg loss: 0.001795 \n",
            "\n",
            "f1 score is: 0.11840779334306717\n",
            " the confusion matrix is:\n",
            "tensor([[  0,  52,   0, 148],\n",
            "        [  4,   7,   0, 189],\n",
            "        [  0,   1,   0, 199],\n",
            "        [  0,   4,   0, 196]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.6%, Avg loss: 0.001279 \n",
            "\n",
            "f1 score is: 0.49001869559288025\n",
            " the confusion matrix is:\n",
            "tensor([[148,  13,  38,   1],\n",
            "        [ 59,  13,  95,  33],\n",
            "        [  8,   5, 141,  46],\n",
            "        [  8,   1,  64, 127]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.0%, Avg loss: 0.001191 \n",
            "\n",
            "f1 score is: 0.5855289697647095\n",
            " the confusion matrix is:\n",
            "tensor([[153,  39,   8,   0],\n",
            "        [ 52,  65,  67,  16],\n",
            "        [  8,  41, 124,  27],\n",
            "        [  4,  10,  56, 130]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 0.001122 \n",
            "\n",
            "f1 score is: 0.5921998023986816\n",
            " the confusion matrix is:\n",
            "tensor([[180,  18,   2,   0],\n",
            "        [ 76,  49,  47,  28],\n",
            "        [ 12,  35, 107,  46],\n",
            "        [ 10,   9,  22, 159]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 0.001137 \n",
            "\n",
            "f1 score is: 0.624752938747406\n",
            " the confusion matrix is:\n",
            "tensor([[160,  38,   0,   2],\n",
            "        [ 44,  87,  39,  30],\n",
            "        [  4,  60,  98,  38],\n",
            "        [  5,  16,  20, 159]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.001274 \n",
            "\n",
            "f1 score is: 0.6257189512252808\n",
            " the confusion matrix is:\n",
            "tensor([[154,  32,  14,   0],\n",
            "        [ 38,  83,  66,  13],\n",
            "        [  6,  32, 140,  22],\n",
            "        [  4,  10,  62, 124]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.001239 \n",
            "\n",
            "f1 score is: 0.619408369064331\n",
            " the confusion matrix is:\n",
            "tensor([[145,  47,   7,   1],\n",
            "        [ 39,  74,  60,  27],\n",
            "        [  7,  33, 126,  34],\n",
            "        [  6,  10,  29, 155]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.001503 \n",
            "\n",
            "f1 score is: 0.6217384934425354\n",
            " the confusion matrix is:\n",
            "tensor([[129,  53,  12,   6],\n",
            "        [ 25, 102,  38,  35],\n",
            "        [  6,  49, 104,  41],\n",
            "        [  5,  15,  17, 163]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 0.001566 \n",
            "\n",
            "f1 score is: 0.6154809594154358\n",
            " the confusion matrix is:\n",
            "tensor([[156,  30,  12,   2],\n",
            "        [ 52,  86,  41,  21],\n",
            "        [  9,  50, 103,  38],\n",
            "        [  5,  26,  18, 151]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 0.001796 \n",
            "\n",
            "f1 score is: 0.6230149269104004\n",
            " the confusion matrix is:\n",
            "tensor([[159,  22,  18,   1],\n",
            "        [ 46,  80,  52,  22],\n",
            "        [  8,  54, 107,  31],\n",
            "        [  5,  20,  18, 157]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 0.001291 \n",
            "\n",
            "f1 score is: 0.6297452449798584\n",
            " the confusion matrix is:\n",
            "tensor([[126,  62,   9,   3],\n",
            "        [ 23,  96,  47,  34],\n",
            "        [  4,  34, 122,  40],\n",
            "        [  4,  20,  17, 159]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.0%, Avg loss: 0.001910 \n",
            "\n",
            "f1 score is: 0.5832582712173462\n",
            " the confusion matrix is:\n",
            "tensor([[145,  35,  14,   6],\n",
            "        [ 33,  78,  50,  39],\n",
            "        [  9,  44,  92,  55],\n",
            "        [  4,  19,  20, 157]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.001985 \n",
            "\n",
            "f1 score is: 0.6205915808677673\n",
            " the confusion matrix is:\n",
            "tensor([[146,  35,  14,   5],\n",
            "        [ 26,  79,  66,  29],\n",
            "        [  3,  31, 130,  36],\n",
            "        [  4,  17,  36, 143]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.002070 \n",
            "\n",
            "f1 score is: 0.6171998977661133\n",
            " the confusion matrix is:\n",
            "tensor([[148,  35,  15,   2],\n",
            "        [ 30,  87,  60,  23],\n",
            "        [  6,  47, 116,  31],\n",
            "        [  7,  26,  25, 142]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.5%, Avg loss: 0.002192 \n",
            "\n",
            "f1 score is: 0.5754671692848206\n",
            " the confusion matrix is:\n",
            "tensor([[139,  32,  24,   5],\n",
            "        [ 30,  69,  53,  48],\n",
            "        [  7,  41,  94,  58],\n",
            "        [  3,  15,  16, 166]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.0%, Avg loss: 0.002543 \n",
            "\n",
            "f1 score is: 0.6196644902229309\n",
            " the confusion matrix is:\n",
            "tensor([[139,  45,  16,   0],\n",
            "        [ 24,  84,  62,  30],\n",
            "        [  3,  36, 119,  42],\n",
            "        [  3,  16,  27, 154]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.002994 \n",
            "\n",
            "f1 score is: 0.5817922949790955\n",
            " the confusion matrix is:\n",
            "tensor([[122,  60,  12,   6],\n",
            "        [ 18,  96,  49,  37],\n",
            "        [  4,  48,  89,  59],\n",
            "        [  5,  16,  20, 159]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.002683 \n",
            "\n",
            "f1 score is: 0.6021813750267029\n",
            " the confusion matrix is:\n",
            "tensor([[161,  21,  16,   2],\n",
            "        [ 46,  60,  66,  28],\n",
            "        [ 11,  20, 131,  38],\n",
            "        [  8,  16,  35, 141]])\n",
            "best epoch is: 22\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 0.001726 \n",
            "\n",
            "f1 score is: 0.5827581882476807\n",
            " the confusion matrix is:\n",
            "tensor([[243,  19,  28,   7],\n",
            "        [ 32, 123, 131,  38],\n",
            "        [ 41,  81, 203,  74],\n",
            "        [ 21,  30,  75, 230]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.001264 \n",
            "\n",
            "f1 score is: 0.4455846846103668\n",
            " the confusion matrix is:\n",
            "tensor([[164,  35,   1,   0],\n",
            "        [ 39,  98,  62,   1],\n",
            "        [  5,  47, 144,   4],\n",
            "        [  7,  38, 154,   1]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.4%, Avg loss: 0.001181 \n",
            "\n",
            "f1 score is: 0.5253787040710449\n",
            " the confusion matrix is:\n",
            "tensor([[193,   4,   3,   0],\n",
            "        [ 81,  16,  99,   4],\n",
            "        [ 16,   3, 162,  19],\n",
            "        [  9,   7,  88,  96]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.000982 \n",
            "\n",
            "f1 score is: 0.6731833219528198\n",
            " the confusion matrix is:\n",
            "tensor([[160,  37,   3,   0],\n",
            "        [ 24,  81,  52,  43],\n",
            "        [  1,  17, 137,  45],\n",
            "        [  3,   7,  23, 167]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.000987 \n",
            "\n",
            "f1 score is: 0.6713746786117554\n",
            " the confusion matrix is:\n",
            "tensor([[185,  15,   0,   0],\n",
            "        [ 48,  74,  61,  17],\n",
            "        [  4,  20, 152,  24],\n",
            "        [  5,   7,  51, 137]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.000806 \n",
            "\n",
            "f1 score is: 0.7184686660766602\n",
            " the confusion matrix is:\n",
            "tensor([[180,  17,   2,   1],\n",
            "        [ 18,  91,  48,  43],\n",
            "        [  4,  31, 140,  25],\n",
            "        [  0,   6,  24, 170]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.000897 \n",
            "\n",
            "f1 score is: 0.6989926099777222\n",
            " the confusion matrix is:\n",
            "tensor([[149,  44,   6,   1],\n",
            "        [ 17,  92,  60,  31],\n",
            "        [  7,  10, 155,  28],\n",
            "        [  0,   8,  25, 167]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.000842 \n",
            "\n",
            "f1 score is: 0.7224183082580566\n",
            " the confusion matrix is:\n",
            "tensor([[176,  20,   4,   0],\n",
            "        [ 17, 143,  22,  18],\n",
            "        [  8,  77, 100,  15],\n",
            "        [  1,  28,  12, 159]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.000859 \n",
            "\n",
            "f1 score is: 0.7443833947181702\n",
            " the confusion matrix is:\n",
            "tensor([[168,  21,  11,   0],\n",
            "        [ 15, 123,  34,  28],\n",
            "        [  5,  24, 140,  31],\n",
            "        [  1,  17,  17, 165]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.000972 \n",
            "\n",
            "f1 score is: 0.7380727529525757\n",
            " the confusion matrix is:\n",
            "tensor([[171,  23,   6,   0],\n",
            "        [ 14, 119,  54,  13],\n",
            "        [  3,  23, 162,  12],\n",
            "        [  2,  16,  45, 137]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.000944 \n",
            "\n",
            "f1 score is: 0.7190558314323425\n",
            " the confusion matrix is:\n",
            "tensor([[185,  14,   0,   1],\n",
            "        [ 34,  95,  45,  26],\n",
            "        [  8,  34, 143,  15],\n",
            "        [  0,  14,  28, 158]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 0.001343 \n",
            "\n",
            "f1 score is: 0.630027174949646\n",
            " the confusion matrix is:\n",
            "tensor([[170,  21,   9,   0],\n",
            "        [ 35,  67,  87,  11],\n",
            "        [  6,  13, 169,  12],\n",
            "        [  3,  13,  77, 107]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.001217 \n",
            "\n",
            "f1 score is: 0.6989243626594543\n",
            " the confusion matrix is:\n",
            "tensor([[165,  18,  17,   0],\n",
            "        [ 22,  92,  66,  20],\n",
            "        [  8,  20, 161,  11],\n",
            "        [  0,  17,  40, 143]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 75.5%, Avg loss: 0.000896 \n",
            "\n",
            "f1 score is: 0.7539570331573486\n",
            " the confusion matrix is:\n",
            "tensor([[179,  18,   2,   1],\n",
            "        [ 25, 137,  17,  21],\n",
            "        [ 10,  38, 127,  25],\n",
            "        [  0,  27,  12, 161]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.001107 \n",
            "\n",
            "f1 score is: 0.7044338583946228\n",
            " the confusion matrix is:\n",
            "tensor([[153,  40,   7,   0],\n",
            "        [ 15, 124,  32,  29],\n",
            "        [  8,  44, 126,  22],\n",
            "        [  0,  22,  20, 158]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.001180 \n",
            "\n",
            "f1 score is: 0.7416977882385254\n",
            " the confusion matrix is:\n",
            "tensor([[165,  24,  11,   0],\n",
            "        [ 13, 128,  41,  18],\n",
            "        [  7,  38, 140,  15],\n",
            "        [  0,  25,  17, 158]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.001205 \n",
            "\n",
            "f1 score is: 0.7029880285263062\n",
            " the confusion matrix is:\n",
            "tensor([[157,  24,  11,   8],\n",
            "        [ 23, 100,  55,  22],\n",
            "        [ 10,  22, 157,  11],\n",
            "        [  0,  18,  32, 150]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 0.001328 \n",
            "\n",
            "f1 score is: 0.6773581504821777\n",
            " the confusion matrix is:\n",
            "tensor([[150,  34,  14,   2],\n",
            "        [ 15, 106,  29,  50],\n",
            "        [  6,  44, 117,  33],\n",
            "        [  0,  22,   9, 169]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.001861 \n",
            "\n",
            "f1 score is: 0.6698001027107239\n",
            " the confusion matrix is:\n",
            "tensor([[138,  34,  25,   3],\n",
            "        [ 11,  83,  79,  27],\n",
            "        [  4,  24, 155,  17],\n",
            "        [  0,  12,  27, 161]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.2%, Avg loss: 0.001417 \n",
            "\n",
            "f1 score is: 0.6838322877883911\n",
            " the confusion matrix is:\n",
            "tensor([[170,  14,  16,   0],\n",
            "        [ 17, 114,  66,   3],\n",
            "        [  3,  28, 164,   5],\n",
            "        [  0,  33,  69,  98]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.001780 \n",
            "\n",
            "f1 score is: 0.7424224615097046\n",
            " the confusion matrix is:\n",
            "tensor([[164,  20,  15,   1],\n",
            "        [ 12, 121,  43,  24],\n",
            "        [  3,  37, 134,  26],\n",
            "        [  0,  17,   8, 175]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.001357 \n",
            "\n",
            "f1 score is: 0.7137404680252075\n",
            " the confusion matrix is:\n",
            "tensor([[171,  17,  10,   2],\n",
            "        [ 22, 118,  50,  10],\n",
            "        [  4,  48, 136,  12],\n",
            "        [  0,  28,  29, 143]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.001367 \n",
            "\n",
            "f1 score is: 0.7196134328842163\n",
            " the confusion matrix is:\n",
            "tensor([[152,  31,  17,   0],\n",
            "        [ 19, 111,  54,  16],\n",
            "        [  5,  23, 150,  22],\n",
            "        [  0,  19,  19, 162]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.001623 \n",
            "\n",
            "f1 score is: 0.7287153601646423\n",
            " the confusion matrix is:\n",
            "tensor([[164,   9,  27,   0],\n",
            "        [ 22, 100,  69,   9],\n",
            "        [  3,  17, 169,  11],\n",
            "        [  0,  20,  30, 150]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.001806 \n",
            "\n",
            "f1 score is: 0.732012152671814\n",
            " the confusion matrix is:\n",
            "tensor([[157,  27,  16,   0],\n",
            "        [ 18, 123,  51,   8],\n",
            "        [  3,  31, 152,  14],\n",
            "        [  0,  29,  21, 150]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.001755 \n",
            "\n",
            "f1 score is: 0.691256046295166\n",
            " the confusion matrix is:\n",
            "tensor([[188,   9,   3,   0],\n",
            "        [ 45,  75,  69,  11],\n",
            "        [ 14,  20, 157,   9],\n",
            "        [  2,  15,  40, 143]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.002078 \n",
            "\n",
            "f1 score is: 0.7218462228775024\n",
            " the confusion matrix is:\n",
            "tensor([[173,  20,   7,   0],\n",
            "        [ 29, 115,  41,  15],\n",
            "        [ 11,  39, 132,  18],\n",
            "        [  0,  25,  17, 158]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.001713 \n",
            "\n",
            "f1 score is: 0.7469332218170166\n",
            " the confusion matrix is:\n",
            "tensor([[181,  11,   7,   1],\n",
            "        [ 24, 114,  49,  13],\n",
            "        [  6,  27, 151,  16],\n",
            "        [  2,  28,  17, 153]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.002250 \n",
            "\n",
            "f1 score is: 0.6611050367355347\n",
            " the confusion matrix is:\n",
            "tensor([[171,  13,  14,   2],\n",
            "        [ 26,  50,  73,  51],\n",
            "        [  6,  11, 151,  32],\n",
            "        [  0,   8,  13, 179]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.001923 \n",
            "\n",
            "f1 score is: 0.731550931930542\n",
            " the confusion matrix is:\n",
            "tensor([[152,  28,  19,   1],\n",
            "        [ 11, 126,  52,  11],\n",
            "        [  3,  32, 150,  15],\n",
            "        [  0,  25,  22, 153]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.002457 \n",
            "\n",
            "f1 score is: 0.6503884792327881\n",
            " the confusion matrix is:\n",
            "tensor([[186,   9,   1,   4],\n",
            "        [ 29,  70,  28,  73],\n",
            "        [ 15,  33, 101,  51],\n",
            "        [  3,   9,   8, 180]])\n",
            "best epoch is: 12\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 0.001605 \n",
            "\n",
            "f1 score is: 0.6476108431816101\n",
            " the confusion matrix is:\n",
            "tensor([[273,  14,   3,   7],\n",
            "        [ 37, 138,  55,  94],\n",
            "        [ 37,  69, 169, 124],\n",
            "        [  8,   5,  14, 329]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 26.1%, Avg loss: 0.001720 \n",
            "\n",
            "f1 score is: 0.12243928015232086\n",
            " the confusion matrix is:\n",
            "tensor([[  0,   0,   0, 200],\n",
            "        [  0,   0,   0, 200],\n",
            "        [  0,   0,   9, 191],\n",
            "        [  0,   0,   0, 200]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 0.001423 \n",
            "\n",
            "f1 score is: 0.4042069911956787\n",
            " the confusion matrix is:\n",
            "tensor([[113,   5,  80,   2],\n",
            "        [ 20,   7, 165,   8],\n",
            "        [  1,   1, 192,   6],\n",
            "        [  2,   0, 146,  52]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.0%, Avg loss: 0.001199 \n",
            "\n",
            "f1 score is: 0.5880553126335144\n",
            " the confusion matrix is:\n",
            "tensor([[135,  53,  12,   0],\n",
            "        [ 47,  48,  75,  30],\n",
            "        [  3,  30, 146,  21],\n",
            "        [  2,  11,  36, 151]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.5%, Avg loss: 0.000965 \n",
            "\n",
            "f1 score is: 0.7048205137252808\n",
            " the confusion matrix is:\n",
            "tensor([[172,  13,  15,   0],\n",
            "        [ 36,  90,  30,  44],\n",
            "        [  5,  28, 131,  36],\n",
            "        [  0,  16,   5, 179]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.001054 \n",
            "\n",
            "f1 score is: 0.661220908164978\n",
            " the confusion matrix is:\n",
            "tensor([[165,  24,   9,   2],\n",
            "        [ 32,  78,  20,  70],\n",
            "        [  7,  25, 109,  59],\n",
            "        [  0,   9,   3, 188]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.000943 \n",
            "\n",
            "f1 score is: 0.7173929214477539\n",
            " the confusion matrix is:\n",
            "tensor([[179,   5,  15,   1],\n",
            "        [ 33,  79,  55,  33],\n",
            "        [  5,  13, 166,  16],\n",
            "        [  1,  18,  20, 161]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.9%, Avg loss: 0.001073 \n",
            "\n",
            "f1 score is: 0.6643700003623962\n",
            " the confusion matrix is:\n",
            "tensor([[175,  19,   6,   0],\n",
            "        [ 49,  74,  34,  43],\n",
            "        [  7,  44, 118,  31],\n",
            "        [  3,  15,   6, 176]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.001188 \n",
            "\n",
            "f1 score is: 0.6413630247116089\n",
            " the confusion matrix is:\n",
            "tensor([[140,  52,   4,   4],\n",
            "        [ 10,  98,  13,  79],\n",
            "        [  1,  53,  90,  56],\n",
            "        [  0,  12,   1, 187]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 0.000909 \n",
            "\n",
            "f1 score is: 0.7278889417648315\n",
            " the confusion matrix is:\n",
            "tensor([[169,  22,   9,   0],\n",
            "        [ 21, 108,  46,  25],\n",
            "        [  5,  32, 140,  23],\n",
            "        [  0,  16,  17, 167]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 0.000961 \n",
            "\n",
            "f1 score is: 0.6929327249526978\n",
            " the confusion matrix is:\n",
            "tensor([[145,  46,   7,   2],\n",
            "        [ 13, 143,  14,  30],\n",
            "        [  4,  80,  95,  21],\n",
            "        [  0,  26,   5, 169]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.000908 \n",
            "\n",
            "f1 score is: 0.7233391404151917\n",
            " the confusion matrix is:\n",
            "tensor([[160,  30,  10,   0],\n",
            "        [ 15, 127,  36,  22],\n",
            "        [  2,  52, 126,  20],\n",
            "        [  0,  19,  18, 163]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.4%, Avg loss: 0.001048 \n",
            "\n",
            "f1 score is: 0.687591552734375\n",
            " the confusion matrix is:\n",
            "tensor([[159,  37,   4,   0],\n",
            "        [ 14, 153,  19,  14],\n",
            "        [  7,  83,  91,  19],\n",
            "        [  0,  43,  13, 144]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.001022 \n",
            "\n",
            "f1 score is: 0.7304764986038208\n",
            " the confusion matrix is:\n",
            "tensor([[163,  19,  16,   2],\n",
            "        [ 15, 117,  23,  45],\n",
            "        [  7,  43, 128,  22],\n",
            "        [  0,  16,   6, 178]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.000911 \n",
            "\n",
            "f1 score is: 0.718604326248169\n",
            " the confusion matrix is:\n",
            "tensor([[174,  24,   2,   0],\n",
            "        [ 23, 125,  23,  29],\n",
            "        [  6,  60, 112,  22],\n",
            "        [  1,  27,   8, 164]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.001063 \n",
            "\n",
            "f1 score is: 0.7301409244537354\n",
            " the confusion matrix is:\n",
            "tensor([[182,  12,   5,   1],\n",
            "        [ 26, 113,  39,  22],\n",
            "        [ 11,  46, 126,  17],\n",
            "        [  2,  22,  10, 166]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.001055 \n",
            "\n",
            "f1 score is: 0.6882038116455078\n",
            " the confusion matrix is:\n",
            "tensor([[172,  16,   7,   5],\n",
            "        [ 30,  92,  33,  45],\n",
            "        [  7,  43, 116,  34],\n",
            "        [  1,  14,   7, 178]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 67.9%, Avg loss: 0.001229 \n",
            "\n",
            "f1 score is: 0.6734468936920166\n",
            " the confusion matrix is:\n",
            "tensor([[150,  30,  15,   5],\n",
            "        [ 27,  86,  47,  40],\n",
            "        [  4,  40, 135,  21],\n",
            "        [  1,  13,  14, 172]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.001054 \n",
            "\n",
            "f1 score is: 0.7196826934814453\n",
            " the confusion matrix is:\n",
            "tensor([[171,  11,  16,   2],\n",
            "        [ 23, 108,  47,  22],\n",
            "        [  8,  31, 145,  16],\n",
            "        [  2,  25,  20, 153]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 0.001302 \n",
            "\n",
            "f1 score is: 0.7316814661026001\n",
            " the confusion matrix is:\n",
            "tensor([[173,  20,   4,   3],\n",
            "        [ 20, 136,  21,  23],\n",
            "        [  5,  52, 122,  21],\n",
            "        [  1,  33,  13, 153]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 0.001322 \n",
            "\n",
            "f1 score is: 0.7043196558952332\n",
            " the confusion matrix is:\n",
            "tensor([[172,  15,  12,   1],\n",
            "        [ 21, 109,  42,  28],\n",
            "        [  7,  51, 120,  22],\n",
            "        [  1,  21,  14, 164]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.001503 \n",
            "\n",
            "f1 score is: 0.714958131313324\n",
            " the confusion matrix is:\n",
            "tensor([[188,   6,   4,   2],\n",
            "        [ 29, 118,  29,  24],\n",
            "        [ 14,  57, 112,  17],\n",
            "        [  5,  27,  10, 158]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.001166 \n",
            "\n",
            "f1 score is: 0.6986944675445557\n",
            " the confusion matrix is:\n",
            "tensor([[152,  32,  15,   1],\n",
            "        [ 13, 127,  35,  25],\n",
            "        [  5,  57, 121,  17],\n",
            "        [  1,  25,  19, 155]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.0%, Avg loss: 0.002287 \n",
            "\n",
            "f1 score is: 0.6565658450126648\n",
            " the confusion matrix is:\n",
            "tensor([[121,  60,  18,   1],\n",
            "        [ 11, 111,  54,  24],\n",
            "        [  3,  37, 147,  13],\n",
            "        [  0,  15,  44, 141]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.001520 \n",
            "\n",
            "f1 score is: 0.7101876735687256\n",
            " the confusion matrix is:\n",
            "tensor([[178,   6,  15,   1],\n",
            "        [ 24,  86,  52,  38],\n",
            "        [  8,  27, 137,  28],\n",
            "        [  1,   7,  16, 176]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.001708 \n",
            "\n",
            "f1 score is: 0.7075272798538208\n",
            " the confusion matrix is:\n",
            "tensor([[154,  28,  16,   2],\n",
            "        [ 18, 107,  45,  30],\n",
            "        [  9,  31, 137,  23],\n",
            "        [  0,  21,  10, 169]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.001493 \n",
            "\n",
            "f1 score is: 0.6606259346008301\n",
            " the confusion matrix is:\n",
            "tensor([[164,  27,   8,   1],\n",
            "        [ 18,  96,  59,  27],\n",
            "        [  9,  51, 125,  15],\n",
            "        [  3,  16,  39, 142]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.001433 \n",
            "\n",
            "f1 score is: 0.7070123553276062\n",
            " the confusion matrix is:\n",
            "tensor([[160,  16,  23,   1],\n",
            "        [ 15,  94,  69,  22],\n",
            "        [  8,  17, 158,  17],\n",
            "        [  1,  13,  31, 155]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.002008 \n",
            "\n",
            "f1 score is: 0.7155968546867371\n",
            " the confusion matrix is:\n",
            "tensor([[170,   9,  17,   4],\n",
            "        [ 21, 105,  41,  33],\n",
            "        [  7,  36, 135,  22],\n",
            "        [  0,  24,  11, 165]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.6%, Avg loss: 0.002432 \n",
            "\n",
            "f1 score is: 0.7149286866188049\n",
            " the confusion matrix is:\n",
            "tensor([[169,  19,  11,   1],\n",
            "        [ 21, 116,  29,  34],\n",
            "        [  5,  43, 122,  30],\n",
            "        [  0,  26,   8, 166]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.001505 \n",
            "\n",
            "f1 score is: 0.7100284695625305\n",
            " the confusion matrix is:\n",
            "tensor([[156,  33,  10,   1],\n",
            "        [ 11, 110,  52,  27],\n",
            "        [  4,  32, 145,  19],\n",
            "        [  0,  18,  27, 155]])\n",
            "best epoch is: 18\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.001036 \n",
            "\n",
            "f1 score is: 0.711538553237915\n",
            " the confusion matrix is:\n",
            "tensor([[248,  39,   8,   2],\n",
            "        [ 13, 194,  89,  28],\n",
            "        [  9, 101, 247,  42],\n",
            "        [  3,  45,  31, 277]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.001251 \n",
            "\n",
            "f1 score is: 0.4414176940917969\n",
            " the confusion matrix is:\n",
            "tensor([[140,  57,   0,   3],\n",
            "        [ 36, 100,   0,  64],\n",
            "        [  4,  42,   0, 154],\n",
            "        [  7,  25,   0, 168]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.5%, Avg loss: 0.001235 \n",
            "\n",
            "f1 score is: 0.48133596777915955\n",
            " the confusion matrix is:\n",
            "tensor([[129,  66,   4,   1],\n",
            "        [ 20, 102,   5,  73],\n",
            "        [  0,  36,  14, 150],\n",
            "        [  1,  21,   3, 175]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.2%, Avg loss: 0.001167 \n",
            "\n",
            "f1 score is: 0.5156546831130981\n",
            " the confusion matrix is:\n",
            "tensor([[148,  49,   3,   0],\n",
            "        [ 29,  99,  13,  59],\n",
            "        [  2,  36,  24, 138],\n",
            "        [  2,  22,   5, 171]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.2%, Avg loss: 0.001150 \n",
            "\n",
            "f1 score is: 0.5251365900039673\n",
            " the confusion matrix is:\n",
            "tensor([[145,  53,   2,   0],\n",
            "        [ 25, 120,  12,  43],\n",
            "        [  2,  53,  22, 123],\n",
            "        [  2,  30,   5, 163]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.6%, Avg loss: 0.001358 \n",
            "\n",
            "f1 score is: 0.5225296020507812\n",
            " the confusion matrix is:\n",
            "tensor([[140,  57,   3,   0],\n",
            "        [ 20, 109,  13,  58],\n",
            "        [  2,  39,  25, 134],\n",
            "        [  2,  23,   4, 171]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.5%, Avg loss: 0.001143 \n",
            "\n",
            "f1 score is: 0.5362565517425537\n",
            " the confusion matrix is:\n",
            "tensor([[144,  53,   3,   0],\n",
            "        [ 23, 108,  24,  45],\n",
            "        [  2,  40,  31, 127],\n",
            "        [  2,  24,   5, 169]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.6%, Avg loss: 0.002466 \n",
            "\n",
            "f1 score is: 0.5389825105667114\n",
            " the confusion matrix is:\n",
            "tensor([[155,  43,   2,   0],\n",
            "        [ 25, 114,  16,  45],\n",
            "        [  6,  46,  25, 123],\n",
            "        [  2,  27,   4, 167]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.6%, Avg loss: 0.003741 \n",
            "\n",
            "f1 score is: 0.5145717859268188\n",
            " the confusion matrix is:\n",
            "tensor([[146,  51,   3,   0],\n",
            "        [ 26, 110,  13,  51],\n",
            "        [  3,  46,  19, 132],\n",
            "        [  2,  25,   3, 170]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.6%, Avg loss: 0.003432 \n",
            "\n",
            "f1 score is: 0.5322545170783997\n",
            " the confusion matrix is:\n",
            "tensor([[145,  53,   2,   0],\n",
            "        [ 24, 114,  17,  45],\n",
            "        [  4,  47,  26, 123],\n",
            "        [  0,  28,   4, 168]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.2%, Avg loss: 0.003919 \n",
            "\n",
            "f1 score is: 0.5413715243339539\n",
            " the confusion matrix is:\n",
            "tensor([[144,  54,   2,   0],\n",
            "        [ 24, 120,  13,  43],\n",
            "        [  3,  51,  29, 117],\n",
            "        [  0,  29,   6, 165]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.010213 \n",
            "\n",
            "f1 score is: 0.5688152313232422\n",
            " the confusion matrix is:\n",
            "tensor([[148,  49,   3,   0],\n",
            "        [ 24, 110,  28,  38],\n",
            "        [  5,  42,  52, 101],\n",
            "        [  1,  26,  17, 156]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.5%, Avg loss: 0.018686 \n",
            "\n",
            "f1 score is: 0.5733610391616821\n",
            " the confusion matrix is:\n",
            "tensor([[154,  44,   2,   0],\n",
            "        [ 25, 115,  21,  39],\n",
            "        [  6,  46,  44, 104],\n",
            "        [  1,  27,   9, 163]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.8%, Avg loss: 0.022373 \n",
            "\n",
            "f1 score is: 0.5885927677154541\n",
            " the confusion matrix is:\n",
            "tensor([[161,  37,   2,   0],\n",
            "        [ 25, 118,  20,  37],\n",
            "        [  6,  47,  49,  98],\n",
            "        [  1,  26,  15, 158]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 0.028352 \n",
            "\n",
            "f1 score is: 0.5860828757286072\n",
            " the confusion matrix is:\n",
            "tensor([[154,  44,   2,   0],\n",
            "        [ 21, 109,  27,  43],\n",
            "        [  6,  38,  49, 107],\n",
            "        [  0,  19,   8, 173]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 0.038081 \n",
            "\n",
            "f1 score is: 0.5899003744125366\n",
            " the confusion matrix is:\n",
            "tensor([[152,  46,   2,   0],\n",
            "        [ 21, 116,  22,  41],\n",
            "        [  5,  44,  52,  99],\n",
            "        [  0,  23,  12, 165]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.045295 \n",
            "\n",
            "f1 score is: 0.5963669419288635\n",
            " the confusion matrix is:\n",
            "tensor([[161,  37,   2,   0],\n",
            "        [ 24, 116,  20,  40],\n",
            "        [  6,  44,  50, 100],\n",
            "        [  0,  22,  12, 166]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.5%, Avg loss: 0.053103 \n",
            "\n",
            "f1 score is: 0.5923742055892944\n",
            " the confusion matrix is:\n",
            "tensor([[160,  38,   2,   0],\n",
            "        [ 23, 122,  17,  38],\n",
            "        [  6,  51,  45,  98],\n",
            "        [  0,  24,  11, 165]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 0.061008 \n",
            "\n",
            "f1 score is: 0.6064997911453247\n",
            " the confusion matrix is:\n",
            "tensor([[158,  40,   2,   0],\n",
            "        [ 20, 118,  23,  39],\n",
            "        [  6,  44,  57,  93],\n",
            "        [  0,  20,  16, 164]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.065353 \n",
            "\n",
            "f1 score is: 0.6215391755104065\n",
            " the confusion matrix is:\n",
            "tensor([[158,  40,   2,   0],\n",
            "        [ 20, 113,  32,  35],\n",
            "        [  6,  39,  76,  79],\n",
            "        [  0,  19,  26, 155]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.069353 \n",
            "\n",
            "f1 score is: 0.623002290725708\n",
            " the confusion matrix is:\n",
            "tensor([[165,  33,   2,   0],\n",
            "        [ 22, 119,  24,  35],\n",
            "        [  7,  45,  67,  81],\n",
            "        [  0,  21,  23, 156]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 0.067505 \n",
            "\n",
            "f1 score is: 0.6211704015731812\n",
            " the confusion matrix is:\n",
            "tensor([[163,  35,   2,   0],\n",
            "        [ 20, 115,  26,  39],\n",
            "        [  6,  41,  66,  87],\n",
            "        [  0,  19,  19, 162]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 0.074205 \n",
            "\n",
            "f1 score is: 0.6160979866981506\n",
            " the confusion matrix is:\n",
            "tensor([[165,  33,   2,   0],\n",
            "        [ 21, 124,  21,  34],\n",
            "        [  7,  54,  59,  80],\n",
            "        [  0,  23,  21, 156]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.072888 \n",
            "\n",
            "f1 score is: 0.6226538419723511\n",
            " the confusion matrix is:\n",
            "tensor([[162,  36,   2,   0],\n",
            "        [ 20, 119,  22,  39],\n",
            "        [  7,  43,  66,  84],\n",
            "        [  0,  20,  20, 160]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.6%, Avg loss: 0.079279 \n",
            "\n",
            "f1 score is: 0.6231504678726196\n",
            " the confusion matrix is:\n",
            "tensor([[165,  33,   2,   0],\n",
            "        [ 23, 119,  20,  38],\n",
            "        [  7,  46,  64,  83],\n",
            "        [  0,  22,  17, 161]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.9%, Avg loss: 0.082992 \n",
            "\n",
            "f1 score is: 0.6279535293579102\n",
            " the confusion matrix is:\n",
            "tensor([[166,  32,   2,   0],\n",
            "        [ 23, 115,  23,  39],\n",
            "        [  7,  42,  70,  81],\n",
            "        [  0,  19,  21, 160]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.2%, Avg loss: 0.087294 \n",
            "\n",
            "f1 score is: 0.6347154974937439\n",
            " the confusion matrix is:\n",
            "tensor([[165,  33,   2,   0],\n",
            "        [ 22, 120,  28,  30],\n",
            "        [  7,  45,  75,  73],\n",
            "        [  0,  21,  25, 154]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.2%, Avg loss: 0.085895 \n",
            "\n",
            "f1 score is: 0.6315877437591553\n",
            " the confusion matrix is:\n",
            "tensor([[165,  33,   2,   0],\n",
            "        [ 22, 117,  24,  37],\n",
            "        [  7,  42,  70,  81],\n",
            "        [  0,  19,  19, 162]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 0.087757 \n",
            "\n",
            "f1 score is: 0.6400965452194214\n",
            " the confusion matrix is:\n",
            "tensor([[166,  32,   2,   0],\n",
            "        [ 20, 119,  30,  31],\n",
            "        [  7,  43,  77,  73],\n",
            "        [  0,  20,  24, 156]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 0.088669 \n",
            "\n",
            "f1 score is: 0.6405560374259949\n",
            " the confusion matrix is:\n",
            "tensor([[166,  32,   2,   0],\n",
            "        [ 20, 112,  34,  34],\n",
            "        [  7,  38,  81,  74],\n",
            "        [  0,  16,  25, 159]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 0.088812 \n",
            "\n",
            "f1 score is: 0.6257683038711548\n",
            " the confusion matrix is:\n",
            "tensor([[162,  36,   2,   0],\n",
            "        [ 20, 115,  25,  40],\n",
            "        [  6,  40,  67,  87],\n",
            "        [  0,  17,  17, 166]])\n",
            "best epoch is: 28\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.011387 \n",
            "\n",
            "f1 score is: 0.6215466260910034\n",
            " the confusion matrix is:\n",
            "tensor([[262,  34,   1,   0],\n",
            "        [ 25, 178,  62,  59],\n",
            "        [  9, 136, 125, 129],\n",
            "        [  7,  30,  27, 292]])\n",
            "current epoch: 0\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 0.001723 \n",
            "\n",
            "f1 score is: 0.22505654394626617\n",
            " the confusion matrix is:\n",
            "tensor([[140,   0,   0,  60],\n",
            "        [ 79,   0,   0, 121],\n",
            "        [ 24,   0,   0, 176],\n",
            "        [ 77,   0,   0, 123]])\n",
            "current epoch: 1\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.8%, Avg loss: 0.001708 \n",
            "\n",
            "f1 score is: 0.24440497159957886\n",
            " the confusion matrix is:\n",
            "tensor([[173,   0,   0,  27],\n",
            "        [110,   0,   0,  90],\n",
            "        [ 47,   0,   0, 153],\n",
            "        [ 79,   0,   0, 121]])\n",
            "current epoch: 2\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 41.0%, Avg loss: 0.001654 \n",
            "\n",
            "f1 score is: 0.28062257170677185\n",
            " the confusion matrix is:\n",
            "tensor([[156,   0,   0,  44],\n",
            "        [ 81,   0,   0, 119],\n",
            "        [ 23,   0,   0, 177],\n",
            "        [ 28,   0,   0, 172]])\n",
            "current epoch: 3\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.1%, Avg loss: 0.001565 \n",
            "\n",
            "f1 score is: 0.2555573880672455\n",
            " the confusion matrix is:\n",
            "tensor([[ 92,   4,   0, 104],\n",
            "        [ 17,   0,   0, 183],\n",
            "        [ 10,   0,   0, 190],\n",
            "        [  3,   0,   0, 197]])\n",
            "current epoch: 4\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 41.8%, Avg loss: 0.001512 \n",
            "\n",
            "f1 score is: 0.3595614433288574\n",
            " the confusion matrix is:\n",
            "tensor([[ 84,  78,   0,  38],\n",
            "        [ 12,  80,   0, 108],\n",
            "        [  4,  37,   0, 159],\n",
            "        [  2,  28,   0, 170]])\n",
            "current epoch: 5\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 48.0%, Avg loss: 0.001431 \n",
            "\n",
            "f1 score is: 0.455716073513031\n",
            " the confusion matrix is:\n",
            "tensor([[165,  27,   1,   7],\n",
            "        [ 72,  80,  12,  36],\n",
            "        [ 25,  72,  39,  64],\n",
            "        [ 28,  52,  20, 100]])\n",
            "current epoch: 6\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.5%, Avg loss: 0.001465 \n",
            "\n",
            "f1 score is: 0.33472558856010437\n",
            " the confusion matrix is:\n",
            "tensor([[ 83,  82,   0,  35],\n",
            "        [ 19,  57,   0, 124],\n",
            "        [  1,  25,   0, 174],\n",
            "        [  0,  23,   1, 176]])\n",
            "current epoch: 7\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.2%, Avg loss: 0.001368 \n",
            "\n",
            "f1 score is: 0.4679552912712097\n",
            " the confusion matrix is:\n",
            "tensor([[134,  60,   1,   5],\n",
            "        [ 37, 108,  15,  40],\n",
            "        [  8,  66,  25, 101],\n",
            "        [  7,  52,  14, 127]])\n",
            "current epoch: 8\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 0.001322 \n",
            "\n",
            "f1 score is: 0.4688746929168701\n",
            " the confusion matrix is:\n",
            "tensor([[161,  34,   2,   3],\n",
            "        [ 68,  74,  20,  38],\n",
            "        [ 13,  57,  38,  92],\n",
            "        [ 21,  37,  19, 123]])\n",
            "current epoch: 9\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.9%, Avg loss: 0.001300 \n",
            "\n",
            "f1 score is: 0.5050971508026123\n",
            " the confusion matrix is:\n",
            "tensor([[157,  30,   2,  11],\n",
            "        [ 60,  69,  24,  47],\n",
            "        [ 11,  46,  66,  77],\n",
            "        [ 17,  29,  31, 123]])\n",
            "current epoch: 10\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 52.5%, Avg loss: 0.001280 \n",
            "\n",
            "f1 score is: 0.5210103392601013\n",
            " the confusion matrix is:\n",
            "tensor([[149,  46,   2,   3],\n",
            "        [ 53,  71,  36,  40],\n",
            "        [  9,  44,  99,  48],\n",
            "        [ 15,  29,  55, 101]])\n",
            "current epoch: 11\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.001274 \n",
            "\n",
            "f1 score is: 0.49617016315460205\n",
            " the confusion matrix is:\n",
            "tensor([[144,  48,   2,   6],\n",
            "        [ 47,  79,  22,  52],\n",
            "        [  3,  60,  47,  90],\n",
            "        [  6,  37,  19, 138]])\n",
            "current epoch: 12\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 0.001288 \n",
            "\n",
            "f1 score is: 0.5307440161705017\n",
            " the confusion matrix is:\n",
            "tensor([[132,  47,   5,  16],\n",
            "        [ 28,  54,  57,  61],\n",
            "        [  1,  31, 122,  46],\n",
            "        [  2,  19,  60, 119]])\n",
            "current epoch: 13\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.5%, Avg loss: 0.001225 \n",
            "\n",
            "f1 score is: 0.5439910888671875\n",
            " the confusion matrix is:\n",
            "tensor([[160,  31,   2,   7],\n",
            "        [ 54,  58,  40,  48],\n",
            "        [ 10,  37, 104,  49],\n",
            "        [  8,  25,  45, 122]])\n",
            "current epoch: 14\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 0.001244 \n",
            "\n",
            "f1 score is: 0.5422172546386719\n",
            " the confusion matrix is:\n",
            "tensor([[161,  35,   2,   2],\n",
            "        [ 54,  74,  44,  28],\n",
            "        [  8,  58, 115,  19],\n",
            "        [ 12,  45,  56,  87]])\n",
            "current epoch: 15\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 0.001300 \n",
            "\n",
            "f1 score is: 0.5126311779022217\n",
            " the confusion matrix is:\n",
            "tensor([[128,  40,  11,  21],\n",
            "        [ 27,  31,  63,  79],\n",
            "        [  2,  16, 111,  71],\n",
            "        [  0,   8,  35, 157]])\n",
            "current epoch: 16\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 56.8%, Avg loss: 0.001208 \n",
            "\n",
            "f1 score is: 0.5488564372062683\n",
            " the confusion matrix is:\n",
            "tensor([[165,  22,   1,  12],\n",
            "        [ 52,  77,  12,  59],\n",
            "        [  9,  64,  60,  67],\n",
            "        [  9,  28,  11, 152]])\n",
            "current epoch: 17\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.5%, Avg loss: 0.001228 \n",
            "\n",
            "f1 score is: 0.5694356560707092\n",
            " the confusion matrix is:\n",
            "tensor([[143,  37,   1,  19],\n",
            "        [ 34,  78,  17,  71],\n",
            "        [  2,  53,  81,  64],\n",
            "        [  2,  20,  20, 158]])\n",
            "current epoch: 18\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 0.001240 \n",
            "\n",
            "f1 score is: 0.5344127416610718\n",
            " the confusion matrix is:\n",
            "tensor([[137,  30,  18,  15],\n",
            "        [ 31,  31,  84,  54],\n",
            "        [  2,  10, 147,  41],\n",
            "        [  0,   8,  60, 132]])\n",
            "current epoch: 19\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.6%, Avg loss: 0.001195 \n",
            "\n",
            "f1 score is: 0.5653082728385925\n",
            " the confusion matrix is:\n",
            "tensor([[141,  35,   5,  19],\n",
            "        [ 31,  52,  46,  71],\n",
            "        [  2,  27, 116,  55],\n",
            "        [  1,  15,  32, 152]])\n",
            "current epoch: 20\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 59.4%, Avg loss: 0.001151 \n",
            "\n",
            "f1 score is: 0.5798313021659851\n",
            " the confusion matrix is:\n",
            "tensor([[161,  21,  11,   7],\n",
            "        [ 37,  53,  63,  47],\n",
            "        [  7,  25, 130,  38],\n",
            "        [  2,  20,  47, 131]])\n",
            "current epoch: 21\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.001116 \n",
            "\n",
            "f1 score is: 0.6209767460823059\n",
            " the confusion matrix is:\n",
            "tensor([[163,  25,   1,  11],\n",
            "        [ 39,  94,  17,  50],\n",
            "        [  6,  55,  95,  44],\n",
            "        [  2,  29,  21, 148]])\n",
            "current epoch: 22\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.4%, Avg loss: 0.001140 \n",
            "\n",
            "f1 score is: 0.5985360741615295\n",
            " the confusion matrix is:\n",
            "tensor([[155,  30,  10,   5],\n",
            "        [ 32,  70,  61,  37],\n",
            "        [  4,  25, 138,  33],\n",
            "        [  1,  22,  57, 120]])\n",
            "current epoch: 23\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 0.001097 \n",
            "\n",
            "f1 score is: 0.6226044297218323\n",
            " the confusion matrix is:\n",
            "tensor([[165,  30,   1,   4],\n",
            "        [ 39, 105,  23,  33],\n",
            "        [  5,  61,  96,  38],\n",
            "        [  4,  36,  27, 133]])\n",
            "current epoch: 24\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.001083 \n",
            "\n",
            "f1 score is: 0.6263793110847473\n",
            " the confusion matrix is:\n",
            "tensor([[164,  25,   6,   5],\n",
            "        [ 35,  81,  44,  40],\n",
            "        [  4,  36, 121,  39],\n",
            "        [  2,  23,  36, 139]])\n",
            "current epoch: 25\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 0.001099 \n",
            "\n",
            "f1 score is: 0.620745062828064\n",
            " the confusion matrix is:\n",
            "tensor([[168,  23,   7,   2],\n",
            "        [ 39,  96,  37,  28],\n",
            "        [  7,  46, 118,  29],\n",
            "        [  2,  33,  49, 116]])\n",
            "current epoch: 26\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 0.001129 \n",
            "\n",
            "f1 score is: 0.6075420379638672\n",
            " the confusion matrix is:\n",
            "tensor([[140,  44,  11,   5],\n",
            "        [ 23,  76,  66,  35],\n",
            "        [  2,  21, 146,  31],\n",
            "        [  0,  18,  57, 125]])\n",
            "current epoch: 27\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 0.001055 \n",
            "\n",
            "f1 score is: 0.6450382471084595\n",
            " the confusion matrix is:\n",
            "tensor([[172,  25,   0,   3],\n",
            "        [ 39, 110,  16,  35],\n",
            "        [  4,  62,  96,  38],\n",
            "        [  3,  35,  22, 140]])\n",
            "current epoch: 28\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.001057 \n",
            "\n",
            "f1 score is: 0.6217449307441711\n",
            " the confusion matrix is:\n",
            "tensor([[182,   6,   7,   5],\n",
            "        [ 47,  61,  53,  39],\n",
            "        [ 11,  23, 134,  32],\n",
            "        [  3,  21,  41, 135]])\n",
            "current epoch: 29\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.001086 \n",
            "\n",
            "f1 score is: 0.6413341164588928\n",
            " the confusion matrix is:\n",
            "tensor([[177,  18,   2,   3],\n",
            "        [ 44, 113,  29,  14],\n",
            "        [  7,  54, 116,  23],\n",
            "        [  3,  41,  47, 109]])\n",
            "best epoch is: 27\n",
            "TESTING THE BEST EPOCH\n",
            "\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.000626 \n",
            "\n",
            "f1 score is: 0.648705244064331\n",
            " the confusion matrix is:\n",
            "tensor([[264,  24,   4,   5],\n",
            "        [ 30, 202,  74,  18],\n",
            "        [ 24, 139, 199,  37],\n",
            "        [ 16,  66,  59, 215]])\n",
            "Algorithm: Adadelta, accuracy is 48.11046511627907 and f1 score is0.4658815860748291\n",
            "Algorithm: Adagrad, accuracy is 74.8546511627907 and f1 score is0.75447678565979\n",
            "Algorithm: Adam, accuracy is 68.09593023255815 and f1 score is0.6798825263977051\n",
            "Algorithm: AdamW, accuracy is 63.735465116279066 and f1 score is0.64720618724823\n",
            "Algorithm: Adamax, accuracy is 72.4563953488372 and f1 score is0.7297154664993286\n",
            "Algorithm: ASGD, accuracy is 66.35174418604652 and f1 score is0.6590175032615662\n",
            "Algorithm: NAdam, accuracy is 58.06686046511628 and f1 score is0.5827581882476807\n",
            "Algorithm: RAdam, accuracy is 66.06104651162791 and f1 score is0.6476108431816101\n",
            "Algorithm: RMSprop, accuracy is 70.20348837209302 and f1 score is0.711538553237915\n",
            "Algorithm: Rprop, accuracy is 62.28197674418605 and f1 score is0.6215466260910034\n",
            "Algorithm: SGD, accuracy is 63.95348837209303 and f1 score is0.648705244064331\n"
          ]
        }
      ],
      "source": [
        "# step 5\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "\n",
        "  model_Adadelta = LeNet().to(device)\n",
        "  optimizer = torch.optim.Adadelta(model_Adadelta.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_Adadelta)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Adadelta, acc_Adadelta, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_Adagrad = LeNet().to(device)\n",
        "  optimizer = torch.optim.Adagrad(model_Adagrad.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_Adagrad)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Adagrad, acc_Adagrad, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_Adam = LeNet().to(device)\n",
        "  optimizer = torch.optim.Adam(model_Adam.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_Adam)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Adam, acc_Adam, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_AdamW = LeNet().to(device)\n",
        "  optimizer = torch.optim.AdamW(model_AdamW.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_AdamW)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_AdamW, acc_AdamW, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "\n",
        "  model_Adamax = LeNet().to(device)\n",
        "  optimizer = torch.optim.Adamax(model_Adamax.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_Adamax)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Adamax, acc_Adamax, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_ASGD = LeNet().to(device)\n",
        "  optimizer = torch.optim.ASGD(model_ASGD.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_ASGD)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_ASGD, acc_ASGD, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "\n",
        "  model_NAdam = LeNet().to(device)\n",
        "  optimizer = torch.optim.NAdam(model_NAdam.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_NAdam)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_NAdam, acc_NAdam, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_RAdam = LeNet().to(device)\n",
        "  optimizer = torch.optim.RAdam(model_RAdam.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_RAdam)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_RAdam, acc_RAdam, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_RMSprop = LeNet().to(device)\n",
        "  optimizer = torch.optim.RMSprop(model_RMSprop.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_RMSprop)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_RMSprop, acc_RMSprop, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_Rprop = LeNet().to(device)\n",
        "  optimizer = torch.optim.Rprop(model_Rprop.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_Rprop)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Rprop, acc_Rprop, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model_SGD = LeNet().to(device)\n",
        "  optimizer = torch.optim.SGD(model_SGD.parameters(), lr = learning_rate)\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model_SGD)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_SGD, acc_SGD, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "\n",
        "  print(f\"Algorithm: Adadelta, accuracy is {acc_Adadelta} and f1 score is{f1_Adadelta}\")\n",
        "  print(f\"Algorithm: Adagrad, accuracy is {acc_Adagrad} and f1 score is{f1_Adagrad}\")\n",
        "  print(f\"Algorithm: Adam, accuracy is {acc_Adam} and f1 score is{f1_Adam}\")\n",
        "  print(f\"Algorithm: AdamW, accuracy is {acc_AdamW} and f1 score is{f1_AdamW}\")\n",
        "  print(f\"Algorithm: Adamax, accuracy is {acc_Adamax} and f1 score is{f1_Adamax}\")\n",
        "  print(f\"Algorithm: ASGD, accuracy is {acc_ASGD} and f1 score is{f1_ASGD}\")\n",
        "  print(f\"Algorithm: NAdam, accuracy is {acc_NAdam} and f1 score is{f1_NAdam}\")\n",
        "  print(f\"Algorithm: RAdam, accuracy is {acc_RAdam} and f1 score is{f1_RAdam}\")\n",
        "  print(f\"Algorithm: RMSprop, accuracy is {acc_RMSprop} and f1 score is{f1_RMSprop}\")\n",
        "  print(f\"Algorithm: Rprop, accuracy is {acc_Rprop} and f1 score is{f1_Rprop}\")\n",
        "  print(f\"Algorithm: SGD, accuracy is {acc_SGD} and f1 score is{f1_SGD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo9l4klBBWeX"
      },
      "source": [
        "Algorithm: Adadelta, accuracy is 48.11046511627907 and f1 score is 0.4658815860748291   \n",
        "\n",
        "Algorithm: Adagrad, accuracy is 74.8546511627907 and f1 score is 0.75447678565979\n",
        "\n",
        "Algorithm: Adam, accuracy is 68.09593023255815 and f1 score is 0.6798825263977051 \n",
        "\n",
        "Algorithm: AdamW, accuracy is 63.735465116279066 and f1 score is 0.64720618724823 \n",
        "\n",
        "Algorithm: Adamax, accuracy is 72.4563953488372 and f1 score is 0.7297154664993286 \n",
        "\n",
        "Algorithm: ASGD, accuracy is 66.35174418604652 and f1 score is 0.6590175032615662 \n",
        "\n",
        "Algorithm: NAdam, accuracy is 58.06686046511628 and f1 score is 0.5827581882476807 \n",
        "\n",
        "Algorithm: RAdam, accuracy is 66.06104651162791 and f1 score is 0.6476108431816101 \n",
        "\n",
        "Algorithm: RMSprop, accuracy is 70.20348837209302 and f1 score is 0.711538553237915 \n",
        "\n",
        "Algorithm: Rprop, accuracy is 62.28197674418605 and f1 score is 0.6215466260910034 \n",
        "\n",
        "Algorithm: SGD, accuracy is 63.95348837209303 and f1 score is 0.648705244064331 \n",
        "\n",
        "runtime 1h 43m 37s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKHArhRZBWle"
      },
      "source": [
        "![Screenshot 2022-08-05 020727.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4uBXUTZHb7K"
      },
      "source": [
        "We can see wild diffrences in the accuracy and f1 score, just by changing the optimiser. The best result was achieved using Adagrad and the worst with Adadelta. From the results we got, we can conclude that choosing the correct optimiser is exxtremely important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h1BsAQkF20Y"
      },
      "source": [
        "**QUESTION 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9_T-KTJIvtu",
        "outputId": "ba295c2c-fe0c-420e-bc59-ab4843b42b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import os, random\n",
        "from google.colab import drive\n",
        "import numpy as np \n",
        "from torchvision import datasets \n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score\n",
        "from torchmetrics import ConfusionMatrix\n",
        "drive.mount('/content/drive')\n",
        "torch.use_deterministic_algorithms(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ou0MlzTF9GW",
        "outputId": "d1cd7fc9-b22f-4527-f56a-e0bf5dd12e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (convolution_layer): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): CELU(alpha=1.0)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): CELU(alpha=1.0)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (7): CELU(alpha=1.0)\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (10): CELU(alpha=1.0)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): CELU(alpha=1.0)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): CELU(alpha=1.0)\n",
            "    (4): Linear(in_features=256, out_features=32, bias=True)\n",
            "    (5): CELU(alpha=1.0)\n",
            "    (6): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# step 1\n",
        "melgramtraindata = np.load('/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/X.npy')\n",
        "melgramtrainlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/labels.npy\")\n",
        "\n",
        "melgramtestdata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/X.npy\")\n",
        "melgramtestlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/labels.npy\")\n",
        "\n",
        "melgramvaldata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/X.npy\")\n",
        "melgramvallabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/labels.npy\")\n",
        "\n",
        "\n",
        "def torch_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "torch_seed(seed=0)\n",
        "\n",
        "class melgramdataset(Dataset): #use opposite labels\n",
        "  def __init__(self,data, labels,labels_map,transform=None, target_transform=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.melgramlabels = []\n",
        "    for l in labels:\n",
        "      self.melgramlabels.append(labels_map[l])\n",
        "    self.melgramlabels = np.array(self.melgramlabels)\n",
        "    self.melgramdata = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.melgramlabels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sound = self.melgramdata[index]\n",
        "    label = self.melgramlabels[index]\n",
        "    if self.transform:\n",
        "      sound = self.transform(sound)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "\n",
        "    return sound, label\n",
        "\n",
        "labels_map = {\n",
        "    0: \"classical\",\n",
        "    1: \"blues\",\n",
        "    2: \"rock_metal_hardrock\",\n",
        "    3: \"hiphop\",\n",
        "}\n",
        "\n",
        "opposite_labels_map = {\n",
        "    \"classical\":0,\n",
        "    \"blues\":1,\n",
        "    \"rock_metal_hardrock\":2,\n",
        "    \"hiphop\":3,\n",
        "}\n",
        "\n",
        "training_data = melgramdataset(melgramtraindata, melgramtrainlabels, opposite_labels_map, torch.tensor)\n",
        "val_data = melgramdataset(melgramvaldata, melgramvallabels, opposite_labels_map, torch.tensor)\n",
        "test_data = melgramdataset(melgramtestdata, melgramtestlabels, opposite_labels_map, torch.tensor)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=800 ,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=1376 ,shuffle=False )\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(LeNet, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(1024, 256),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(256,32),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "print(model)\n",
        "\n",
        "def testing(dataloader, costfunct, model):\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      test_loss += costfunct(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  acc = 100*correct\n",
        "\n",
        "  f1 = f1_score(pred, y, num_classes=4, average=\"macro\")\n",
        "  confmatrix = ConfusionMatrix(num_classes=4).to(device)\n",
        "  model.train()\n",
        "  return test_loss, f1, acc, confmatrix\n",
        "\n",
        "\n",
        "def training_with_validation(epochs, optimizer, dataloader, validationdata,costfunct, model):\n",
        "  \n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1⁸\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "  return bestmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcjHLirZ2Yp1",
        "outputId": "e0ce1971-6813-435c-ff45-18e72df84a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss is: 0.001011837447105452, f1 score is: 0.7710870504379272, accuracy is: 76.88953488372093\n"
          ]
        }
      ],
      "source": [
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "model = LeNet().to(device)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)\n",
        "bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "test_loss, f1, acc, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "print(f\"test loss is: {test_loss}, f1 score is: {f1}, accuracy is: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFoslsIpNTqF"
      },
      "source": [
        "test loss is: 0.001011837447105452, f1 score is: 0.7710870504379272, accuracy is: 76.88953488372093\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hmaAtB4y1Y6"
      },
      "outputs": [],
      "source": [
        "execute = False\n",
        "\n",
        "\n",
        "\n",
        "class LeNetwithfun(nn.Module):\n",
        "  def __init__(self, fun) -> None:\n",
        "      super(LeNetwithfun, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          fun(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          fun(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          fun(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          fun(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          fun(),\n",
        "          nn.Linear(1024, 256),\n",
        "          fun(),\n",
        "          nn.Linear(256,32),\n",
        "          fun(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "if execute:\n",
        "  model = LeNetwithfun(nn.ELU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_ELU, acc_ELU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Hardshrink).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Hardshrink, acc_Hardshrink, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Hardsigmoid).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Hardsigmoid, acc_Hardsigmoid, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model = LeNetwithfun(nn.Hardtanh).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Hardtanh, acc_Hardtanh, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Hardswish).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Hardswish, acc_Hardswish, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "\n",
        "  model = LeNetwithfun(nn.LeakyReLU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_LeakyReLU, acc_LeakyReLU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.LogSigmoid).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_LogSigmoid, acc_LogSigmoid, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.PReLU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_PReLU, acc_PReLU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.ReLU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_ReLU, acc_ReLU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.ReLU6).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_ReLU6, acc_ReLU6, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.RReLU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_RReLU, acc_RReLU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.SELU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_SELU, acc_SELU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.CELU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_CELU, acc_CELU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.GELU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_GELU, acc_GELU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Sigmoid).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Sigmoid, acc_Sigmoid, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.SiLU).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_SiLU, acc_SiLU, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Mish).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Mish, acc_Mish, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Softplus).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Softplus, acc_Softplus, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Softshrink).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Softshrink, acc_Softshrink, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Softsign).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Softsign, acc_Softsign, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Tanh).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Tanh, acc_Tanh, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  model = LeNetwithfun(nn.Tanhshrink).to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #using SGD because it is faster\n",
        "  bestmodel = training_with_validation(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model)\n",
        "  print(\"TESTING THE BEST EPOCH\\n\\n\")\n",
        "  test_loss, f1_Tanhshrink, acc_Tanhshrink, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  \n",
        "  print(f\"Algorithm: ELU, accuracy is {acc_ELU} and f1 score is{f1_ELU}\")\n",
        "  print(f\"Algorithm: Hardshrink, accuracy is {acc_Hardshrink} and f1 score is{f1_Hardshrink}\")\n",
        "  print(f\"Algorithm: Hardsigmoid, accuracy is {acc_Hardsigmoid} and f1 score is{f1_Hardsigmoid}\")\n",
        "  print(f\"Algorithm: Hardtanh, accuracy is {acc_Hardtanh} and f1 score is{f1_Hardtanh}\")\n",
        "  print(f\"Algorithm: Hardswish, accuracy is {acc_Hardswish} and f1 score is{f1_Hardswish}\")\n",
        "  print(f\"Algorithm: LeakyReLU, accuracy is {acc_LeakyReLU} and f1 score is{f1_LeakyReLU}\")\n",
        "  print(f\"Algorithm: LogSigmoid, accuracy is {acc_LogSigmoid} and f1 score is{f1_LogSigmoid}\")\n",
        "  print(f\"Algorithm: PReLU, accuracy is {acc_PReLU} and f1 score is{f1_PReLU}\")\n",
        "  print(f\"Algorithm: ReLU, accuracy is {acc_ReLU} and f1 score is{f1_ReLU}\")\n",
        "  print(f\"Algorithm: ReLU6, accuracy is {acc_ReLU6} and f1 score is{f1_ReLU6}\")\n",
        "  print(f\"Algorithm: RReLU, accuracy is {acc_RReLU} and f1 score is{f1_RReLU}\")\n",
        "  print(f\"Algorithm: SELU, accuracy is {acc_SELU} and f1 score is{f1_SELU}\")\n",
        "  print(f\"Algorithm: CELU, accuracy is {acc_CELU} and f1 score is{f1_CELU}\")\n",
        "  print(f\"Algorithm: GELU, accuracy is {acc_GELU} and f1 score is{f1_GELU}\")\n",
        "  print(f\"Algorithm: Sigmoid, accuracy is {acc_Sigmoid} and f1 score is{f1_Sigmoid}\")\n",
        "  print(f\"Algorithm: SiLU, accuracy is {acc_SiLU} and f1 score is{f1_SiLU}\")\n",
        "  print(f\"Algorithm: Mish, accuracy is {acc_Mish} and f1 score is{f1_Mish}\")\n",
        "  print(f\"Algorithm: Softplus, accuracy is {acc_Softplus} and f1 score is{f1_Softplus}\")\n",
        "  print(f\"Algorithm: Softshrink, accuracy is {acc_Softshrink} and f1 score is{f1_Softshrink}\")\n",
        "  print(f\"Algorithm: Softsign, accuracy is {acc_Softsign} and f1 score is{f1_Softsign}\")\n",
        "  print(f\"Algorithm: Tanh, accuracy is {acc_Tanh} and f1 score is{f1_Tanh}\")\n",
        "  print(f\"Algorithm: Tanhshrink, accuracy is {acc_Tanhshrink} and f1 score is{f1_Tanhshrink}\")\n",
        " \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emagYCuszlqj"
      },
      "source": [
        "Algorithm: ELU, accuracy is 71.36627906976744 and f1 score is 0.7176399230957031\n",
        "\n",
        "Algorithm: Hardshrink, accuracy is 72.60174418604652 and f1 score is 0.7265828847885132\n",
        "\n",
        "Algorithm: Hardsigmoid, accuracy is 23.546511627906977 and f1 score is 0.09529411792755127\n",
        "\n",
        "Algorithm: Hardtanh, accuracy is 56.25 and f1 score is 0.5464939475059509\n",
        "\n",
        "Algorithm: Hardswish, accuracy is 71.2936046511628 and f1 score is 0.7253984808921814\n",
        "\n",
        "Algorithm: LeakyReLU, accuracy is 67.15116279069767 and f1 score is 0.6638474464416504\n",
        "\n",
        "Algorithm: LogSigmoid, accuracy is 39.098837209302324 and f1 score is 0.2722686529159546\n",
        "\n",
        "Algorithm: PReLU, accuracy is 69.04069767441861 and f1 score is 0.6988807916641235\n",
        "\n",
        "Algorithm: ReLU, accuracy is 56.83139534883721 and f1 score is 0.5654309391975403\n",
        "\n",
        "Algorithm: ReLU6, accuracy is 44.622093023255815 and f1 score is 0.42181751132011414\n",
        "\n",
        "Algorithm: RReLU, accuracy is 65.26162790697676 and f1 score is 0.6531794667243958\n",
        "\n",
        "Algorithm: SELU, accuracy is 73.54651162790698 and f1 score is 0.7299819588661194\n",
        "\n",
        "Algorithm: CELU, accuracy is 74.12790697674419 and f1 score is 0.7502588033676147\n",
        "\n",
        "Algorithm: GELU, accuracy is 73.98255813953489 and f1 score is 0.743916928768158\n",
        "\n",
        "Algorithm: Sigmoid, accuracy is 23.546511627906977 and f1 score is 0.09529411792755127\n",
        "\n",
        "Algorithm: SiLU, accuracy is 72.23837209302324 and f1 score is 0.7225213050842285\n",
        "\n",
        "Algorithm: Mish, accuracy is 69.3313953488372 and f1 score is 0.6946865916252136\n",
        "\n",
        "Algorithm: Softplus, accuracy is 69.04069767441861 and f1 score is 0.6950930953025818\n",
        "\n",
        "Algorithm: Softshrink, accuracy is 28.997093023255815 and f1 score is 0.11239437013864517\n",
        "\n",
        "Algorithm: Softsign, accuracy is 35.61046511627907 and f1 score is 0.3194787800312042 \n",
        "\n",
        "Algorithm: Tanh, accuracy is 52.8343023255814 and f1 score is 0.4770197868347168\n",
        "\n",
        "Algorithm: Tanhshrink, accuracy is 28.997093023255815 and f1 score is 0.11258465051651001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwq33Wpc9JZV"
      },
      "source": [
        "![Screenshot 2022-08-09 171416.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stYQz5tnLGVr"
      },
      "outputs": [],
      "source": [
        "# step 4\n",
        "\n",
        "\n",
        "def training_with_scheduler(epochs, optimizer, dataloader, validationdata,costfunct, model, scheduler):\n",
        "  \n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "  return bestmodel\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "model = LeNet().to(device)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate) \n",
        "\n",
        "# lr_lambda = lambda epoch : epoch/10\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: LambdaLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# lr_lambda = lambda epoch: 0.95\n",
        "# scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: MultiplicativeLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: StepLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: MultiStepLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: ConstantLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_1, acc_1, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: LinearLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_2, acc_2, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: ExponentialLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_3, acc_3, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: CosineAnnealingLR, accuracy is {acc_} and f1 score is {f1_}\")\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, verbose=True)\n",
        "# bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "# test_loss, f1_7, acc_7, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "# print(f\"Algorithm: CosineAnnealingWarmRestarts, accuracy is {acc_} and f1 score is {f1_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrOOrpztUKC1"
      },
      "source": [
        "Algorithm: LambdaLR, accuracy is 72.4563953488372 and f1 score is 0.7266383171081543\n",
        "\n",
        "Algorithm: MultiplicativeLR, accuracy is 75.79941860465115 and f1 score is 0.7606614828109741\n",
        "\n",
        "Algorithm: StepLR, accuracy is 78.41569767441861 and f1 score is 0.7872134447097778\n",
        "\n",
        "Algorithm: MultiStepLR, accuracy is 76.30813953488372 and f1 score is 0.7659837007522583\n",
        "\n",
        "Algorithm: ConstantLR, accuracy is 74.63662790697676 and f1 score is 0.7512418031692505\n",
        "\n",
        "Algorithm: LinearLR, accuracy is 76.30813953488372 and f1 score is 0.7670813202857971\n",
        "\n",
        "Algorithm: ExponentialLR, accuracy is 76.16279069767442 and f1 score is 0.7657319903373718\n",
        "\n",
        "Algorithm: CosineAnnealingLR, accuracy is 76.16279069767442 and f1 score is 0.7656395435333252\n",
        "\n",
        "Algorithm: CosineAnnealingWarmRestarts, accuracy is 76.09011627906976 and f1 score is 0.7642688155174255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdT4wWtDJjWn"
      },
      "outputs": [],
      "source": [
        "# step 5\n",
        "\n",
        "class LeNet_with_batchnorm2d(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(LeNet_with_batchnorm2d, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.BatchNorm2d(1),\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(1024, 256),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(256,32),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxMGzxqNIQs3"
      },
      "outputs": [],
      "source": [
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "model = LeNet_with_batchnorm2d().to(device)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate) \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, verbose=True)\n",
        "bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PCWaJPAMCC7"
      },
      "source": [
        "without batch normalization:Accuracy: 74.9%, Avg loss: 0.000682 f1 score is: 0.7530226707458496\n",
        "\n",
        "with batch normalization:Accuracy: 76.3%, Avg loss: 0.000687 f1 score is: 0.7673460841178894"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDTu5FvUQqYm"
      },
      "outputs": [],
      "source": [
        "# step 6\n",
        "\n",
        "class LeNet_with_batchnormd(nn.Module):\n",
        "  def __init__(self, per) -> None:\n",
        "      super(LeNet_with_batchnormd, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.BatchNorm2d(1),\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          nn.Dropout(p=per),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(1024, 256),\n",
        "          nn.Dropout(p=per),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(256,32),\n",
        "          nn.Dropout(p=per),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(32,4),\n",
        "          nn.Dropout(p=per),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 60\n",
        "batch_size = 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMPZ09AnIVUv"
      },
      "outputs": [],
      "source": [
        "for num_epochs in [30,60]:\n",
        "print(f\"epochs: {num_epochs}\")\n",
        "for (per,wd) in [(0.1,1e-8), (0.1,1e-5), (0.1,1e-3), (0.2,1e-8), (0.2,1e-5), (0.2,1e-3), (0.3,1e-8),(0.3,1e-5), (0.3,1e-3),(0.4,1e-8), (0.4,1e-5), (0.4,1e-3)]:\n",
        "  model = LeNet_with_batchnormd(per).to(device)\n",
        "  optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate, weight_decay= wd) \n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
        "  bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "  test_loss, f1_t, acc_t, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "  test_loss, f1_v, acc_v, confmatrix = testing(val_dataloader, costfun, bestmodel)\n",
        "  print(f\"For weight decay = {wd} and dropout = {per} we got:Accuracy for test: {acc_t}, f1 score for test: {f1_t}, Accuracy for val: {acc_v}, f1 score for val: {f1_v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CVg2Pa9SaIW"
      },
      "source": [
        "**No combinations:**\n",
        "\n",
        "**30 EPOCHS**:\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0 we got:Accuracy for test: 76.96220930232558, f1 score for test: 0.7719160318374634, Accuracy for val: 78.125, f1 score for val: 0.7776024341583252\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0 we got:Accuracy for test: 77.32558139534885, f1 score for test: 0.7764490842819214, Accuracy for val: 78.0, f1 score for val: 0.7776146531105042\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0 we got:Accuracy for test: 76.16279069767442, f1 score for test: 0.7656721472740173, Accuracy for val: 78.125, f1 score for val: 0.7756579518318176\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0 we got:Accuracy for test: 71.36627906976744, f1 score for test: 0.6865783929824829, Accuracy for val: 71.375, f1 score for val: 0.6890854835510254\n",
        "\n",
        "For weight decay = 0 and dropout = 0.1 we got:Accuracy for test: 75.36337209302324, f1 score for test: 0.7552271485328674, Accuracy for val: 77.875, f1 score for val: 0.7759751081466675\n",
        "\n",
        "For weight decay = 0 and dropout = 0.2 we got:Accuracy for test: 73.32848837209302, f1 score for test: 0.7360669374465942, Accuracy for val: 74.25, f1 score for val: 0.7397034168243408\n",
        "\n",
        "For weight decay = 0 and dropout = 0.3 we got:Accuracy for test: 67.58720930232558, f1 score for test: 0.6742035150527954, Accuracy for val: 71.25, f1 score for val: 0.7081637978553772\n",
        "\n",
        "For weight decay = 0 and dropout = 0.4 we got:Accuracy for test: 63.44476744186046, f1 score for test: 0.6315640211105347, Accuracy for val: 61.75000000000001, f1 score for val: 0.6087625026702881\n",
        "\n",
        "For weight decay = 0 and dropout = 0.5 we got:Accuracy for test: 59.01162790697675, f1 score for test: 0.5850566029548645, Accuracy for val: 59.375, f1 score for val: 0.5810511112213135\n",
        "\n",
        "For weight decay = 0 and dropout = 0.6 we got:Accuracy for test: 50.36337209302325, f1 score for test: 0.49405768513679504, Accuracy for val: 53.0, f1 score for val: 0.5114620327949524\n",
        "\n",
        "**60 EPOCHS**: \n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0 we got:Accuracy for test: 76.09011627906976, f1 score for test: 0.7630730867385864, Accuracy for val: 79.875, f1 score for val: 0.7963074445724487\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0 we got:Accuracy for test: 78.85174418604652, f1 score for test: 0.7913280129432678, Accuracy for val: 79.375, f1 score for val: 0.7898926138877869\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0 we got:Accuracy for test: 76.67151162790698, f1 score for test: 0.7680922746658325, Accuracy for val: 80.375, f1 score for val: 0.8024773597717285\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0 we got:Accuracy for test: 70.56686046511628, f1 score for test: 0.6333122849464417, Accuracy for val: 70.75, f1 score for val: 0.6368185877799988\n",
        "\n",
        "For weight decay = 0 and dropout = 0.1 we got:Accuracy for test: 74.92732558139535, f1 score for test: 0.7546381950378418, Accuracy for val: 76.875, f1 score for val: 0.7662799954414368\n",
        "\n",
        "For weight decay = 0 and dropout = 0.2 we got:Accuracy for test: 72.89244186046511, f1 score for test: 0.7278017997741699, Accuracy for val: 77.125, f1 score for val: 0.7686965465545654\n",
        "\n",
        "For weight decay = 0 and dropout = 0.3 we got:Accuracy for test: 69.3313953488372, f1 score for test: 0.6901178359985352, Accuracy for val: 69.0, f1 score for val: 0.6827294826507568\n",
        "\n",
        "For weight decay = 0 and dropout = 0.4 we got:Accuracy for test: 65.33430232558139, f1 score for test: 0.6507346034049988, Accuracy for val: 67.625, f1 score for val: 0.6690561175346375\n",
        "\n",
        "For weight decay = 0 and dropout = 0.5 we got:Accuracy for test: 56.395348837209305, f1 score for test: 0.5587469935417175, Accuracy for val: 57.49999999999999, f1 score for val: 0.5643199682235718\n",
        "\n",
        "For weight decay = 0 and dropout = 0.6 we got:Accuracy for test: 51.38081395348837, f1 score for test: 0.5067238807678223, Accuracy for val: 53.0, f1 score for val: 0.5153448581695557\n",
        "\n",
        "**With combinations:**\n",
        "\n",
        "**30 EPOCHS**:\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.1 we got:Accuracy for test: 75.07267441860465, f1 score for test: 0.7529845237731934, Accuracy for val: 79.5, f1 score for val: 0.792290985584259\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.1 we got:Accuracy for test: 73.32848837209302, f1 score for test: 0.7323817610740662, Accuracy for val: 77.25, f1 score for val: 0.7700823545455933\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.1 we got:Accuracy for test: 74.56395348837209, f1 score for test: 0.7481275200843811, Accuracy for val: 80.0, f1 score for val: 0.7976380586624146\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0.1 we got:Accuracy for test: 67.00581395348837, f1 score for test: 0.6312659978866577, Accuracy for val: 65.5, f1 score for val: 0.6120901703834534\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.2 we got:Accuracy for test: 72.52906976744185, f1 score for test: 0.7259721159934998, Accuracy for val: 72.0, f1 score for val: 0.715598464012146\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.2 we got:Accuracy for test: 72.60174418604652, f1 score for test: 0.7253112196922302, Accuracy for val: 74.125, f1 score for val: 0.7374292612075806\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.2 we got:Accuracy for test: 72.96511627906976, f1 score for test: 0.7311593294143677, Accuracy for val: 74.125, f1 score for val: 0.7375927567481995\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0.2 we got:Accuracy for test: 65.55232558139535, f1 score for test: 0.6266173720359802, Accuracy for val: 64.0, f1 score for val: 0.608869731426239\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.3 we got:Accuracy for test: 68.67732558139535, f1 score for test: 0.6860613822937012, Accuracy for val: 71.625, f1 score for val: 0.71070396900177\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.3 we got:Accuracy for test: 68.53197674418605, f1 score for test: 0.6842492818832397, Accuracy for val: 70.125, f1 score for val: 0.6962434649467468\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.3 we got:Accuracy for test: 68.09593023255815, f1 score for test: 0.680064857006073, Accuracy for val: 69.375, f1 score for val: 0.6909332275390625\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0.3 we got:Accuracy for test: 60.75581395348837, f1 score for test: 0.5865418910980225, Accuracy for val: 59.0, f1 score for val: 0.569930911064148\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.4 we got:Accuracy for test: 61.19186046511628, f1 score for test: 0.6078724265098572, Accuracy for val: 67.75, f1 score for val: 0.6742795705795288\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.4 we got:Accuracy for test: 62.7906976744186, f1 score for test: 0.6263649463653564, Accuracy for val: 63.74999999999999, f1 score for val: 0.6310532093048096\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.4 we got:Accuracy for test: 63.29941860465116, f1 score for test: 0.6305813193321228, Accuracy for val: 65.125, f1 score for val: 0.6453076601028442\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0.4 we got:Accuracy for test: 56.61337209302325, f1 score for test: 0.5547423362731934, Accuracy for val: 57.375, f1 score for val: 0.5526957511901855\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.5 we got:Accuracy for test: 56.97674418604651, f1 score for test: 0.5644086599349976, Accuracy for val: 59.875, f1 score for val: 0.5887171030044556\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.5 we got:Accuracy for test: 57.55813953488372, f1 score for test: 0.5717458724975586, Accuracy for val: 59.62499999999999, f1 score for val: 0.587802529335022\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.5 we got:Accuracy for test: 55.377906976744185, f1 score for test: 0.5458181500434875, Accuracy for val: 59.75, f1 score for val: 0.5873796343803406\n",
        "\n",
        "For weight decay = 0.1 and dropout = 0.5 we got:Accuracy for test: 52.32558139534884, f1 score for test: 0.5170111060142517, Accuracy for val: 52.625, f1 score for val: 0.5140092968940735\n",
        "\n",
        "**60 EPOCHS**:\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.1 we got:Accuracy for test: 75.29069767441861, f1 score for test: 0.7545542120933533, Accuracy for val: 78.125, f1 score for val: 0.7790539264678955\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.1 we got:Accuracy for test: 74.78197674418605, f1 score for test: 0.7497494220733643, Accuracy for val: 77.5, f1 score for val: 0.769476056098938\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.1 we got:Accuracy for test: 75.65406976744185, f1 score for test: 0.7602909803390503, Accuracy for val: 78.25, f1 score for val: 0.7789798974990845\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.2 we got:Accuracy for test: 73.25581395348837, f1 score for test: 0.7352230548858643, Accuracy for val: 76.0, f1 score for val: 0.7556354403495789\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.2 we got:Accuracy for test: 71.94767441860465, f1 score for test: 0.7200361490249634, Accuracy for val: 72.875, f1 score for val: 0.7266834378242493\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.2 we got:Accuracy for test: 72.09302325581395, f1 score for test: 0.7227920889854431, Accuracy for val: 75.125, f1 score for val: 0.7464937567710876\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.3 we got:Accuracy for test: 69.69476744186046, f1 score for test: 0.6974359750747681, Accuracy for val: 71.0, f1 score for val: 0.7050396203994751\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.3 we got:Accuracy for test: 68.09593023255815, f1 score for test: 0.6800686120986938, Accuracy for val: 71.375, f1 score for val: 0.7074965238571167\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.3 we got:Accuracy for test: 68.24127906976744, f1 score for test: 0.6819179654121399, Accuracy for val: 69.625, f1 score for val: 0.6930192708969116\n",
        "\n",
        "For weight decay = 1e-08 and dropout = 0.4 we got:Accuracy for test: 63.08139534883721, f1 score for test: 0.6279135942459106, Accuracy for val: 65.625, f1 score for val: 0.6491997241973877\n",
        "\n",
        "For weight decay = 1e-05 and dropout = 0.4 we got:Accuracy for test: 65.26162790697676, f1 score for test: 0.650551438331604, Accuracy for val: 63.74999999999999, f1 score for val: 0.6308689117431641\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.4 we got:Accuracy for test: 62.57267441860465, f1 score for test: 0.6212955117225647, Accuracy for val: 65.875, f1 score for val: 0.6528435945510864"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWQuppsvl2CK",
        "outputId": "1c1f2e54-8d23-4e3c-ce86-bf6cf5192798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For batch size: 2\n",
            "Time needed is: 893.9695086479187\n",
            "Accuracy is: 73.61918604651163 and f1 score is: 0.7379705905914307\n",
            "For batch size: 4\n",
            "Time needed is: 653.2055225372314\n",
            "Accuracy is: 75.1453488372093 and f1 score is: 0.7510440349578857\n",
            "For batch size: 8\n",
            "Time needed is: 533.0639019012451\n",
            "Accuracy is: 75.72674418604652 and f1 score is: 0.7590613961219788\n",
            "For batch size: 16\n",
            "Time needed is: 467.64280557632446\n",
            "Accuracy is: 76.52616279069767 and f1 score is: 0.7672462463378906\n",
            "For batch size: 32\n",
            "Time needed is: 440.75231981277466\n",
            "Accuracy is: 76.38081395348837 and f1 score is: 0.766360342502594\n",
            "For batch size: 64\n",
            "Time needed is: 467.5130226612091\n",
            "Accuracy is: 77.32558139534885 and f1 score is: 0.775765597820282\n",
            "For batch size: 128\n",
            "Time needed is: 418.356303691864\n",
            "Accuracy is: 78.99709302325581 and f1 score is: 0.7926731109619141\n"
          ]
        }
      ],
      "source": [
        "# step 7\n",
        "\n",
        "# batch size test\n",
        "\n",
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "execute = False\n",
        "if execute:\n",
        "  for i in [2,4,8,16,32,64,128]:\n",
        "    print(f\"For batch size: {i}\")\n",
        "    train_dataloader = DataLoader(training_data, batch_size=i,shuffle=True )\n",
        "    start_time = time.time()\n",
        "    model = LeNet_with_batchnorm2d().to(device)\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
        "    bestmodel = training_with_scheduler(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler)\n",
        "    test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "    print(f\"Time needed is: {time.time() - start_time}\")\n",
        "    print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az8VPy-Nx_WC"
      },
      "source": [
        "For batch size: 2\n",
        "Time needed is: 893.9695086479187\n",
        "Accuracy is: 73.61918604651163 and f1 score is: 0.7379705905914307\n",
        "\n",
        "For batch size: 4\n",
        "Time needed is: 653.2055225372314\n",
        "Accuracy is: 75.1453488372093 and f1 score is: 0.7510440349578857\n",
        "\n",
        "For batch size: 8\n",
        "Time needed is: 533.0639019012451\n",
        "Accuracy is: 75.72674418604652 and f1 score is: 0.7590613961219788\n",
        "\n",
        "For batch size: 16\n",
        "Time needed is: 467.64280557632446\n",
        "Accuracy is: 76.52616279069767 and f1 score is: 0.7672462463378906\n",
        "\n",
        "For batch size: 32\n",
        "Time needed is: 440.75231981277466\n",
        "Accuracy is: 76.38081395348837 and f1 score is: 0.766360342502594\n",
        "\n",
        "For batch size: 64\n",
        "Time needed is: 467.5130226612091\n",
        "Accuracy is: 77.32558139534885 and f1 score is: 0.775765597820282\n",
        "\n",
        "For batch size: 128\n",
        "Time needed is: 418.356303691864\n",
        "Accuracy is: 78.99709302325581 and f1 score is: 0.7926731109619141"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-r02UdFy6es"
      },
      "source": [
        "As the batch size increases, the time needed of course becomes lower, since we process more data at once. The accuracy and f1 scores also increase, the bigger the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8k_flE-ok0X",
        "outputId": "74947574-a859-4d23-8169-9142bf4d2813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For patience: 1\n",
            "Time needed is: 49.31858801841736\n",
            "Accuracy is: 76.96220930232558 and f1 score is: 0.7757563591003418\n",
            "For patience: 2\n",
            "Time needed is: 65.9721462726593\n",
            "Accuracy is: 75.5813953488372 and f1 score is: 0.7509776949882507\n",
            "For patience: 3\n",
            "Time needed is: 127.21571493148804\n",
            "Accuracy is: 77.5436046511628 and f1 score is: 0.777355432510376\n",
            "For patience: 4\n",
            "Time needed is: 110.31999111175537\n",
            "Accuracy is: 75.36337209302324 and f1 score is: 0.7577177286148071\n",
            "For patience: 5\n",
            "Time needed is: 109.87355256080627\n",
            "Accuracy is: 77.03488372093024 and f1 score is: 0.7623605132102966\n",
            "For patience: 6\n",
            "Time needed is: 186.7848629951477\n",
            "Accuracy is: 78.19767441860465 and f1 score is: 0.7833513021469116\n",
            "For patience: 7\n",
            "Time needed is: 155.87920761108398\n",
            "Accuracy is: 75.65406976744185 and f1 score is: 0.7589560151100159\n",
            "For patience: 8\n",
            "Time needed is: 156.2926914691925\n",
            "Accuracy is: 76.74418604651163 and f1 score is: 0.7695696949958801\n",
            "For patience: 9\n",
            "Time needed is: 295.57422947883606\n",
            "Accuracy is: 77.39825581395348 and f1 score is: 0.7760665416717529\n",
            "For patience: 10\n",
            "Time needed is: 247.39183568954468\n",
            "Accuracy is: 75.43604651162791 and f1 score is: 0.7568193674087524\n"
          ]
        }
      ],
      "source": [
        "# early stopping\n",
        "\n",
        "def training_with_early_stopping(epochs, optimizer, dataloader, validationdata,costfunct, model, scheduler, patience = 7):\n",
        "  conc = 0\n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    \n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "      conc = 0\n",
        "    else:\n",
        "      conc+=1\n",
        "      if conc == patience:\n",
        "        return bestmodel\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "  return bestmodel\n",
        "\n",
        "execute = False\n",
        "if execute:\n",
        "  costfun = nn.CrossEntropyLoss()\n",
        "  learning_rate = 0.002\n",
        "  num_epochs = 30\n",
        "  train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "  for i in [1,2,3,4,5,6,7,8,9,10]:\n",
        "    print(f\"For patience: {i}\")\n",
        "    start_time = time.time()\n",
        "    model = LeNet_with_batchnorm2d().to(device)\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
        "    bestmodel = training_with_early_stopping(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler,i)\n",
        "    test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "    print(f\"Time needed is: {time.time() - start_time}\")\n",
        "    print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9YNM4mvzRAk"
      },
      "source": [
        "For patience: 1\n",
        "Time needed is: 49.31858801841736\n",
        "Accuracy is: 76.96220930232558 and f1 score is: 0.7757563591003418\n",
        "\n",
        "For patience: 2\n",
        "Time needed is: 65.9721462726593\n",
        "Accuracy is: 75.5813953488372 and f1 score is: 0.7509776949882507\n",
        "\n",
        "For patience: 3\n",
        "Time needed is: 127.21571493148804\n",
        "Accuracy is: 77.5436046511628 and f1 score is: 0.777355432510376\n",
        "\n",
        "For patience: 4\n",
        "Time needed is: 110.31999111175537\n",
        "Accuracy is: 75.36337209302324 and f1 score is: 0.7577177286148071\n",
        "\n",
        "For patience: 5\n",
        "Time needed is: 109.87355256080627\n",
        "Accuracy is: 77.03488372093024 and f1 score is: 0.7623605132102966\n",
        "\n",
        "For patience: 6\n",
        "Time needed is: 186.7848629951477\n",
        "Accuracy is: 78.19767441860465 and f1 score is: 0.7833513021469116\n",
        "\n",
        "For patience: 7\n",
        "Time needed is: 155.87920761108398\n",
        "Accuracy is: 75.65406976744185 and f1 score is: 0.7589560151100159\n",
        "\n",
        "For patience: 8\n",
        "Time needed is: 156.2926914691925\n",
        "Accuracy is: 76.74418604651163 and f1 score is: 0.7695696949958801\n",
        "\n",
        "For patience: 9\n",
        "Time needed is: 295.57422947883606\n",
        "Accuracy is: 77.39825581395348 and f1 score is: 0.7760665416717529\n",
        "\n",
        "For patience: 10\n",
        "Time needed is: 247.39183568954468\n",
        "Accuracy is: 75.43604651162791 and f1 score is: 0.7568193674087524"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuAeNGGRzRGR"
      },
      "source": [
        "As the patience increases, we need more time to run more epochs. The best result was achieved with patience = 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMZUryg5QLC"
      },
      "source": [
        "**QUESTION 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6SQK4qZ5kKw",
        "outputId": "2d72e0de-b1a3-4078-eccf-7403c7a28c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import os, random\n",
        "from google.colab import drive\n",
        "import numpy as np \n",
        "from torchvision import datasets \n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score\n",
        "from torchmetrics import ConfusionMatrix\n",
        "drive.mount('/content/drive')\n",
        "torch.use_deterministic_algorithms(True)\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5lB_pB85mpy"
      },
      "outputs": [],
      "source": [
        "melgramtraindata = np.load('/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/X.npy')\n",
        "melgramtrainlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/train/melgrams/labels.npy\")\n",
        "\n",
        "melgramtestdata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/X.npy\")\n",
        "melgramtestlabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/test/melgrams/labels.npy\")\n",
        "\n",
        "melgramvaldata = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/X.npy\")\n",
        "melgramvallabels = np.load(\"/content/drive/My Drive/HW3DATA/music_genre_data_di/val/melgrams/labels.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4oSTFBT5qio"
      },
      "outputs": [],
      "source": [
        "def torch_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "torch_seed(seed=0)\n",
        "\n",
        "class melgramdataset(Dataset): #use opposite labels\n",
        "  def __init__(self,data, labels,labels_map,transform=None, target_transform=None) -> None:\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.melgramlabels = []\n",
        "    for l in labels:\n",
        "      self.melgramlabels.append(labels_map[l])\n",
        "    self.melgramlabels = np.array(self.melgramlabels)\n",
        "    self.melgramdata = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.melgramlabels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sound = self.melgramdata[index]\n",
        "    label = self.melgramlabels[index]\n",
        "    if self.transform:\n",
        "      sound = self.transform(sound)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "\n",
        "    return sound, label\n",
        "\n",
        "labels_map = {\n",
        "    0: \"classical\",\n",
        "    1: \"blues\",\n",
        "    2: \"rock_metal_hardrock\",\n",
        "    3: \"hiphop\",\n",
        "}\n",
        "\n",
        "opposite_labels_map = {\n",
        "    \"classical\":0,\n",
        "    \"blues\":1,\n",
        "    \"rock_metal_hardrock\":2,\n",
        "    \"hiphop\":3,\n",
        "}\n",
        "\n",
        "training_data = melgramdataset(melgramtraindata, melgramtrainlabels, opposite_labels_map, torch.tensor)\n",
        "val_data = melgramdataset(melgramvaldata, melgramvallabels, opposite_labels_map, torch.tensor)\n",
        "test_data = melgramdataset(melgramtestdata, melgramtestlabels, opposite_labels_map, torch.tensor)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=800 ,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=1376 ,shuffle=False )\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfDocJw653EC"
      },
      "outputs": [],
      "source": [
        "def testing(dataloader, costfunct, model):\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      test_loss += costfunct(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  acc = 100*correct\n",
        "\n",
        "  f1 = f1_score(pred, y, num_classes=4, average=\"macro\")\n",
        "  confmatrix = ConfusionMatrix(num_classes=4).to(device)\n",
        "  return test_loss, f1, acc, confmatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnWTE2Ca6lg0"
      },
      "outputs": [],
      "source": [
        "def training_with_early_stopping(epochs, optimizer, dataloader, validationdata,costfunct, model, scheduler, patience = 7):\n",
        "  conc = 0\n",
        "  bestmodel = None \n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(dataloader.dataset)\n",
        "  for ep in range(epochs):\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      loss = costfunct(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    tl,f1, acc, conf = testing(validationdata, costfunct, model)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model \n",
        "      bestf1 = f1\n",
        "      bestepoch = ep\n",
        "      conc = 0\n",
        "    else:\n",
        "      conc+=1\n",
        "      if conc == patience:\n",
        "        bestmodel.eval()\n",
        "        return bestmodel\n",
        "    \n",
        "    scheduler.step()\n",
        "  bestmodel.eval()\n",
        "  return bestmodel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omgB9Cvu6CzI"
      },
      "outputs": [],
      "source": [
        "class LeNet_with_batchnorm2d(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(LeNet_with_batchnorm2d, self).__init__()\n",
        "      self.convolution_layer = nn.Sequential(\n",
        "          nn.BatchNorm2d(1),\n",
        "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
        "          nn.CELU(),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "      )\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(1024, 1024),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(1024, 256),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(256,32),\n",
        "          nn.CELU(),\n",
        "          nn.Linear(32,4),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits =  self.linear_relu_stack(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rpm2Ihs6_Wl",
        "outputId": "604b602d-496c-4461-ab53-2bb4b7667ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is: 78.125 and f1 score is: 0.7836589813232422\n"
          ]
        }
      ],
      "source": [
        "costfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 60\n",
        "train_dataloader = DataLoader(training_data, batch_size=128,shuffle=True )\n",
        "model = LeNet_with_batchnorm2d().to(device)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate) \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
        "bestmodel = training_with_early_stopping(num_epochs, optimizer, train_dataloader, val_dataloader, costfun, model, scheduler,6)\n",
        "test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJAktzmh7wY4"
      },
      "outputs": [],
      "source": [
        "# step 1\n",
        "\n",
        "def infrence(dataloader, model):\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      predictions.append(labels_map[pred.argmax(1).item()])\n",
        "\n",
        "  return predictions\n",
        "  \n",
        "# test_dataloader =  DataLoader(test_data, batch_size=1 ,shuffle=False )\n",
        "\n",
        "# print(infrence(test_dataloader, bestmodel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fUSeNSnCL6K",
        "outputId": "4b14f3d5-27f6-4507-c5ca-5f9c6d4beec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100     3  100     3    0     0      3      0  0:00:01 --:--:--  0:00:01     3\n",
            "100     3  100     3    0     0      2      0  0:00:01  0:00:01 --:--:--     2\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 1794k  100 1794k    0     0  1024k      0  0:00:01  0:00:01 --:--:-- 1024k\n"
          ]
        }
      ],
      "source": [
        "# step 2\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n",
        "!sudo chmod a+rx /usr/local/bin/youtube-dl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-COPdBaC_Rr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "import youtube\n",
        "\n",
        "\n",
        "def ytimport(ytlink, gerne):\n",
        "  youtube.youtube_to_melgram(ytlink)\n",
        "  melgrams =  np.load('youtube_melgrams.npy')\n",
        "  os.remove(\"youtube_melgrams.npy\")\n",
        "  os.remove(\"temp.wav\")\n",
        "  labels = []\n",
        "  for m in melgrams:\n",
        "    labels.append(gerne)\n",
        "  melgrams = np.array(melgrams)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  yt_data = melgramdataset(melgrams, labels, opposite_labels_map, torch.tensor)\n",
        "  yt_dataloader =  DataLoader(yt_data, batch_size=1 ,shuffle=False )\n",
        "  return yt_dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "KrBh_OrBHck3",
        "outputId": "db285a6c-8759-4cfe-d795-5cc9a2731a0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classical\n",
            "Accuracy is: 100.0 and f1 score is: 1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = ytimport(\"https://www.youtube.com/watch?v=9E6b3swbnWg\", \"classical\")\n",
        "print(\"Classical\")\n",
        "test_loss, f1_, acc_, confmatrix = testing(data, costfun, bestmodel)\n",
        "plt.plot(infrence(data, bestmodel))\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "qkaC-a8RP_pr",
        "outputId": "2fcf1295-1d12-4e41-9d21-758fad50f20b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hiphop\n",
            "Accuracy is: 6.88622754491018 and f1 score is: 0.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = ytimport(\"https://www.youtube.com/watch?v=EDwb9jOVRtU\", \"hiphop\")\n",
        "print(\"hiphop\")\n",
        "test_loss, f1_, acc_, confmatrix = testing(data, costfun, bestmodel)\n",
        "plt.plot(infrence(data, bestmodel))\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "SkdA3HoLP_xf",
        "outputId": "db544aa8-29d6-44ac-97bb-09d7c299ee17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rock_metal_hardrock\n",
            "Accuracy is: 0.0 and f1 score is: 0.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = ytimport(\"https://www.youtube.com/watch?v=OMaycNcPsHI\", \"rock_metal_hardrock\")\n",
        "print(\"rock_metal_hardrock\")\n",
        "test_loss, f1_, acc_, confmatrix = testing(data, costfun, bestmodel)\n",
        "plt.plot(infrence(data, bestmodel))\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "HzhWEFHUP_7w",
        "outputId": "b1a01dcc-1150-4fd5-daa4-a0149f6cab40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blues\n",
            "Accuracy is: 1.5974440894568689 and f1 score is: 0.0\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD4CAYAAAD7CAEUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2debQkV33fv79670kgRkJoiZBBYkBSvICNQsaKBAr2cewQOHawYzDGNiaJTzgsxuH4EENMTHCOjbHBEOMFAjYGG5klbCK2BAgQCBBIGqGRNDDaJSQNWka7RtJo5nX98kfVrbvUr25Vv+m6ffv173POO/26upbbXffW7/7WS8wMRVEURRlKMe8GKIqiKIuFCg5FURRlKlRwKIqiKFOhgkNRFEWZChUciqIoylSszrsBKTjmmGN469at826GoijKwnDppZfexczHSp8theDYunUrtm/fPu9mKIqiLAxE9L2uz9RUpSiKokyFCg5FURRlKlRwKIqiKFOhgkNRFEWZChUciqIoylSMLjiIaCsR7RS2f4WIto19fUVRFGW2qMahKIqiTEUqwbFKRGcR0S4i+gQRHeZ+SER7nf9fREQfrP8/log+SUSX1H/Pqbf/BBHtqP8uI6LDE32PDXPPQ/tx7pW3zbsZijKYy26+Fzt335/kWnftfRSf23l78/6Wex7GV6/Zk+TaOVOWjI9vvwXrk3LeTfFIJTh+EMBfMfMPA3gAwKsHHvdnAN7FzD8O4BcB/HW9/fUAXsPMpwL41wAeCQ8kolcQ0XYi2r5nz/w74Gcu241XnfVtPPTo+ryboiiD+MN/2oU//cLVSa71qW/fileddSn2HZgAAD504U143UcvS3LtnLn81vvwO5+4AhffeM+8m+KRSnDcwszfqP//MIAzBx730wD+goh2APgsgCOIaAuAbwB4JxH9FoAjmbn1NGbm9zHzNmbeduyxYtZ8UtbLasYw0YWzlAXhQMlYL9P01/3rJZgBMzzWS8b6RMeK+f0PJLoPQ0lVciT81rH3j3H+LwCczsz7gv3fRkT/BOAFAL5BRM9j5qtm09RxMPed89I4FaUTZkaZaKJjxoe5Xpnw2jlTlvb3yIlUGseJRHRG/f+vAPh68PkdRPTDRFQA+AVn+xcAvNa8IaJT69eTmPlKZv5jAJcA+KHxmj4bzH3nlsxUlDxxNYCxmdQPSHM55vbschnh1j95kEpwXA3gNUS0C8ATALwn+PyNAP4RwIUAXA/ybwHYRkRXENF3Abyy3v46ItpJRFcAOADg3FFbPwOMwMhs4qAonTA4WX81M2o2rwmvnTO5TjhHN1Ux802QNYKfdPb5BIBPCMfeBeAlwvbXhttyhwNVXFFypyzTPbCsiQrNq44VK0jLzEzcmseRCDuTUpTFgGEf5GPTRJuaGbaaqgA4pru5tqKNCo5EMPuvipI7nPDp7Zqo6i35PS3ngH1u5PVjqOBIhB0OeXUARYmRqr82znFP49Cx0vhG59yOEBUcibDOvzk3RFEGUoXEprqWvWbqa+dMqRrHcqOmKmXRqMJx0zrHvXBcHSzWhJfZT6GCIxFqqlIWDUY6E0mokae8ds6oc3zZyXTmoChdMKfLpbAJgHac6FiBF2WWEyo4EhHacBUld9Kaquw1q1c/IXBZcX0+OaGCIxGaOa4sGklNVWFUlWnDko+X8PfIBRUciVj2AaAsHilNVdY5HpYeWW6sAM3rl1DBkQidQSmLRqVxJMrjEJzj1fvlHjC5fn8VHInI1VapKF2UzMlqJIW13NyaVctMrr5RFRypyNRWqShdpKwX1c4cD0uQLCt5+kZVcCRCVW9l0ZhHAmBzbacNy0yuicMqOBKhzj5F6aZVkkcHCgBNAFx6cq05oyhdpFy+1fhSQl9gbrb91OT6O6jgSESuKqeidJF06VihVpX7uqzkqoGp4EhEruWRFaULTuiaDjPFdbxU5FrjTgVHInTpWGXRKBM6x01UVRiGu+zjpVk6NrOfQQVHInItj6woXaQ1VTVXba7tvi4ruf4OKjgSoeGFyuIxD1OVvbb7sqxYk11eP4QKjkTYUgp5dQBF6SJlHoctq26vXb1f7vGiGseSo0vHKotGyeke211huLnZ9lOTaxi/Co5EqKlKWTQY6ZzTJo9Dixz65Jo4rIIjEap6K4tGSud4a+lYdXEAyHfCqYIjGWqqUhaLpEvHduRvLP14UVPVchOWVFCU3EnZVXXpWJlcfT0qOBKhmbDKopHWx6GmKolMK46o4EhFrmF1itLFXJeOVdMugLYGlgsqOBLBwn+KkjOMhEvHdmocyz1ecv3+KjgSkautUlG6qMqqp7lWe+lYHS9AvjW7VHCkQk1VyoKRR1n1JR8wmSYOq+BIhCY0KYtGynCOVh6HacOSDxd1ji85qnori0ZS53jj4whf01w/V8rSN93lggqORKizT1k0mBOG43ZoGss+XnLVvFRwJIJb/yhK3jDSdddWVJVpw5KPl1y/vwqORORarExRukhpqmqbqHS8APn6RlVwJEKXjlUWjaRLxwY+wFzDUFOjS8cuOZoJqywi6TLH6+tpkUOPXCtOqOBIhNbeURaJ1Kaisss7vuQjRpeOXXI0oUlZJFL311YCYNCOZUU1jiVHl45VFolyThqHlhzx0aVjlxw7k8qrAyiKhOmlyfM4tMihR67LMajgSESuKqeiSKTur6GGo+OlItffQQVHMtRUpSwOqWe6k7DkiGmHjhcA+WleKjgSoXHpyiKROqipZaIKfB3LivX9zLkhASo4EqGZsMoikTph1UZVqWbukqvmpYIjEbl2AEWRSG2qapdVVwEC5BskoIIjEXYA5NUBFEUiZR6HWxNLo6p8mu+f2c+ggiMRGpeuLBIp8zjcMaF5HD65+kZVcCQms/uvKCIpTasTRzq0M8eXfMBkmjisgiMRqnori4T7oBr74e3OpsNormUfLblW7FLBkQhVvZVFgqWH+Uj4gsM3VS27xhGa7nJBBUcitMihski43XTsh5Y7mdIihz6aOb7kqIlKWSS44/8x8Hwcraiq5SbX76+CIxG5zhwURSKlqcq7VrD+xLKPl1wtFSo4EqFLxyqLhG8+GrfPuhpHE35amvfLPV506dglR2dQyiLhCovxnePOdYOLLft4sT6fvH4IFRyJUJutslB4D/NxLyVpFba223KPmOZ3yOxnUMGRCE1oUhYJ3zmePo9DK/RU5DrhVMGRCF06VlkkxKS8kfB9HFpyxEWXjl1yNHNcWSSS5nGU7evqeKnI1TeqgiMRmtCkLBIp8zg87SZ8XfLxkmsYvwqOVOhCTsoCkTKPY+Jdy59h63ipyE3zUsGRiFzLIyuKBCdUOSQhpUvHVuTq61HBkYhcbZWKIpHSxzFxfRzhyoNLPl7UVLXkqOqtLBJeAuDI1xLDcTWPA0C7BEsuqOBIRK5hdYoiEcvmnjViyZGg9MiyUqrGsdzkmgGqKBIsRDqNdy3n/8akq8EkgBY5VGpy6wCKIuH20tF9HJHMcR0veQpQFRyJUB+HskikjKqS8jh06dgKdY4vObmG1SmKREpTVekt5OSH4S67xqFLxy45qnori4SncIyucbSvpZnjFblaKlRwJEIFhrJIpM3j0MzxLnLNZ1HBkQidQSmLRMo8DskspgmzFbkWe1TBkQhdOlZZJPyKtemiqlp5HEs+XhqfT2b5LCo4EqFx6coiMfelY9VUBaCtgeWCCo5EqKlKWSS8pLyxBYcQahgmAi4ruSYOq+BIRK62SkXpYy5Lx+owAdBenyQXVHAkQpeOVRaJfJaOXe4Bk2uNOxUcici15oyiSCRdOtarVRW8LvlwUVOVAiC/DqAoEgkrjkRNVcs+XtRUteRoVJWySCRdOtZNAAQnLXeSPZlaKlRwJELj0pVFwg90SuscT2kmy51ca9yp4EiEZsIqi0U7KW8sfMERxHAt+XjJtfSKCo5E5NoBFEUibR6Hfy3fVLXcIybXfBYVHInINaxOUST8SKe0JUdKz1Q16qWzR5eOXXrUVKUsDimd46GGkbLcSe7kmjisgiMRuXYARZFwe+n4ZdWd6wbOcR0veU44VXAkQhOalEUiqY8jEn677OMl13yWgxYcRLSViHYK2/8XEf10z7EfJKIXHWwbFoFcw+oURSKlLy6MqgrfLzO5ll5ZHevEzPzmsc69iKipSlkkvMzxhNVx26aq5WazZ46vENH7ieg7RPQFInqsq00Q0U1E9CdEdCURXUxEJzvHPpeILiSiG5z9iYjeTkQ762NeUm//SSK6gIj+iYiuJqL3ElG25rY7H9yHt3/+KpSlkw1bv+zcfT8++I0bo8d/buft+OJ37/C2feySm7H9pnsAAH/7jRvxne/f33zGzHjXeddg932PtM71yP4J3nrOLuw7MLHte2Af3vH5q72Be80dD+J9F1zfOv6b19+NT156K8777h141Ycvxae+fWvz2dk7duNr1+6JfheX9UmJN336Srzuo5fhjgf2eZ/d/8gB/NE5u3BgktnKNZuQS266B68+61L8zderfvi1a/fg7B278aVdd+Dcnbc1+3XNdv/m6zdi120PNO+ZGe887xrccs/D+N1PX4lXffhS/PbHduDuvY9G2zFxTn/V7Q/ir79mx8X2m+7Fxy65eSNfb2oe2HcAbz1nF/av59P3OFPJMauH7ikA/pKZnw7gPgC/KOxzPzP/KIC/APC/ne3HAzgTwM8CeFu97T8AOBXAMwH8NIC3E9Hx9WenAXgtgB8BcFK9bwsiegURbSei7Xv2DH+ozZKvXr0Hf3n+9bj13kdaM4ezd+zG2z53VfT493/tBnwgEC7vOu9afOySWwAAbzv3Kpy94/vNZ3c/tB9/9qVrW8IGAC67+V6874IbcNnN9zXbvnLNHvzF+dd5guYfr7gNbz3nqpaJ4CMX34x3f/lafOTim3HuztvxoW9+r/nsPV+5Hn/nvO/jlnsfwVkX3YzP7Pg+LrrxHu+zi264G//nghtw1W0PDj6fsjE+c9lunHPl7XjPV64DAPz9N7+Hvzr/enzgGzfiw9+yD+uuZ9bbzt3l9b87H3wU7/7StfjghTfhHy66Gd+64W586rLduPR790bb4fa1L+66A+/64jXN+89e/n2887xrpMNmzsU33IP3XXCDJwznjX1u5CU5ZiU4bmTmHfX/lwLYKuzzEef1DGf7Z5i5ZObvAjiu3nYmgI8w84SZ7wDwVQA/Xn92MTPfwMyT+lxnSg1i5vcx8zZm3nbsscdu+IsdDK59slk6tp7dT8p+f8ek5NZsb8LslS9xtQXzvzRDNLHy7iCV9rfb2seXjv3ZPc+k5Kls0TEbdq423c2IvcdUv6/u7yS4+V33Nrzv5rj1Wlt86WknBteRCa/X3c5xmWTY95qlY/NpEoDZCQ5XF51A9p1wx//usTTgWq3AiwHHzAVjbZmwHWCmseFDX6LaJ9jmCJNJyV7y1MTZ3m4Le/t07d91jrKs2tKcJzimb/CH5wrbZd/77VDGI5w4mP4U9jnpTnA9gZkI9/JA/bq6Unjn72xHz63uGyezIjbxmhe5LseQ0j/wEuf1mz37fg3AS4hohYiOBfBcABfXn51GRE+tfRsvAfD1UVo7A9zZeRiO687eY8eH+7jbysCRGMsytddt7+9vkwePua50HubpZkSxzGBJo1HGob1okrmX/RpHVz8ArMZxyIrVZIa0o+iYNqZ6kEvjYd40JUfm3I6Q0aKqBJ5ARFeg0jBe2rPvp1GZsy5H9Zv9DjPfTkQ/BOASVH6SkwGcX++bJXYm146qqsxQ1aAkkkfMpGzPvCdlNbsvhZl/s00yVZnZVI9pq2vWNSnttcPzSCa1GFKbm/fOb6aMS6hdGjNV2H+kWyuZdcy29YmvcfSaooyGUhTYL9z4abTZgyGmsc+LXPM4DlpwMPNNAJ7hvH9Hx65vZ+Y3BMf+x+D9lvqVAfy3+i/kAWb+2YNocjLch3C4dKyrHXTIjcYc4G/zZ4XeQz9iox26f9esqzFNdJxnGsEhtWHId1BmS/hQMoKjde+FY2P9yTz8V2sVou9Wmj61UlBl6O5o59hwhn0vVw0821DWzYAJM5yUrqnKn7HHbPmuZmHP6c8KvRmfoA0YXDt2uH/fNnPd0vHVeIXpyil9HMIs1V7fnlMZl/BeG2d3+NtLXdT4QSTt0Wgca0M1jvrjLlNVKn+XHT9JLjcIa6nIiySmKmbeOqPzfAXAV2ZxrhSYgcSM5s6bDuCqxWsr8vGS0zl0YHoDN2LmaR7IwgzR95PIwseYo0RTleBQjeH5OAQnfLiPMg7h5KMs7QTBJRal596nxlRVd4bV2sfR9+AvS8ZKQRGTbVrBkVNghp1wzrUZLVTjGJHSmZ23TVXth3aI7Kj0yzLIzu6Yqaq9vyR8pOu6ZozwPBv2cQSHub+ZMi6haabkdqRUtV/7WGmCYSYPB4zGURTedboomVFQzGQb/x6zIhybWdBMODNqE1RwjIqrVZjbHg64XlNVaMoJTVWCmSkmOCSzlG/u8tvu7usKstBEMc2DXor9b95naGferLRMVbWZKvzppYeWGFjRpXH0aKMTZhREnbH4yUxVHdr2PHG1wZxQwTEi1vko5XGY1+5OamaA0jaOmJ6GOsclZ2DTTuG6riDj4LrTjDVJw7Hv69eMBu9mJdQejebYelCLGkf1OhH634Egqqo3HLesBUeHypEuHDc/MykHr7mggmNEPKdjvY2DARd7QJaBc7wSQP7gduv8WL+H1JZ6H0m7KN392hqFOa4SHn77zb7TPOilh439DvL1ldkThmabIIcuYe7SFZYN2DyOtWJoHgdqH4f8uTHPjk2OfY+FiVoOqOAYESlRL+wIsT4a+g5cLSUWDil1MknljYfjth8eblSV227JFxPD3bXLx5HR2N20SCHVzO3JjGSqkvqB+X+9DDSOnps5KRlEQNElOZCmP5hr5PSQtm2abztCVHCMiGQHbjSOAbObSWA2cI8ppRmf4H8I2yLN9r2ZZ4dN1TjGu6KqppmlSX6W8L36OMbH1VZd31l4K6VbYTXemKnKRFXF28FcR1XF2ppAcmQdVZWZsUoFx4iEMzDAdoAhs5sqpt49nz1W0gxiWox0PTnSSn5wc+DjCGer04w1qc32fXsfZRzCdcUbM6hgpgwR+1PLVDUsqqpxjkckR4r+kKW22+FznDcpS44sHVYDsE//5sG4gagqV0Ow53b39/eT29LeX9RqgnOE2owfVTXdLE3ys3RdRxmPUPMz+TgtYS4cK/kDbB5HoHH03MuSkZfgyKjvcfCaCyo4RqQJT5y4D3//sw37OEozE5E0iPZJYxFUkimtVe48iNH3/RRTlhwRtKjwfW4zrM2I5OMYHlXV7r/mMLMI11pT5LCnHSV3Zo3b68U/nwVDIh1TExvT80RNVSMi+jhMkcNA8+g6Xox4cgRKWPrD3c9Fztloz7CsRhG0pZlNlq3rTlijqhYRT4CXVpNt+TgEyRGPqqo1jmJoWXWORlW55x6TIX7H1DQ/XT5NAqCCY1QkH0eTCTpgJuFGMbnHuMUPpaiWmI/Dd4RL55DbFWpPbgRXZRvv/BotJC2p3abMRsompK1x2AoB3n5CeLd0n6xz3E8A7I+qqk1VEfd4ikinrhymedJYAObbjBZqqhoRaQbDkc9ax3N3VFVXdFO4zWB9Ks42wZ8gbXPfm4dC2fhTIO4fQ/KzhNdRwTE+odYY5mEYpDthNd72NmuqKlrXkWBmFEX8gZ1G46hfM+p7jcKRUZsA1ThGhYNZOtCe0ff6ODo0BEljGZI5Ls32/W2m7eF3qV7Xy7D97XP0EY+q6v9dlNkQappNfy3le+IfK/Sd+sFrjl8phvk4bFTVvPM48pu05DoeVHCMiBTrbv4tIxFQhrLk1uA226Xoo+hCTsL+UgRVX1SVEYKtKKsNCo6uWlU52Zk3K2GBwvAeG6Q7IZbkD44viFBQv6mqZGAl5uDAEkdVNaaqfNoEqOAYFcmMw81n/bObMFrJPcYIHj+6qb2tuS77+7j/T+PjOFBfmNn6N8Jz9CFFirW/Q14DZTMi+TgAe48N8aVj29qjOX6FCCsF9TvHS0bR4xxf1jyOZunYjNoEqOAYlSYhqmwPrr6ZdbPiXoeGIEVVxfwmku9Ayj7vmnV1CaqNVBSdxMJxM4xs2ayEfctbP8ZBemhJix5Ngj5SFJXW0bseB1fhuLGSI0l9HBn1vUbjyKdJAFRwjErzIHeelNZU1R54LmLUirM6npiIJwiTVlskZ/oA4dMljGZuqmoE3ODTKRskzN/puoeiqUrqO8HxlamKBtWq6k0ATFBWPEcfx0Z8iClQwTEijanKuedhyZGuThovBzLc2d1cN+JMlxZWapuQ2ud0HarTaByShtO+fl4DZTPi3rJYLo5s+mxPGMJ+t1IYU1V/O2LrcVT7JDBVZRjRx8FrLqjgGBH7YG5rHH0z9ZiG4JYiGWyqEh7wkmmia/B0CY6NaAiSMAzf5zR4NyuhAO+6h2IJmyAcu9rm70f1qn79JUfiS8cCaUJkpbI8c0dNVcuHzbZuz7D7VFB3c1iK3UsALNvHSONUdoRXr2LF3A5NAABWnTDLjWgI5norBbVMEGWOg3eTYh7YgIngG26qknxhYb8zzvGDXToWSJUAaNuTC7lOpFRwjIj1cfi2ZKD/ASlrEvazoSVEDFKormyn9s8Vnhvwi9d1JQzGMA+B1aLtONUEwHRMmL1JQCxQI0RcOjY43vg4+rSFiYmqiu4TPcVMiI2feaGmqiWkMUu5duD6daiPQ9rX9XFMmwAohU+Gqwy6bZfa45bLLjv2j2EG6NpK0e3jyGjwblbK0snuLqctjY/mOLstEBxF7RzvOS8zBiQAjt8fsgzHVVPV8iGWHAkejF0PSMmB7M6ISiF0UCoDYdvSPq/kn+hcOrZD49hYVJU9TzvsN7/Bu1kpmW09KRaq4jr7hUyECUM7qgqDEgAnJWOlThaM7TM2sajEecGZ6hwqOEZE8nGE0U3dDsn2eVwzlzTTN8dEl44V9pejqmRNAHCWBOWN2YXNg2S1KDqjt3IavJuVktmrYNvp4xA2S/ep5eMohiUATrhaOjZe5DB6iplgc5Xy6XthTbhcUMExIpL93/wnlSNxiZUGMYvuhMfHoqrE0NuInToc7O511gpnlupEZg0dcObcayuCj6MnaECZHZPSrpkRM1WJgmMaH0ePf6JZOjamcSToDzmXu8ltPKjgGBFpRm/9CmafrlletxbgPqRFn0VE4+hb+KlLE3KPsxpHWBJF/CotzABdXWlH3LDwmynjwI6pKixs6O0nbJN8US0fB1Fd9bbfVBXLGpfOPQZ5+jjqcTvndoSo4BiRWFn1rtIezbGSFuBoHtZn0b6elGUrLhPbCKT2daeNqpKO6cIcslYUmjk+RybMTaDDgYhaIOdxtO9TeC9NGZH+kiPWkd65TwofR4YRfaYlGTUJgAqOUbE+jtLZ5n82jY/DPVbWFtDaZpDKM7MgJLp8Fu5xNqpKbmcf5nqrK+2Im1zj1jcjZeloHFJEhUH4KNZ3DCsFYWVAVNWQPI4UE4kcI/pyHQ8qOEZE9HE029D6zDvW9XG0oqoc7UOMqorMEAXfCQtaQ8x2LUVVhcfEMPutFkU7qqpHE1NmR8l2edf1SDEocelYwfQZ9juq60/1l1WvoqrmvnRshoEZTVPyaRIAFRyj0ixsI8zmwmzw1rGe78I/xi0PEdNM/PO1zyslIdoHt9wW4OB9HKWncQSCI8NV2DYrJXPjHD8Q0TjkqKrqtWv9eJORPiiqqqyFjC4d2yJTuaGCY0wkfwEHA25DUVVso6p8H0P1KmaOS0JCeEjHMswNblSV5GPpY8K2NlH4vNpImXZlY0xKbhIAY6YqcSJSSv3J7mhyMqqoqvi9rKKqEM/jSBFVFYlKnBfhpDEXVHCMiLQUZ2iK2YiPg7kjHHKDUVXy0rHdtmubNCZHdfVRcl3HiNrH5LpU5maEGU5U1XSmqljfAezaGkMyx5uoqrkvHWte8+l8uY4HFRwjImVVW+dz/drl4/C0gPa+68LsKLYIkrhMrKARSRVCw/O5ZSqkPJI+yrJK+JJmoxvJRFc2xoStxjGtqSq27DAwnamq5P5aVUsfVZWZsUoFx4g0dmC3yCH8zjmsVlV7XxM+6R7uaiQhocBy/5ciraTrG9YcH0fXjDNGWZuqCuGhImk8yjhUmePGxxHROMQovfocQuAH4Goc/Q/iktG/kFOC/iD59+ZNjn4XQAXHqMglR6rXPntq1zochvV6oIv7DYyqkmZYclSVf66momopO9b7mJTVg2KFYmXVMxspmwyzXvxq4+OImaraWB9bexvg+DiKfh9HFVWVwdKxZmxm9JTm4DUXVHCMiBUOwtKxfRqH4HT0NY62kIgVThTNUpEQXUkgGRpTFbN4vj5M3H5RtM+tCYBpsBWKNxZVJZf1t58XhevjiN/MZunYyD5JNI7I+JkXdrG1fNoEqOAYFRuyaLe1l47tOrZtAnL3Nc7MvhIizXUjJqiYD0Y6n1tRVbp+H8amLT1UNI8jDebnHZTHEelPUo4SUAU/mNc+0w9zJWjmnwAYn8zNg0bjyKdJAFRwjEoZ0Th6TVWC+cjdt9E4pjVViVFV7etKJi1DU1E1cI4PtQ2bhK+CpLLqftuUcWiSMAdoHFIXdYtbhtsANGtrEPWbfkxZ9ZjOkcRU1Tj8R7/UcNh7yQYVHCPS+Dgm/sPaD3WVu0SfJmHO6Tu26/2FB7icMFi9StnfscS+NS8c1z3fsO5tEr6qiJuwnRkO3k1IU6G46M/jkD6x2rTcT2pr5vClY4u+kiPjd4jQjJwDOWpBgAqOURETAIP33c5x+78oONz6V4FGEsvj6C+r7rc9/B8IihxGfCFduAlfrXDcTAfKZsOtFwZMb6qKVVYG/DyOIc7xHKKqckw+VVPVEtKYpQIfgjTrDxF9HM7YPjBpD9ghCYB9Yb6S+arLVOUuHWveD8E4Q2Ufh3xNZbaYn3dIHod8vO0nkv+sERyCVim1pegxVaUIkc1xdp9RUzxUcIxIV1n1WI5Es12KdPJMVXYkhTMlKbJSjIJpjkNrW6x4YbP4TyuqSvwqLcyDQhQcGSZhbd7weikAABxrSURBVEbsKoymOu60ZdXdz802R3DUT5YheRyTMo+FnGLjZ164iX85RVap4BgRyccRztK7EwDt/1JBRCk3JJY8J2ktoVnKxPaH7QpnjO7SsRvxcRibdtTHkdHg3YxY57iJquq+d9JtlXN/7OdeVNUAHwdRfHad4qGZY/LpEOvEPFDBMSJSCRDm0MchHytHVdnP3UxfqQhiZ1sipiqvk0b8ME2Rw3JjCzmZqCqp5LZU6l2ZPaYf2DyO6RIApZX//CKHJqqqf+nYso6qipXVSDGRyLLcjTCBzAEVHCMiRZ4weNAsXc7jcE1Vkl+iPYC79nG3yQUT7bFhh+0qqz60Xxsfh1THSPILKbPH/LyrQ6KqRI3D/b/df4qmVlX/A2/C3OsLSVpyJKOu55mq5tiOEBUcI9JVVj2WIyFtF2tVeVFV8K4jxt0LduimDLv02cCoqliGeRfMdqnQVuZ4hoN3MxJGVR2IeJ9FH4cwyRBLjgyKqqo1lFwER0adz/3aOc2lVHCMiC2rXg3KylEY9x9I26UOLWkckmYStkXSJKyPo33N6rr+uUzsf8ndx8SoNI66HEVHraqcBu9mpMnjGLJ0rIC0aqR7+/2oqh7BUfeHmKkqSXXcyPiZF6Y8j/k/F1RwjEhjM60HpUmGis3mw2O98zi7unH37agqYYYYiaoKBYh7rvAYwC85IuWF9GHi9qWIG42qSoPpPhstOSIvh+xqHMPX4yiZ67HRvU+KhNDY+JkXDFuiPidUcIxIqMKbMgy++il3UmkhJXdbLI8jZpMuBeEglyNxjw0Fx0EuHcvdPg7JbKbMnlDj2OjSsYDcf5r1OHrCcasow2ps9O03NrHxMy+Y5efGvFHBMSLhg7kgKaqqQ+MQo6pcU5Xg4xBsze22ONs4eO1oV1dU1WTDUVWwS8cGE13J7KHMHtNHhpRVl25rX/BGUx23p6y6OaSKqoq0N8FEom8559QYYWlCm3NazEkFx4g0eRyl7QBDZ+ly1IrdttHM8XhUld0/tkCTyTZmjkdfdWF8HFLEjWQ2U2aP+Z3XhuRxSEvHiuG49nPXOR43QfmTqi5SKKC5FdhshKoJf8+jWQBUcIxKM4OpH/JFUc2qvCS8jk7aG1Ul5HGUzmw9fCBH1+OQPhMEjOFgo6qqBMBqIafuqKqMRskmxHSfQXkcwq2Q+oe7zU0AjGkL5tiipxjiMkZVmVaYjHrN41gSjIAws7lq9jVsnW4pbNYzVQmCJWZeiiUAGuXFNzvJbQH8hZw2mgBYFbWT1hz326uMQxOOu9HquNIERHKOC4t1+eex+8/dVJVZAqARFEVjqsoHFRwjEj7QjTo+rORIXOPwfBzsv4b/u+9Z2EcqUhdboMnUN2L2q+MOHW9lWc1EpUgaKWxYmT225Ehc46hKgUimT/s/N/3PERxNraphmkRltuxv75hYX9/olxqEaY8xVXEm7QJUcIyKmbmYUMeVxlTlPJS7EgAjvgjA93HEFmkK9xHDfCOfVdf129ZoHKUfJjl0VjjhqjZRrKy6RlWNS/PApnhUVZePQgzecPYbWlbd+jjiJUfS5HHkZSY1v0ehzvHlwS8YWL2a2VdMM5C2S87xdW9VwX7BEe4jaRSSNiKd62CXjq3W4yAxOUzzONLgzmZXCurM4yDIDyy5/9htTThuTykRdkxVsZl+Eud4boKjeW7473NABcdISB3d2HGHOJRFH4dnqmqfI+bjiCUJhiVLqvZ3t7FZOpa585gYpox2dD2OnEbJJsTcN6JK6+jycXRVrZXue9fSsTFtwYuqirU3QX/oW855XqiPY4mQHnwmU7rLl+DSt3SsVKuqS2Nw30v+ECmSKdbGZunYcpj2FDLheulYIatYeggps6fJEajXweiqVUWQNQY/AdBsczQOMq/xxL7Sace8o6rMJXKZs7i/jfs+B1RwjIT04CtqZ/CQSCRxiVcvATA+4wtneWFUVZ/zPVb6fbWJqhr2XUKYGStdPg5NAEyCDdioTVUxjWNDeRxOAmA0qspoPvGyGil8HPlFVVWvJtAgk2YBUMExGtJNXmkEh902zMfR3hZLAAz/d9+L/hAxAbB9rKErqmq6IofUZBdL5VVyGbybFdfvVhB1OsepcnIIx7cnDGLmeE+tKtfXEtdMus8xK7LzcdSv6hxfIqQHn+kAQwoDSj4GP49DSACM+CViEVTm3+6oqtBUVTT7xBIFuyjZllVvXTezwbtZaRLvas2v2zkuP9BdOSMJe5s5PszHsdKXYZ4yqiqTsNew5EhGckMFx1jEfBxDIpEkH4O3dOyk/eDv0hjc96KPoyecNxyzNqoq0IyGrjlempIj7VIKuQ3ezYprPy/6TFV9GocQ9edHVQ0xVcWfiymXjs1l0tJohVpyZHmQZlkmJt6fYcvHS5nb7ozugJcA2NYi+qKq+mzUMe1lzYmqKgVtoY+yDsc1EynXjMaZDd7Nio2qqoIUuhIAuzK6+5aOtVFVlamq68E/1Dm+lFFVjTnRvM2kXVDBMRpS36sSAA9m6Vj7uVdypLQP3q7zxgohytvsse2lY+U1xwcXOWRuHlj+9Z19chm8mxRzq0yV4q4ih4R+jUPKvXFrVbnXa5+neu0rhjh2d4gV9ZwXrQTATNoFqOAYDenBR9SOqhq0dKygJUh+AX8hpvB8kX0FG3VMe2mKHHJYd0v8Ki2YqwdK6OOQ/C7KOLhlcFaKiKDuWE9DKs/vL+Tkv/blK5kCoF2MHVW1kXyksWmiqjSPY3mQZt8rhfEL9HdS0e7fsW9j3olU3Q1n9V7UVI/G0vJxzGrp2MB2O+R3UWaDdY5bAS7R9Unv0rHOehzu9brbEddYx+4PQ6wAqWnlcWQ0m1LBMRLdUVVBKfKOWbq0vGznvkOiqgI7tJz5i9Y29xj7PWy0zEZmaiVzHVXln18FRzqGCo6ucudu/5LWUHFrVQHdwQ5uzazYHR+qzW6UjeQjjY1pRYYrx6rgGAup7xk7rjsON7p0rH+t9oOXg/9Dp7McFSObusKHeFHYZV+HmN1a7S3t0rF+m+w+uQzezYqbP1FEngIdaRz+vRL6T+PjKND6zMV10scmC2NHVZXCeJs3ph26dOwS4YYZGkw9HrdOUMz2a451o6rc84VRSf4xTlvY7m8iXIxgqGoJtdssaTzm3MY/MamjqpprDuzYZejjcNpvrpPTINmMmN96pXDyBOD31+q9/EB373tjanS2uQmAQHc/d530Xfc8Nk5mhTcmM5m0GOd4U1Y9Iy+HCo6RMJ1vzZnOmVm6GQRrRdFdcoS5OdZ1Hrvns59X70tG6xipLSXDa0MjeNxtgnPeHG/MG2WdANicd+CAmzCjKNpmDPc6uQzezUrZTAZ8U9VaoH50RVW5fdEtZ2P7COrXuH0+XKtGIkV/MH1wrSiyqVrQCNUm+nCOjQlQwTESZmCaCCTARlWx81lXH2V2a9Rws809n1ve3LyG26S2uCam1RVqmYrcbeF2oGqXKZddcrsdfTAbU5XfPqlNyji4voWiaPcpA3XlcQh9rSzt8W4CYLVPvB2xpWNj42RWlN6YzKPvWVOVeZ9HuwAVHKPRdERnUK6YpWPr2c1q0b3IzaTkZpU815Tjnm81iLZwPxcFR2HDX80xVRvgnWe1IHGBplXH/GBU+tK55tCZmqlVZWy3YYLjak+2sXLwmHteaY92+2phfU9APHPc7U/hNusch7ePdB6zf9cdj42TWeH2vVy03fZCTvmggmMkTN8zdZ2AaqbObAfL2krR+YAs61l5QX64qns+878bYhtuk9rC7G8LCx+628LtgLvsK3ttGvqsL7leQChw+rHTpkzG7qbFzvThm6pWCk+QVKYqwcch9DW3L7jVcdFxDvfYWK2q2DiZFf6YHPVSg3EDGADVOJaCZpbuqP5mVjXEJFOW3BQCdMNVJcHhaiRuAcKwLe7+bvtC5/Tqij/rCk1d1bKvVJ8H4jVjGCeqMcX1XV+ZPaZPheG4leDo1zgmZdtUNXFMVaGPozcBkLqdvylMl42PY2U6f92YcCPcNapqabDmIUfjIDtLN591235NVq/vg5B8HK7GEG5z/xd9HI4jvPFlBO1ytxdkHaomQmtaH0fJ7EVVtXwchdFgMhopmwx3NuuaplZX/Pdda4Ezs7cSZLit8XH0OHZdH0fXPrFxMiskP+C8MU2wtaryQQXHSNgZjOPjKMzSsfazWFSVWV61WTq27NA4BI0kpnG4NabWVuyA9baJx5NnuzZRVdNmtlZRVY7gMO0PfjPVOsbDDbF2TVMrha+BdEZVMbfuk7vNXToW6O4bpeNr6TZVJfBxCFp5LvTV+5oHKjhGws5gfI3Dj2gqOmfVzFzP7IOoKck53jz429vc/93PbKRU0Z7xBzZldrYbtdkmAFqfx9CxXWlT1Iq4CX8zlRvj4a714N7TlcBZbqrbhlQRVL5vqyy52Wa6fd+yp7asBtA1pw774xhwMEZyeEi72pj7PgdUcIyEG6VhMLHqfvRSt+3XDGo/3NbestXAHjtxBq4cjusswORFVQU+hqBdbhSYa7uelI6/YookLbMeRxhxE/5mOQ2UzYa7dKyrRVa+Jz8kV1w6VoyqghBVFdcerebTrXGkiLKbCGNk3rRMVfNvUoMKjpEwM7q1QONg+BFN3TV8ah+Hk7lblr7pay2Mo3dMBZLgWHP8H277pKgmt5O6ESdGba4ixCrtyZg3pvJxFG0fR/ibqeAYD9MHCye6zWiBbiZ5UaBz6djwPsWiqrqew67m03W/Y+NkVoRjJIe+Z1qgS8cuEc0sPfBxgN3ZTX9UFbk+jg5TlVvAMJwFuv+7+08bVWVmou5M0pzHzSTvg2vzlpuxHBZZXFUfx+jYqCrrhzD3kRzBQZDzK+SoKnaiqvzoqu5aVfbaXXfb7aNjUQZjJIcVKDWqaglpZjBuVFWtcrPzWV8ex0rhLx0rmqpcH4eQU9HYb539G62naPs4wnbZKCjbiVfIZo6bmeqQ57w5rfGL+O33fzOVG+NhfQvkZXm7Gf2AqW/WvhHM7fvkbmt8HBSfwbv5JJ3O8aLbFzgrytYYmX/nc3NcABUcS0HpzN4NYZHDWL7ChOFEVdkZnWiqEqJa5Kgqu79fYsGancw2P6qqFg6O45TInqcohheHmzQPLDvTbflYgu+lzB5zH9wgBZNbMzSqKtQM3W3u0rHuPiFuPkms5Ei6qKp43klarFZYvcuhTRUqOEYinMEAbh6H/ayrfzbmISJPo1hxNJhYHL1ccsRZK1yIYPIjrWxbmG1RQnd2as7jZpL3/y7VPuScy63F5bcpn4Gy2bCOV2uaslFVYR5HGzdQw88jCvI4eswsbj5J1z7TROxtFGmMzBv3Hrnvc0AFx0jYSrPtQWirwHbbbssmqsp3PrrRSK04+nKYc9yNqjLtC7eFy8gWdVvMQ8ZoQlUpd1tmvQ9jO3ad42FUVdOmnEbKJsNbOtbxcRAF63NQR8mR0u87Zs0Xs81G3/nXa7XD0Xy67rabBDsWLed4Dj6O+rXxccyvKS1UcIxEmIkKoPZX+J/1+jjID8d1fQM2w9oc4y/ras8Fb/+S3dm9FTR+1V72jjczUTcCx5zHRlUN/10KckpuBz4Om/3efz5lY7g+DjfgIYyq6jJVuVUKfG01cI73hFaz0x+6NNZpIvY2SnuMzL/zNfeox080D1RwjIT1cfimKsCPGe+a2UxKG8VkNYpqxm9m/auOPTb0qYhRVY6QmATtC7dJUVWhI9UcY7SgIT4JybZuw439NmlU1XiY/BtyEgDNffRrVcklR8KcobA/tUuOdGgcpRVgXbfbrZ4wFuEYyaHvNaaqwn+fAyo4RsL0u0O86rh1p6xrlh8SyYhlY5ZybL/M7K3YdshK22dxiOAfkD4Lt01K9trsayxV3kUltOrvQvY8NqpqiKnKCo4wVDP8zXIYvJsVEw0HWHOS0T7cBECTtCod7/cdvz+11hzvuJVuImKX8zemjcwKDtqfw0M69HHkZKxSwTESYe4EYB/4626GdsT2a8wGrg/DnfW70UdubggQCA4hP8I6A11zg90W1qqq2mJnkkXddjeZb8is0HWGNoI0jKrKMG59szGpzZ6Ac09FU1XH0rHMnX0HsJOkpgJyRz+3s+q4czxVVNVqRlFVraVj59+khg0JDiJ6CxG9flaNIKILc2jHLJFrVVWvbieNqfBmht8k6DF7s35r0nGFg93WnCuMoCrdZDtbtsQzVQUai3GAuzNJa6rynfgxXKdsZ3XcDAvNbTbc9cGbgIfCOMjtftShcfSZqtzSNOZ6EtZ02T2fHhp4cTDkXXIkrrXNgyw0DmZ+9rzbMGvCKA3ADtD10voMujoDM5oZvhuu6jvH7YPXJs9Foqqk/b1S62i2hRqLqZvVmDWIUJa1yaOgaMkI/3vVDwpnZtusOS60SRkHoykCVhM299gtqw7ID3SvvE3p9x3vnD0lR1wHcKdzPKKNzApz/rVmdj//vmeaYAR5Dm0yrA7ZiYh+HcDrUfWhKwBc73z2XwC8AsAhAK4D8DJmfpiIXgzgfwKYALifmZ9LRE8H8Lf1vgWAX2Tma4loLzNvqc/3BgC/BqAEcC4zv7HrGgf/9eP83J9/HfsOTDZ07IP71gH463GYQfTRS25uZu/3PbwfP/POr7aOv/XeR/D0HzgCBREuvP5u/Mw7v4pb730Yz3zy44MYfODvv/U9/L8rvl9dr54xvePzV+P9F9wAAHik/g7ms9f8w7exPjGmhWrbi9/7Tdz/yIFm274DZdOu2x/Yh8MPXfVi/ImAy265F8zAGScdjYIIX951p/hdXNYbjcPObH/v7J34489dhYf3T7w2/ecPXuL5iJTZsWfvoy0fB1FljgzzOC687q7Wfd376Hpzn/72wpvw6R27Adh7F577DZ+8AocdstJqx311nyMiPHZtBQcm6619CgJuvffh3r51MDR9r+5vv/Gh7Th0db59b9961SYjhH/745eLv2GMJxx2CD7+yjNm3rZewVE/7P8HgGcz811EdBSA33J2+RQzv7/e9w8A/AaAPwfwZgDPY+bdRHRkve8rAfwZM59FRIcA8H4FIno+gBcC+Fe18Dmq5xqxdr8ClbDBiSee2Pc1RU469nHYfxDhHE847BC87Iyn4O6HHsUxWw7FL207AXc++CgmZYkfPO4InHnK0bh7737RKXjKcVvwvKc/EesTxpeuuqPZ9qJ/eQKe8aTH47Kb78PPPfMHcORha7jmjgcBAM/4gcfjpaedgEnJuPPBfd75Tn/a0Xj5GVtx/yMH8PD+9aZ9v3b6ibj7oUdxoP6eW49+HJ7/jONxx4P7mhnOKcdtwWlbj8Ipxx2Ou/Y+CgB42RlPwdFXHgIA+PlTn4S79u7H16/bM+h3eeaTH48zTz4G/+yIQ/HS005oBBYAPOfko/HyZz8F9z28vxk4yuw55bgt+NEnVcPyxdtOwP5JieecfAyedORj8dCjE6ytVMEQ9z28H+dffWfr+H/+xMPxc888HlsOXcF1e/YCqPrfr55+Ih536Ap+8gePBQD88PFH4Je2PRl7H20LBMMTj3gsjtlyCD716mfjy1fdieecfAwuufEePPOEI7Hrtgdx4lGHNf1zTJ5z8tH49TOegnsfyqfv/fjWo/CyM56C+x85gIf2d/+GXRzxmLURWgVQn/pDRK8F8ERmfpOz7S0A9jLzO4joJwD8AYAjAWwB8HlmfiURvRfASQA+jurBfzcR/QqANwH4u3rbtfX59jLzFiL6UwBXGSHhXK/rGk07Yt9h27ZtvH379oE/iaIoikJElzLzNumzWehiHwTwm8z8owB+H8BjAICZX4lKUzkBwKVEdDQz/wOAfw/gEQDnENFPHcw1FEVRlPQMERxfBvBiIjoaABzzkeFwALcR0RqAXzUbiegkZr6Imd8MYA+AE4joaQBuYOZ3AzgbwI8F5zoPwH8iosOCa4nXUBRFUdLT6+Ng5u8Q0R8C+CoRTQBcBuAmZ5ffA3ARKuFwEaqHPAC8nYhOQVW14EsALgfwBgAvI6IDAG4H8NbgWp8jolMBbCei/QDOAfC7kWsoiqIoien1cWwG1MehKIoyHWP7OBRFUZQlQgWHoiiKMhUqOBRFUZSpUMGhKIqiTMVSOMeJaA+A723w8GMA3DXD5syDRf8Oi95+QL9DLiz6d0jZ/qcw87HSB0shOA4GItreFVmwKCz6d1j09gP6HXJh0b9DLu1XU5WiKIoyFSo4FEVRlKlQwdHP++bdgBmw6N9h0dsP6HfIhUX/Dlm0X30ciqIoylSoxqEoiqJMhQoORVEUZSpUcHRARP+OiK4mouuI6I3zbs9QiOgmIrqSiHYQ0fZ621FEdB4RXVu/PmHe7XQhog8Q0Z1EtNPZJraZKt5d35criOhZ82u5peM7vIWIdtf3YgcRvcD57L/X3+FqInrefFptIaITiOh8IvouEX2HiP5rvX1h7kPkOyzSfXgMEV1MRJfX3+H36+1PJaKL6rZ+rF5BFUR0aP3+uvrzrUkaysz6F/yhWtL2egBPQ7XO+eUAfmTe7RrY9psAHBNs+xMAb6z/fyOAP553O4P2PRfAswDs7GszgBcAOBdVuf7TAVw07/ZHvsNbALxe2PdH6j51KICn1n1tZc7tPx7As+r/DwdwTd3OhbkPke+wSPeBAGyp/19DtYzE6ahWUv3levt7Abyq/v/VAN5b///LAD6Wop2qccicBuA6Zr6BmfcD+CiqtdAXlRcC+FD9/4cA/Pwc29KCmS8AcE+wuavNLwTwd1zxLQBHEtHxaVraTcd36OKFAD7KzI8y840ArkPV5+YGM9/GzN+u/38QwC4AT8IC3YfId+gix/vAzLy3frtW/zGAnwLwiXp7eB/M/fkEgH9DRDR2O1VwyDwJwC3O+1sR74A5wQC+QESXEtEr6m3HMfNt9f+3AzhuPk2biq42L9q9+c3alPMBx0SY9XeozR3/AtVsdyHvQ/AdgAW6D0S0QkQ7ANyJalXU6wHcx8zr9S5uO5vvUH9+P4Cjx26jCo7Nx5nM/CwAzwfwGiJ6rvshVzrtQsVgL2Kba94D4CQApwK4DcCfzrc5/RDRFgCfBPA6Zn7A/WxR7oPwHRbqPjDzhJlPBfBkVBrQD825SS1UcMjsBnCC8/7J9bbsYebd9eudAD6NquPdYcwI9eud82vhYLravDD3hpnvqB8CJYD3w5pBsvwORLSG6oF7FjN/qt68UPdB+g6Ldh8MzHwfgPMBnIHKFGiW+nbb2XyH+vPHA7h77Lap4JC5BMApdSTDIaicTp+dc5t6IaLHEdHh5n8A/xbATlRtf3m928sBnD2fFk5FV5s/C+DX66ie0wHc75hSsiKw+f8CqnsBVN/hl+uImKcCOAXAxanb51Lbxf8GwC5mfqfz0cLch67vsGD34VgiOrL+/7EAfgaVr+Z8AC+qdwvvg7k/LwLw5VozHJd5RhDk/IcqauQaVPbFN827PQPb/DRUUSKXA/iOaTcqm+eXAFwL4IsAjpp3W4N2fwSVCeEAKvvtb3S1GVXUyV/W9+VKANvm3f7Id/j7uo1XoBrgxzv7v6n+DlcDeH4G7T8TlRnqCgA76r8XLNJ9iHyHRboPPwbgsrqtOwG8ud7+NFRC7ToA/xfAofX2x9Tvr6s/f1qKdmrJEUVRFGUq1FSlKIqiTIUKDkVRFGUqVHAoiqIoU6GCQ1EURZkKFRyKoijKVKjgUBRFUaZCBYeiKIoyFf8fe2QjnBE/1ckAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = ytimport(\"https://www.youtube.com/watch?v=l45f28PzfCI\", \"blues\")\n",
        "print(\"blues\")\n",
        "test_loss, f1_, acc_, confmatrix = testing(data, costfun, bestmodel)\n",
        "plt.plot(infrence(data, bestmodel))\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1IRCRX6-vLe",
        "outputId": "b87158d0-eab8-4eb9-b109-b94fc69b2823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is: 78.125 and f1 score is: 0.7836589813232422\n"
          ]
        }
      ],
      "source": [
        "test_loss, f1_, acc_, confmatrix = testing(test_dataloader, costfun, bestmodel)\n",
        "print(f\"Accuracy is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HscnIxq6CoMS"
      },
      "source": [
        "We see that the results are really biased towards classical music. This is not expected behavior since the model is in evaluation mode and won't learn from testing (model.eval()). Perhaps better regularisation would fix the issus as well as a bigger batch size (more than 1 that is at the moment to accomodate the infrence function).\n",
        "\n",
        "Let us compare the results for each song:\n",
        "\n",
        "\n",
        "*   classical: It is accurate, no questions there. It got all the labels right. The track is a good representation of classical music and it was an easy guess.\n",
        "*   hiphop: The first seconds are wierd since there is no music, so the classical classification is unexpected. In addition the song brought back childhood memories, thank you for reminding its existance. The model did a really poor job guessing except from the seconds 225 to 250 that guessed correct. During those seconds there are no lyrics and just music that may have helped in the correct guessing.\n",
        "*   rock: So the model did get nothing right. Somewhat impressive... The Placebo band's wikipedia page should be updated that the band plays classical music whith a hair of hiphop.\n",
        "*   blues: The model also didnt do so well here. This time it wasnt stuck on classical but also guessed some seconds correctly. \n",
        "\n",
        "\n",
        "Last to make sure the model didnt become overfitted while testing the classical music, I tested the model with the test dataset after the predictions from youtube and I got the same result before the predictions just after the training.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
